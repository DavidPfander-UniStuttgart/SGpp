thresholds:[0.0, 111.11111111111111, 222.22222222222223, 333.33333333333337, 444.44444444444446, 555.55555555555554, 666.66666666666674, 777.77777777777783, 888.88888888888891, 1000.0]threshold_step:111.111111111

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 0.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[60654,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 0


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2143s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5017s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.85208s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37747
find clusters duration: 65.7937s
detected clusters: 1
size cluster i: 0 -> 1020000
datapoints in clusters: 1020000
score: 1


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:37:29 2018
elapsed time: 100.667s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 4: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:51028] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:51028] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 1
datapoints_clusters: 1020000
score: 1.0
elapsed_time: 100.667

--------------- END RUN -----------------
-> new best_score:0.00980392156862745new best_threshold:0.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 111.11111111111111 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[58221,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 111.111


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2028s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.4753s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.89089s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38662
find clusters duration: 1.17709s
detected clusters: 73
size cluster i: 0 -> 390390
size cluster i: 1 -> 20006
size cluster i: 2 -> 30024
size cluster i: 3 -> 10003
size cluster i: 4 -> 9954
size cluster i: 5 -> 10003
size cluster i: 6 -> 10006
size cluster i: 7 -> 10004
size cluster i: 8 -> 10003
size cluster i: 9 -> 10001
size cluster i: 10 -> 10008
size cluster i: 11 -> 20017
size cluster i: 12 -> 30025
size cluster i: 13 -> 10010
size cluster i: 14 -> 10007
size cluster i: 15 -> 10001
size cluster i: 16 -> 10007
size cluster i: 17 -> 10000
size cluster i: 18 -> 10004
size cluster i: 19 -> 30037
size cluster i: 20 -> 30024
size cluster i: 21 -> 8395
size cluster i: 22 -> 10002
size cluster i: 23 -> 20014
size cluster i: 24 -> 10003
size cluster i: 25 -> 10005
size cluster i: 26 -> 10008
size cluster i: 27 -> 10001
size cluster i: 28 -> 10003
size cluster i: 29 -> 10013
size cluster i: 30 -> 10003
size cluster i: 31 -> 10002
size cluster i: 32 -> 10007
size cluster i: 33 -> 20006
size cluster i: 34 -> 10007
size cluster i: 35 -> 10004
size cluster i: 36 -> 10006
size cluster i: 37 -> 10003
size cluster i: 38 -> 10004
size cluster i: 39 -> 9112
size cluster i: 40 -> 10007
size cluster i: 41 -> 5951
size cluster i: 42 -> 10003
size cluster i: 43 -> 7807
size cluster i: 44 -> 10002
size cluster i: 45 -> 10000
size cluster i: 46 -> 5096
size cluster i: 47 -> 9765
size cluster i: 48 -> 9219
size cluster i: 49 -> 10002
size cluster i: 50 -> 4
size cluster i: 51 -> 4
size cluster i: 52 -> 3
size cluster i: 53 -> 2
size cluster i: 54 -> 2
size cluster i: 55 -> 2
size cluster i: 56 -> 2
size cluster i: 57 -> 5
size cluster i: 58 -> 3
size cluster i: 59 -> 2
size cluster i: 60 -> 2
size cluster i: 61 -> 3
size cluster i: 62 -> 2
size cluster i: 63 -> 4
size cluster i: 65 -> 3
size cluster i: 66 -> 2
size cluster i: 68 -> 3
size cluster i: 69 -> 2
size cluster i: 70 -> 2
datapoints in clusters: 986040
score: 70.5695


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:38:16 2018
elapsed time: 36.0961s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:51415] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:51415] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 73
datapoints_clusters: 986040
score: 70.5695
elapsed_time: 36.0961

--------------- END RUN -----------------
-> new best_score:0.49465588235294117new best_threshold:111.111111111

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 222.22222222222223 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[57759,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 222.222


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.165s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5425s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.82387s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 35996
find clusters duration: 0.293102s
detected clusters: 115
size cluster i: 0 -> 10002
size cluster i: 1 -> 20001
size cluster i: 2 -> 20007
size cluster i: 3 -> 9644
size cluster i: 4 -> 10003
size cluster i: 5 -> 30015
size cluster i: 6 -> 10000
size cluster i: 7 -> 10001
size cluster i: 8 -> 10000
size cluster i: 9 -> 10000
size cluster i: 10 -> 10007
size cluster i: 11 -> 50016
size cluster i: 12 -> 10000
size cluster i: 13 -> 8779
size cluster i: 14 -> 10001
size cluster i: 15 -> 10004
size cluster i: 16 -> 10000
size cluster i: 17 -> 9995
size cluster i: 18 -> 10000
size cluster i: 19 -> 10002
size cluster i: 20 -> 10002
size cluster i: 21 -> 20008
size cluster i: 22 -> 10001
size cluster i: 23 -> 10001
size cluster i: 24 -> 10001
size cluster i: 25 -> 10001
size cluster i: 26 -> 10002
size cluster i: 27 -> 20003
size cluster i: 28 -> 9995
size cluster i: 29 -> 9902
size cluster i: 30 -> 10001
size cluster i: 31 -> 10002
size cluster i: 32 -> 10001
size cluster i: 33 -> 10006
size cluster i: 34 -> 10001
size cluster i: 35 -> 10003
size cluster i: 36 -> 10002
size cluster i: 37 -> 10003
size cluster i: 38 -> 10004
size cluster i: 39 -> 9995
size cluster i: 40 -> 20004
size cluster i: 41 -> 9245
size cluster i: 42 -> 10000
size cluster i: 43 -> 10001
size cluster i: 44 -> 10001
size cluster i: 45 -> 10000
size cluster i: 46 -> 30011
size cluster i: 47 -> 10000
size cluster i: 48 -> 10005
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 9973
size cluster i: 53 -> 10001
size cluster i: 54 -> 10003
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 20012
size cluster i: 59 -> 20005
size cluster i: 60 -> 10002
size cluster i: 61 -> 10003
size cluster i: 62 -> 9833
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10000
size cluster i: 66 -> 10000
size cluster i: 67 -> 10000
size cluster i: 68 -> 10002
size cluster i: 69 -> 10000
size cluster i: 70 -> 10004
size cluster i: 71 -> 9997
size cluster i: 72 -> 10000
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 2349
size cluster i: 76 -> 841
size cluster i: 77 -> 9951
size cluster i: 78 -> 7006
size cluster i: 79 -> 1371
size cluster i: 80 -> 283
size cluster i: 81 -> 102
size cluster i: 82 -> 2
size cluster i: 83 -> 7
size cluster i: 84 -> 58
size cluster i: 85 -> 2
size cluster i: 86 -> 3
size cluster i: 87 -> 2
size cluster i: 88 -> 2
size cluster i: 89 -> 2
size cluster i: 90 -> 6
size cluster i: 91 -> 2
size cluster i: 93 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 3
size cluster i: 99 -> 2
size cluster i: 100 -> 2
size cluster i: 101 -> 2
size cluster i: 104 -> 2
size cluster i: 108 -> 2
size cluster i: 109 -> 2
size cluster i: 110 -> 2
size cluster i: 114 -> 2
datapoints in clusters: 919542
score: 76.6285


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:39:01 2018
elapsed time: 35.1352s


Finishing: 
---------- 
Beginning cleanup...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 7: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:51749] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:51749] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 115
datapoints_clusters: 919542
score: 76.6285
elapsed_time: 35.1352

--------------- END RUN -----------------
-> new best_score:0.7736813725490196new best_threshold:222.222222222

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 333.33333333333337 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[57401,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 333.333


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1324s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5405s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.89787s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 34254
find clusters duration: 0.260039s
detected clusters: 100
size cluster i: 0 -> 10001
size cluster i: 1 -> 9220
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9543
size cluster i: 8 -> 9909
size cluster i: 9 -> 9962
size cluster i: 10 -> 10001
size cluster i: 11 -> 30005
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10001
size cluster i: 18 -> 10001
size cluster i: 19 -> 20004
size cluster i: 20 -> 10001
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 10000
size cluster i: 25 -> 9989
size cluster i: 26 -> 10001
size cluster i: 27 -> 8662
size cluster i: 28 -> 6017
size cluster i: 29 -> 10000
size cluster i: 30 -> 10001
size cluster i: 31 -> 9405
size cluster i: 32 -> 10002
size cluster i: 33 -> 9980
size cluster i: 34 -> 10001
size cluster i: 35 -> 10000
size cluster i: 36 -> 10000
size cluster i: 37 -> 10001
size cluster i: 38 -> 19971
size cluster i: 39 -> 9520
size cluster i: 40 -> 10000
size cluster i: 41 -> 10000
size cluster i: 42 -> 10000
size cluster i: 43 -> 10001
size cluster i: 44 -> 9216
size cluster i: 45 -> 10001
size cluster i: 46 -> 10000
size cluster i: 47 -> 9457
size cluster i: 48 -> 20002
size cluster i: 49 -> 9962
size cluster i: 50 -> 7694
size cluster i: 51 -> 2491
size cluster i: 52 -> 10000
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 20005
size cluster i: 59 -> 10003
size cluster i: 60 -> 9151
size cluster i: 61 -> 10001
size cluster i: 62 -> 10001
size cluster i: 63 -> 10000
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10000
size cluster i: 67 -> 9704
size cluster i: 68 -> 10000
size cluster i: 69 -> 8160
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 8542
size cluster i: 76 -> 10000
size cluster i: 77 -> 9000
size cluster i: 78 -> 10001
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 5604
size cluster i: 82 -> 3283
size cluster i: 83 -> 4698
size cluster i: 84 -> 5879
size cluster i: 85 -> 290
size cluster i: 86 -> 4
size cluster i: 87 -> 4
size cluster i: 88 -> 2
size cluster i: 89 -> 9
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 3
datapoints in clusters: 865385
score: 84.8417


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:39:46 2018
elapsed time: 35.1735s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:52099] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:52099] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 100
datapoints_clusters: 865385
score: 84.8417
elapsed_time: 35.1735

--------------- END RUN -----------------
-> new best_score:0.8091107843137255new best_threshold:333.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 444.44444444444446 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[59226,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 444.444


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.129s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.514s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.83327s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 31068
find clusters duration: 0.218601s
detected clusters: 133
size cluster i: 0 -> 10000
size cluster i: 1 -> 4632
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 9993
size cluster i: 6 -> 9954
size cluster i: 7 -> 5735
size cluster i: 8 -> 7371
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 9904
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10001
size cluster i: 15 -> 9382
size cluster i: 16 -> 10000
size cluster i: 17 -> 9994
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10000
size cluster i: 22 -> 9977
size cluster i: 23 -> 10000
size cluster i: 24 -> 10000
size cluster i: 25 -> 9964
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10000
size cluster i: 33 -> 9999
size cluster i: 34 -> 10000
size cluster i: 35 -> 10000
size cluster i: 36 -> 10000
size cluster i: 37 -> 10000
size cluster i: 38 -> 10000
size cluster i: 39 -> 4898
size cluster i: 40 -> 10000
size cluster i: 41 -> 10000
size cluster i: 42 -> 20001
size cluster i: 43 -> 9999
size cluster i: 44 -> 10000
size cluster i: 45 -> 9994
size cluster i: 46 -> 9654
size cluster i: 47 -> 10000
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 9146
size cluster i: 52 -> 10000
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 9212
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 9997
size cluster i: 60 -> 6395
size cluster i: 61 -> 9642
size cluster i: 62 -> 10000
size cluster i: 63 -> 8392
size cluster i: 64 -> 9929
size cluster i: 65 -> 10000
size cluster i: 66 -> 5915
size cluster i: 67 -> 10000
size cluster i: 68 -> 8650
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 9645
size cluster i: 72 -> 8336
size cluster i: 73 -> 9970
size cluster i: 74 -> 10000
size cluster i: 75 -> 4903
size cluster i: 76 -> 4995
size cluster i: 77 -> 10000
size cluster i: 78 -> 5137
size cluster i: 79 -> 4222
size cluster i: 80 -> 2952
size cluster i: 81 -> 2711
size cluster i: 82 -> 1986
size cluster i: 83 -> 2251
size cluster i: 84 -> 568
size cluster i: 85 -> 404
size cluster i: 86 -> 625
size cluster i: 87 -> 475
size cluster i: 88 -> 148
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 3
size cluster i: 92 -> 2
size cluster i: 93 -> 64
size cluster i: 94 -> 3
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 3
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 100 -> 4
size cluster i: 102 -> 2
size cluster i: 104 -> 3
size cluster i: 105 -> 3
size cluster i: 106 -> 3
size cluster i: 107 -> 3
size cluster i: 108 -> 2
size cluster i: 109 -> 2
size cluster i: 110 -> 2
size cluster i: 113 -> 2
size cluster i: 117 -> 2
size cluster i: 123 -> 2
size cluster i: 124 -> 3
size cluster i: 132 -> 2
datapoints in clusters: 768203
score: 50.4604


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:40:31 2018
elapsed time: 35.045s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:52448] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:52448] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with theWarning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
value given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 133
datapoints_clusters: 768203
score: 50.4604
elapsed_time: 35.045

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 555.5555555555555 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[58776,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 555.556


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1028s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.4869s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.84804s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 27036
find clusters duration: 0.189146s
detected clusters: 155
size cluster i: 0 -> 10000
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 9995
size cluster i: 4 -> 9387
size cluster i: 5 -> 9083
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 8395
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10000
size cluster i: 13 -> 9611
size cluster i: 14 -> 9927
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10000
size cluster i: 18 -> 10000
size cluster i: 19 -> 1992
size cluster i: 20 -> 10000
size cluster i: 21 -> 8963
size cluster i: 22 -> 10000
size cluster i: 23 -> 10000
size cluster i: 24 -> 10000
size cluster i: 25 -> 10000
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 10000
size cluster i: 29 -> 9809
size cluster i: 30 -> 10000
size cluster i: 31 -> 9995
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10000
size cluster i: 36 -> 9250
size cluster i: 37 -> 10000
size cluster i: 38 -> 10000
size cluster i: 39 -> 9507
size cluster i: 40 -> 9999
size cluster i: 41 -> 9757
size cluster i: 42 -> 10000
size cluster i: 43 -> 10000
size cluster i: 44 -> 10001
size cluster i: 45 -> 10000
size cluster i: 46 -> 5911
size cluster i: 47 -> 10000
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 5776
size cluster i: 51 -> 10000
size cluster i: 52 -> 10000
size cluster i: 53 -> 10000
size cluster i: 54 -> 9851
size cluster i: 55 -> 6451
size cluster i: 56 -> 10000
size cluster i: 57 -> 8716
size cluster i: 58 -> 10000
size cluster i: 59 -> 7114
size cluster i: 60 -> 1427
size cluster i: 61 -> 10000
size cluster i: 62 -> 4822
size cluster i: 63 -> 10000
size cluster i: 64 -> 10000
size cluster i: 65 -> 10001
size cluster i: 66 -> 4097
size cluster i: 67 -> 3223
size cluster i: 68 -> 9177
size cluster i: 69 -> 10000
size cluster i: 70 -> 6486
size cluster i: 71 -> 9981
size cluster i: 72 -> 1224
size cluster i: 73 -> 524
size cluster i: 74 -> 1903
size cluster i: 75 -> 631
size cluster i: 76 -> 741
size cluster i: 77 -> 1488
size cluster i: 78 -> 45
size cluster i: 79 -> 323
size cluster i: 80 -> 1105
size cluster i: 81 -> 731
size cluster i: 82 -> 13
size cluster i: 83 -> 55
size cluster i: 84 -> 2
size cluster i: 85 -> 2
size cluster i: 86 -> 2
size cluster i: 87 -> 3
size cluster i: 88 -> 3
size cluster i: 89 -> 5
size cluster i: 90 -> 2
size cluster i: 91 -> 5
size cluster i: 92 -> 2
size cluster i: 94 -> 4
size cluster i: 95 -> 3
size cluster i: 96 -> 27
size cluster i: 97 -> 3
size cluster i: 98 -> 2
size cluster i: 99 -> 3
size cluster i: 100 -> 2
size cluster i: 101 -> 3
size cluster i: 103 -> 2
size cluster i: 104 -> 2
size cluster i: 105 -> 2
size cluster i: 107 -> 2
size cluster i: 108 -> 2
size cluster i: 109 -> 2
size cluster i: 111 -> 3
size cluster i: 112 -> 2
size cluster i: 113 -> 2
size cluster i: 114 -> 4
size cluster i: 117 -> 2
size cluster i: 118 -> 2
size cluster i: 119 -> 3
size cluster i: 120 -> 3
size cluster i: 121 -> 3
size cluster i: 122 -> 3
size cluster i: 126 -> 3
size cluster i: 128 -> 4
size cluster i: 129 -> 3
size cluster i: 130 -> 2
size cluster i: 131 -> 2
size cluster i: 132 -> 2
size cluster i: 133 -> 2
size cluster i: 134 -> 2
size cluster i: 135 -> 2
size cluster i: 137 -> 2
size cluster i: 139 -> 2
size cluster i: 140 -> 4
size cluster i: 145 -> 2
datapoints in clusters: 667657
score: 29.4555


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:41:16 2018
elapsed time: 34.9569s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 7: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:52770] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:52770] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 155
datapoints_clusters: 667657
score: 29.4555
elapsed_time: 34.9569

--------------- END RUN -----------------
datapoints_clusters too low
thresholds:[ 246.91358025  271.60493827  296.2962963   320.98765432  345.67901235
  370.37037037  395.0617284   419.75308642]threshold_step:24.6913580247

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 246.91358024691363 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[58598,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 246.914


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1809s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5958s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.8297s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 35496
find clusters duration: 0.284217s
detected clusters: 109
size cluster i: 0 -> 10002
size cluster i: 1 -> 9995
size cluster i: 2 -> 20005
size cluster i: 3 -> 10003
size cluster i: 4 -> 30012
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9999
size cluster i: 8 -> 10000
size cluster i: 9 -> 10000
size cluster i: 10 -> 10003
size cluster i: 11 -> 40010
size cluster i: 12 -> 10000
size cluster i: 13 -> 7618
size cluster i: 14 -> 10001
size cluster i: 15 -> 10002
size cluster i: 16 -> 10000
size cluster i: 17 -> 10000
size cluster i: 18 -> 10002
size cluster i: 19 -> 10002
size cluster i: 20 -> 20006
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 9097
size cluster i: 25 -> 9987
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 20003
size cluster i: 29 -> 9965
size cluster i: 30 -> 9604
size cluster i: 31 -> 10001
size cluster i: 32 -> 10002
size cluster i: 33 -> 9996
size cluster i: 34 -> 10005
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10002
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 9948
size cluster i: 41 -> 20002
size cluster i: 42 -> 9995
size cluster i: 43 -> 10001
size cluster i: 44 -> 10001
size cluster i: 45 -> 10000
size cluster i: 46 -> 30008
size cluster i: 47 -> 9990
size cluster i: 48 -> 10004
size cluster i: 49 -> 10000
size cluster i: 50 -> 9997
size cluster i: 51 -> 10000
size cluster i: 52 -> 9872
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 20010
size cluster i: 59 -> 10003
size cluster i: 60 -> 8276
size cluster i: 61 -> 10001
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 9461
size cluster i: 65 -> 10001
size cluster i: 66 -> 10000
size cluster i: 67 -> 10000
size cluster i: 68 -> 10000
size cluster i: 69 -> 10000
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 10003
size cluster i: 73 -> 9969
size cluster i: 74 -> 10000
size cluster i: 75 -> 9967
size cluster i: 76 -> 10001
size cluster i: 77 -> 10000
size cluster i: 78 -> 889
size cluster i: 79 -> 291
size cluster i: 80 -> 9671
size cluster i: 81 -> 5048
size cluster i: 82 -> 419
size cluster i: 83 -> 2
size cluster i: 84 -> 21
size cluster i: 85 -> 9
size cluster i: 86 -> 2
size cluster i: 87 -> 3
size cluster i: 88 -> 3
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 92 -> 3
size cluster i: 93 -> 5
size cluster i: 94 -> 3
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 2
size cluster i: 102 -> 2
size cluster i: 106 -> 2
datapoints in clusters: 910241
score: 81.2078


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:41:55 2018
elapsed time: 35.2159s


Finishing: 
---------- 
Beginning cleanup...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 5: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:53084] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:53084] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 109
datapoints_clusters: 910241
score: 81.2078
elapsed_time: 35.2159

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 271.60493827160496 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[64292,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 271.605


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2522s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5152s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.83905s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 35167
find clusters duration: 0.269601s
detected clusters: 101
size cluster i: 0 -> 10001
size cluster i: 1 -> 9945
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 30009
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9979
size cluster i: 8 -> 9999
size cluster i: 9 -> 10000
size cluster i: 10 -> 10002
size cluster i: 11 -> 30007
size cluster i: 12 -> 10000
size cluster i: 13 -> 10001
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10002
size cluster i: 19 -> 20006
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 8191
size cluster i: 25 -> 9931
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 20003
size cluster i: 29 -> 9828
size cluster i: 30 -> 9009
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 9966
size cluster i: 34 -> 10004
size cluster i: 35 -> 10000
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9811
size cluster i: 41 -> 20002
size cluster i: 42 -> 9967
size cluster i: 43 -> 10001
size cluster i: 44 -> 10001
size cluster i: 45 -> 10000
size cluster i: 46 -> 30005
size cluster i: 47 -> 9919
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 9984
size cluster i: 51 -> 10000
size cluster i: 52 -> 9595
size cluster i: 53 -> 6233
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10001
size cluster i: 60 -> 20008
size cluster i: 61 -> 10003
size cluster i: 62 -> 6962
size cluster i: 63 -> 10001
size cluster i: 64 -> 10002
size cluster i: 65 -> 10000
size cluster i: 66 -> 8732
size cluster i: 67 -> 10001
size cluster i: 68 -> 9992
size cluster i: 69 -> 10000
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 10003
size cluster i: 75 -> 9873
size cluster i: 76 -> 10000
size cluster i: 77 -> 9857
size cluster i: 78 -> 10001
size cluster i: 79 -> 10000
size cluster i: 80 -> 2
size cluster i: 81 -> 9057
size cluster i: 82 -> 3134
size cluster i: 83 -> 194
size cluster i: 84 -> 94
size cluster i: 85 -> 96
size cluster i: 86 -> 2
size cluster i: 87 -> 2
size cluster i: 88 -> 2
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 92 -> 2
size cluster i: 94 -> 3
datapoints in clusters: 900460
score: 87.3976


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:42:40 2018
elapsed time: 35.2024s


Finishing: 
---------- 
Beginning cleanup...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 7: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:53406] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:53406] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 101
datapoints_clusters: 900460
score: 87.3976
elapsed_time: 35.2024

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 296.2962962962963 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[64125,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 296.296


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.117s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5231s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.86899s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 34923
find clusters duration: 0.259817s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 9780
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9907
size cluster i: 8 -> 9990
size cluster i: 9 -> 9992
size cluster i: 10 -> 10001
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10001
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 6909
size cluster i: 25 -> 9773
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 10002
size cluster i: 29 -> 9549
size cluster i: 30 -> 7987
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 9870
size cluster i: 34 -> 10003
size cluster i: 35 -> 9998
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9429
size cluster i: 41 -> 19997
size cluster i: 42 -> 9876
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 10002
size cluster i: 47 -> 9766
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 9897
size cluster i: 51 -> 20002
size cluster i: 52 -> 9999
size cluster i: 53 -> 9063
size cluster i: 54 -> 4701
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 20006
size cluster i: 62 -> 10003
size cluster i: 63 -> 5480
size cluster i: 64 -> 10001
size cluster i: 65 -> 10002
size cluster i: 66 -> 10000
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 9953
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 7685
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10001
size cluster i: 78 -> 9562
size cluster i: 79 -> 10000
size cluster i: 80 -> 9625
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 8019
size cluster i: 85 -> 1579
size cluster i: 86 -> 17
size cluster i: 87 -> 2
size cluster i: 88 -> 3
size cluster i: 89 -> 7
size cluster i: 90 -> 2
size cluster i: 91 -> 3
size cluster i: 92 -> 3
size cluster i: 93 -> 2
size cluster i: 94 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 3
size cluster i: 97 -> 2
size cluster i: 99 -> 3
size cluster i: 100 -> 3
datapoints in clusters: 888508
score: 83.6243


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:43:25 2018
elapsed time: 35.1278s


Finishing: 
---------- 
Node 7 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:53703] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:53703] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 888508
score: 83.6243
elapsed_time: 35.1278

--------------- END RUN -----------------
-> new best_score:0.8317117647058824new best_threshold:296.296296296

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 320.98765432098764 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[63809,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 320.988


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1186s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5613s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.81705s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 34438
find clusters duration: 0.260357s
detected clusters: 105
size cluster i: 0 -> 10001
size cluster i: 1 -> 9453
size cluster i: 2 -> 10000
size cluster i: 3 -> 10001
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9714
size cluster i: 8 -> 9950
size cluster i: 9 -> 9976
size cluster i: 10 -> 10001
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10001
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10003
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 5435
size cluster i: 25 -> 10000
size cluster i: 26 -> 9995
size cluster i: 27 -> 10001
size cluster i: 28 -> 9032
size cluster i: 29 -> 6707
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 9626
size cluster i: 33 -> 10002
size cluster i: 34 -> 9988
size cluster i: 35 -> 10002
size cluster i: 36 -> 10000
size cluster i: 37 -> 10000
size cluster i: 38 -> 10001
size cluster i: 39 -> 19985
size cluster i: 40 -> 9687
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10000
size cluster i: 44 -> 10002
size cluster i: 45 -> 9458
size cluster i: 46 -> 10001
size cluster i: 47 -> 10000
size cluster i: 48 -> 8668
size cluster i: 49 -> 9655
size cluster i: 50 -> 20002
size cluster i: 51 -> 9989
size cluster i: 52 -> 8217
size cluster i: 53 -> 3175
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 20005
size cluster i: 61 -> 10003
size cluster i: 62 -> 9437
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10000
size cluster i: 66 -> 10001
size cluster i: 67 -> 10001
size cluster i: 68 -> 10000
size cluster i: 69 -> 9832
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 6353
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 10001
size cluster i: 77 -> 8960
size cluster i: 78 -> 10000
size cluster i: 79 -> 9241
size cluster i: 80 -> 10001
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 4039
size cluster i: 84 -> 6616
size cluster i: 85 -> 586
size cluster i: 86 -> 5
size cluster i: 87 -> 2
size cluster i: 88 -> 4
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 94 -> 2
size cluster i: 95 -> 2
size cluster i: 98 -> 2
datapoints in clusters: 873855
score: 81.3885


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:44:10 2018
elapsed time: 35.0997s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 5: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:54011] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:54011] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 105
datapoints_clusters: 873855
score: 81.3885
elapsed_time: 35.0997

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 345.67901234567904 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[65409,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 345.679


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0944s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5648s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.9487s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 33984
find clusters duration: 0.245168s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 8894
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9333
size cluster i: 8 -> 9829
size cluster i: 9 -> 9924
size cluster i: 10 -> 10001
size cluster i: 11 -> 10001
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 20004
size cluster i: 15 -> 10001
size cluster i: 16 -> 10000
size cluster i: 17 -> 10000
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 20004
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10000
size cluster i: 24 -> 10000
size cluster i: 25 -> 10000
size cluster i: 26 -> 9975
size cluster i: 27 -> 10001
size cluster i: 28 -> 5201
size cluster i: 29 -> 10000
size cluster i: 30 -> 10001
size cluster i: 31 -> 9130
size cluster i: 32 -> 10001
size cluster i: 33 -> 9959
size cluster i: 34 -> 10000
size cluster i: 35 -> 10000
size cluster i: 36 -> 10000
size cluster i: 37 -> 10001
size cluster i: 38 -> 8219
size cluster i: 39 -> 19942
size cluster i: 40 -> 9315
size cluster i: 41 -> 10000
size cluster i: 42 -> 10000
size cluster i: 43 -> 10000
size cluster i: 44 -> 10001
size cluster i: 45 -> 8930
size cluster i: 46 -> 10001
size cluster i: 47 -> 10000
size cluster i: 48 -> 20002
size cluster i: 49 -> 9936
size cluster i: 50 -> 7114
size cluster i: 51 -> 10000
size cluster i: 52 -> 10000
size cluster i: 53 -> 10000
size cluster i: 54 -> 9999
size cluster i: 55 -> 10000
size cluster i: 56 -> 10001
size cluster i: 57 -> 20005
size cluster i: 58 -> 10002
size cluster i: 59 -> 8845
size cluster i: 60 -> 10001
size cluster i: 61 -> 10000
size cluster i: 62 -> 10000
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10000
size cluster i: 66 -> 9573
size cluster i: 67 -> 10000
size cluster i: 68 -> 7590
size cluster i: 69 -> 10000
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10001
size cluster i: 74 -> 8070
size cluster i: 75 -> 9998
size cluster i: 76 -> 10001
size cluster i: 77 -> 10000
size cluster i: 78 -> 9180
size cluster i: 79 -> 8646
size cluster i: 80 -> 10000
size cluster i: 81 -> 4856
size cluster i: 82 -> 2669
size cluster i: 83 -> 3967
size cluster i: 84 -> 5072
size cluster i: 85 -> 1924
size cluster i: 86 -> 122
size cluster i: 87 -> 3
size cluster i: 88 -> 3
size cluster i: 89 -> 3
size cluster i: 90 -> 3
size cluster i: 91 -> 4
size cluster i: 92 -> 2
size cluster i: 93 -> 3
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 99 -> 2
datapoints in clusters: 856281
score: 80.5912


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:44:55 2018
elapsed time: 35.1937s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 3: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 
Node 7: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:54331] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:54331] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 856281
score: 80.5912
elapsed_time: 35.1937

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 370.3703703703704 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[65229,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 370.37


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1139s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5053s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.8925s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 33238
find clusters duration: 0.240579s
detected clusters: 109
size cluster i: 0 -> 10001
size cluster i: 1 -> 8036
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 8732
size cluster i: 8 -> 9546
size cluster i: 9 -> 9755
size cluster i: 10 -> 10001
size cluster i: 11 -> 10001
size cluster i: 12 -> 9999
size cluster i: 13 -> 10000
size cluster i: 14 -> 20001
size cluster i: 15 -> 10001
size cluster i: 16 -> 9992
size cluster i: 17 -> 10000
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10000
size cluster i: 24 -> 10000
size cluster i: 25 -> 10000
size cluster i: 26 -> 9921
size cluster i: 27 -> 10000
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10001
size cluster i: 31 -> 9866
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 7096
size cluster i: 37 -> 10000
size cluster i: 38 -> 10000
size cluster i: 39 -> 10000
size cluster i: 40 -> 10000
size cluster i: 41 -> 10001
size cluster i: 42 -> 8151
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 8762
size cluster i: 46 -> 20002
size cluster i: 47 -> 9776
size cluster i: 48 -> 5878
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 9991
size cluster i: 53 -> 10000
size cluster i: 54 -> 10001
size cluster i: 55 -> 10002
size cluster i: 56 -> 10002
size cluster i: 57 -> 7947
size cluster i: 58 -> 10001
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10000
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10000
size cluster i: 65 -> 9110
size cluster i: 66 -> 9988
size cluster i: 67 -> 6188
size cluster i: 68 -> 10002
size cluster i: 69 -> 10000
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 9817
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 9986
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 8451
size cluster i: 79 -> 6902
size cluster i: 80 -> 7883
size cluster i: 81 -> 10000
size cluster i: 82 -> 8390
size cluster i: 83 -> 3357
size cluster i: 84 -> 3549
size cluster i: 85 -> 3714
size cluster i: 86 -> 1571
size cluster i: 87 -> 2724
size cluster i: 88 -> 1031
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 10
size cluster i: 92 -> 6
size cluster i: 93 -> 2
size cluster i: 94 -> 4
size cluster i: 95 -> 2
size cluster i: 97 -> 2
size cluster i: 107 -> 2
datapoints in clusters: 836178
score: 74.6002


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:45:40 2018
elapsed time: 35.0815s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:54647] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:54647] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 109
datapoints_clusters: 836178
score: 74.6002
elapsed_time: 35.0815

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 395.0617283950618 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[64783,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 395.062


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0881s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.4893s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.82699s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 32641
find clusters duration: 0.233552s
detected clusters: 111
size cluster i: 0 -> 10001
size cluster i: 1 -> 7022
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 9999
size cluster i: 7 -> 7925
size cluster i: 8 -> 9072
size cluster i: 9 -> 9430
size cluster i: 10 -> 10001
size cluster i: 11 -> 10001
size cluster i: 12 -> 9995
size cluster i: 13 -> 10000
size cluster i: 14 -> 20001
size cluster i: 15 -> 10001
size cluster i: 16 -> 9946
size cluster i: 17 -> 10000
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10000
size cluster i: 22 -> 10000
size cluster i: 23 -> 10000
size cluster i: 24 -> 10000
size cluster i: 25 -> 10000
size cluster i: 26 -> 9805
size cluster i: 27 -> 10000
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 5735
size cluster i: 36 -> 10000
size cluster i: 37 -> 10000
size cluster i: 38 -> 10000
size cluster i: 39 -> 10000
size cluster i: 40 -> 10000
size cluster i: 41 -> 7174
size cluster i: 42 -> 10001
size cluster i: 43 -> 10000
size cluster i: 44 -> 8014
size cluster i: 45 -> 20001
size cluster i: 46 -> 9508
size cluster i: 47 -> 4529
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 9947
size cluster i: 52 -> 10000
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10002
size cluster i: 56 -> 9733
size cluster i: 57 -> 6785
size cluster i: 58 -> 10001
size cluster i: 59 -> 10000
size cluster i: 60 -> 10000
size cluster i: 61 -> 10000
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10000
size cluster i: 65 -> 8466
size cluster i: 66 -> 9959
size cluster i: 67 -> 4647
size cluster i: 68 -> 10002
size cluster i: 69 -> 9996
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 9567
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 9944
size cluster i: 76 -> 9997
size cluster i: 77 -> 10000
size cluster i: 78 -> 7508
size cluster i: 79 -> 5536
size cluster i: 80 -> 6996
size cluster i: 81 -> 10000
size cluster i: 82 -> 7469
size cluster i: 83 -> 2109
size cluster i: 84 -> 2288
size cluster i: 85 -> 2274
size cluster i: 86 -> 853
size cluster i: 87 -> 1726
size cluster i: 88 -> 472
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 92 -> 4
size cluster i: 93 -> 5
size cluster i: 94 -> 2
size cluster i: 95 -> 4
size cluster i: 98 -> 2
size cluster i: 100 -> 3
size cluster i: 102 -> 2
size cluster i: 103 -> 2
size cluster i: 106 -> 2
datapoints in clusters: 814487
score: 71.068


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:46:25 2018
elapsed time: 35.0204s


Finishing: 
---------- 
Node 8 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 5: Exiting... 
Node 4: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:54965] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:54965] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 111
datapoints_clusters: 814487
score: 71.068
elapsed_time: 35.0204

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 419.7530864197531 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[64591,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 419.753


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1547s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.499s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.8879s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 31839
find clusters duration: 0.229328s
detected clusters: 119
size cluster i: 0 -> 10000
size cluster i: 1 -> 5885
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 9998
size cluster i: 6 -> 9983
size cluster i: 7 -> 6913
size cluster i: 8 -> 8348
size cluster i: 9 -> 8980
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 9970
size cluster i: 13 -> 10000
size cluster i: 14 -> 20001
size cluster i: 15 -> 10001
size cluster i: 16 -> 9789
size cluster i: 17 -> 10000
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10000
size cluster i: 22 -> 10000
size cluster i: 23 -> 9999
size cluster i: 24 -> 10000
size cluster i: 25 -> 10000
size cluster i: 26 -> 9987
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 4305
size cluster i: 35 -> 10000
size cluster i: 36 -> 10000
size cluster i: 37 -> 10000
size cluster i: 38 -> 10000
size cluster i: 39 -> 10000
size cluster i: 40 -> 6090
size cluster i: 41 -> 10000
size cluster i: 42 -> 10000
size cluster i: 43 -> 6995
size cluster i: 44 -> 20001
size cluster i: 45 -> 9048
size cluster i: 46 -> 10000
size cluster i: 47 -> 10000
size cluster i: 48 -> 10000
size cluster i: 49 -> 9845
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 9502
size cluster i: 55 -> 5548
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 9569
size cluster i: 60 -> 10000
size cluster i: 61 -> 10000
size cluster i: 62 -> 10000
size cluster i: 63 -> 10000
size cluster i: 64 -> 7534
size cluster i: 65 -> 9855
size cluster i: 66 -> 10000
size cluster i: 67 -> 9982
size cluster i: 68 -> 10000
size cluster i: 69 -> 10000
size cluster i: 70 -> 9176
size cluster i: 71 -> 10000
size cluster i: 72 -> 10001
size cluster i: 73 -> 9839
size cluster i: 74 -> 9992
size cluster i: 75 -> 10000
size cluster i: 76 -> 6346
size cluster i: 77 -> 5930
size cluster i: 78 -> 10000
size cluster i: 79 -> 6363
size cluster i: 80 -> 4051
size cluster i: 81 -> 1216
size cluster i: 82 -> 3187
size cluster i: 83 -> 3345
size cluster i: 84 -> 1118
size cluster i: 85 -> 1317
size cluster i: 86 -> 921
size cluster i: 87 -> 175
size cluster i: 88 -> 392
size cluster i: 89 -> 2
size cluster i: 90 -> 3
size cluster i: 91 -> 2
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 4
size cluster i: 95 -> 4
size cluster i: 97 -> 4
size cluster i: 98 -> 4
size cluster i: 99 -> 2
size cluster i: 100 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 2
size cluster i: 104 -> 2
size cluster i: 105 -> 2
size cluster i: 108 -> 2
size cluster i: 111 -> 2
datapoints in clusters: 791558
score: 62.859


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:47:10 2018
elapsed time: 35.1101s


Finishing: 
---------- 
Beginning cleanup...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:55285] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:55285] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 119
datapoints_clusters: 791558
score: 62.859
elapsed_time: 35.1101

--------------- END RUN -----------------
thresholds:[ 277.09190672  282.57887517  288.06584362  293.55281207  299.03978052
  304.52674897  310.01371742  315.50068587]threshold_step:5.48696844993

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 277.09190672153636 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[62097,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 277.092


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0883s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5573s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.78549s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 35142
find clusters duration: 0.276854s
detected clusters: 95
size cluster i: 0 -> 10001
size cluster i: 1 -> 9921
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 30007
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9971
size cluster i: 8 -> 9998
size cluster i: 9 -> 9999
size cluster i: 10 -> 10002
size cluster i: 11 -> 30007
size cluster i: 12 -> 10000
size cluster i: 13 -> 10001
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10002
size cluster i: 19 -> 20006
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 7943
size cluster i: 25 -> 9916
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 20003
size cluster i: 29 -> 9788
size cluster i: 30 -> 8801
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 9953
size cluster i: 34 -> 10003
size cluster i: 35 -> 10000
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9742
size cluster i: 41 -> 20001
size cluster i: 42 -> 9957
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 30005
size cluster i: 47 -> 9905
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 9966
size cluster i: 51 -> 10000
size cluster i: 52 -> 9506
size cluster i: 53 -> 5897
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 20008
size cluster i: 61 -> 10003
size cluster i: 62 -> 6637
size cluster i: 63 -> 10001
size cluster i: 64 -> 10002
size cluster i: 65 -> 10000
size cluster i: 66 -> 8520
size cluster i: 67 -> 10001
size cluster i: 68 -> 9987
size cluster i: 69 -> 10000
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 10003
size cluster i: 75 -> 9830
size cluster i: 76 -> 10000
size cluster i: 77 -> 9821
size cluster i: 78 -> 10001
size cluster i: 79 -> 10000
size cluster i: 80 -> 8841
size cluster i: 81 -> 2738
size cluster i: 82 -> 135
size cluster i: 83 -> 63
size cluster i: 84 -> 76
size cluster i: 85 -> 5
size cluster i: 86 -> 3
size cluster i: 87 -> 6
size cluster i: 88 -> 2
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 92 -> 2
datapoints in clusters: 898017
score: 83.6388


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:47:55 2018
elapsed time: 35.0441s


Finishing: 
---------- 
Beginning cleanup...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:55595] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:55595] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 95
datapoints_clusters: 898017
score: 83.6388
elapsed_time: 35.0441

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 282.57887517146776 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[61912,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 282.579


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0812s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.536s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.8783s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 35104
find clusters duration: 0.267296s
detected clusters: 102
size cluster i: 0 -> 10001
size cluster i: 1 -> 9888
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 20003
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9955
size cluster i: 8 -> 9995
size cluster i: 9 -> 9996
size cluster i: 10 -> 10002
size cluster i: 11 -> 30007
size cluster i: 12 -> 10000
size cluster i: 13 -> 10001
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10002
size cluster i: 19 -> 20005
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 7676
size cluster i: 25 -> 9880
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 10002
size cluster i: 29 -> 9733
size cluster i: 30 -> 8614
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 9938
size cluster i: 34 -> 10003
size cluster i: 35 -> 10000
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9681
size cluster i: 41 -> 20000
size cluster i: 42 -> 9940
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 30005
size cluster i: 47 -> 9867
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 9959
size cluster i: 51 -> 10000
size cluster i: 52 -> 9406
size cluster i: 53 -> 5567
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 20008
size cluster i: 61 -> 10003
size cluster i: 62 -> 6275
size cluster i: 63 -> 10001
size cluster i: 64 -> 10002
size cluster i: 65 -> 10000
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 9982
size cluster i: 69 -> 10000
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 8300
size cluster i: 73 -> 10000
size cluster i: 74 -> 10000
size cluster i: 75 -> 10002
size cluster i: 76 -> 9775
size cluster i: 77 -> 10000
size cluster i: 78 -> 9773
size cluster i: 79 -> 10001
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 8636
size cluster i: 83 -> 2375
size cluster i: 84 -> 71
size cluster i: 85 -> 24
size cluster i: 86 -> 57
size cluster i: 87 -> 2
size cluster i: 88 -> 3
size cluster i: 89 -> 4
size cluster i: 90 -> 12
size cluster i: 91 -> 2
size cluster i: 92 -> 4
size cluster i: 93 -> 6
size cluster i: 94 -> 2
size cluster i: 95 -> 3
size cluster i: 99 -> 2
datapoints in clusters: 895482
score: 86.0365


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:48:40 2018
elapsed time: 35.1064s


Finishing: 
---------- 
Beginning cleanup...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 5: Exiting... 
Node 6: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:55906] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:55906] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 102
datapoints_clusters: 895482
score: 86.0365
elapsed_time: 35.1064

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 288.0658436213992 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[61481,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 288.066


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1423s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5365s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.06159s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 35045
find clusters duration: 0.258944s
detected clusters: 111
size cluster i: 0 -> 10001
size cluster i: 1 -> 9854
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9944
size cluster i: 8 -> 9992
size cluster i: 9 -> 9994
size cluster i: 10 -> 10002
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10001
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 7378
size cluster i: 25 -> 9841
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 10002
size cluster i: 29 -> 9668
size cluster i: 30 -> 8381
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 9916
size cluster i: 34 -> 10003
size cluster i: 35 -> 9999
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9605
size cluster i: 41 -> 19999
size cluster i: 42 -> 9919
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 30005
size cluster i: 47 -> 9829
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 9930
size cluster i: 51 -> 10000
size cluster i: 52 -> 9288
size cluster i: 53 -> 5215
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 20007
size cluster i: 61 -> 10003
size cluster i: 62 -> 5965
size cluster i: 63 -> 10001
size cluster i: 64 -> 10002
size cluster i: 65 -> 10000
size cluster i: 66 -> 10001
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 9973
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 8060
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 10002
size cluster i: 77 -> 9712
size cluster i: 78 -> 10000
size cluster i: 79 -> 9718
size cluster i: 80 -> 10001
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 8382
size cluster i: 84 -> 2032
size cluster i: 85 -> 29
size cluster i: 86 -> 8
size cluster i: 87 -> 4
size cluster i: 88 -> 2
size cluster i: 89 -> 17
size cluster i: 90 -> 2
size cluster i: 91 -> 15
size cluster i: 92 -> 3
size cluster i: 93 -> 2
size cluster i: 94 -> 3
size cluster i: 96 -> 4
size cluster i: 97 -> 5
size cluster i: 98 -> 2
size cluster i: 100 -> 2
size cluster i: 102 -> 7
size cluster i: 103 -> 2
size cluster i: 104 -> 2
size cluster i: 105 -> 2
size cluster i: 107 -> 3
datapoints in clusters: 892783
score: 77.8997


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:49:25 2018
elapsed time: 35.3539s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:56211] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:56211] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 111
datapoints_clusters: 892783
score: 77.8997
elapsed_time: 35.3539

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 293.5528120713306 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[63348,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 293.553


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1863s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5611s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.95559s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 34990
find clusters duration: 0.25944s
detected clusters: 105
size cluster i: 0 -> 10001
size cluster i: 1 -> 9811
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9919
size cluster i: 8 -> 9990
size cluster i: 9 -> 9993
size cluster i: 10 -> 10001
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10001
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 7055
size cluster i: 25 -> 9802
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 10002
size cluster i: 29 -> 9585
size cluster i: 30 -> 8124
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 9893
size cluster i: 34 -> 10003
size cluster i: 35 -> 9999
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9498
size cluster i: 41 -> 19998
size cluster i: 42 -> 9891
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 10002
size cluster i: 47 -> 9787
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 9911
size cluster i: 51 -> 20002
size cluster i: 52 -> 9999
size cluster i: 53 -> 9142
size cluster i: 54 -> 4869
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 20006
size cluster i: 62 -> 10003
size cluster i: 63 -> 5632
size cluster i: 64 -> 10001
size cluster i: 65 -> 10002
size cluster i: 66 -> 10000
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 9958
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 7823
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10001
size cluster i: 78 -> 9612
size cluster i: 79 -> 10000
size cluster i: 80 -> 9661
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 8139
size cluster i: 85 -> 1735
size cluster i: 86 -> 2
size cluster i: 87 -> 18
size cluster i: 88 -> 7
size cluster i: 89 -> 3
size cluster i: 90 -> 9
size cluster i: 91 -> 3
size cluster i: 92 -> 5
size cluster i: 93 -> 3
size cluster i: 94 -> 3
size cluster i: 95 -> 2
size cluster i: 96 -> 3
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 100 -> 4
size cluster i: 101 -> 3
datapoints in clusters: 889963
score: 82.8887


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:50:10 2018
elapsed time: 35.3045s


Finishing: 
---------- 
Beginning cleanup...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 5: Exiting... 
Node 8: Exiting... 
Node 7: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:56526] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:56526] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 105
datapoints_clusters: 889963
score: 82.8887
elapsed_time: 35.3045

--------------- END RUN -----------------
-> new best_score:0.8331294117647059new best_threshold:293.552812071

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 299.039780521262 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[63043,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 299.04


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1992s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5731s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.97028s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 34861
find clusters duration: 0.256326s
detected clusters: 101
size cluster i: 0 -> 10001
size cluster i: 1 -> 9754
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9898
size cluster i: 8 -> 9989
size cluster i: 9 -> 9990
size cluster i: 10 -> 10001
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 6762
size cluster i: 25 -> 9748
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 10002
size cluster i: 29 -> 9504
size cluster i: 30 -> 7847
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 9852
size cluster i: 34 -> 10003
size cluster i: 35 -> 9998
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9358
size cluster i: 41 -> 19997
size cluster i: 42 -> 9862
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 10002
size cluster i: 47 -> 9740
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 9881
size cluster i: 51 -> 20002
size cluster i: 52 -> 9999
size cluster i: 53 -> 8966
size cluster i: 54 -> 4521
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 20006
size cluster i: 62 -> 10003
size cluster i: 63 -> 5323
size cluster i: 64 -> 10001
size cluster i: 65 -> 10002
size cluster i: 66 -> 10000
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10000
size cluster i: 70 -> 9945
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 7526
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10001
size cluster i: 78 -> 9523
size cluster i: 79 -> 10000
size cluster i: 80 -> 9583
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 7883
size cluster i: 85 -> 1444
size cluster i: 86 -> 2
size cluster i: 87 -> 3
size cluster i: 88 -> 7
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 11
size cluster i: 94 -> 3
size cluster i: 97 -> 2
datapoints in clusters: 886994
score: 86.0906


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:50:55 2018
elapsed time: 35.3375s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Beginning cleanup...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 1: Exiting... 
Node 2: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3Warning: Node  is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:56825] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:56825] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 101
datapoints_clusters: 886994
score: 86.0906
elapsed_time: 35.3375

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 304.5267489711934 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[62594,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 304.527


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2192s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5624s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.78874s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 34742
find clusters duration: 0.263144s
detected clusters: 97
size cluster i: 0 -> 10001
size cluster i: 1 -> 9695
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9858
size cluster i: 8 -> 9979
size cluster i: 9 -> 9988
size cluster i: 10 -> 10001
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 6444
size cluster i: 25 -> 10000
size cluster i: 26 -> 10000
size cluster i: 27 -> 10002
size cluster i: 28 -> 9384
size cluster i: 29 -> 7562
size cluster i: 30 -> 10001
size cluster i: 31 -> 10001
size cluster i: 32 -> 9816
size cluster i: 33 -> 10002
size cluster i: 34 -> 9996
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10000
size cluster i: 38 -> 10001
size cluster i: 39 -> 9223
size cluster i: 40 -> 19993
size cluster i: 41 -> 9824
size cluster i: 42 -> 10001
size cluster i: 43 -> 10000
size cluster i: 44 -> 10000
size cluster i: 45 -> 10002
size cluster i: 46 -> 9672
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 9838
size cluster i: 50 -> 20002
size cluster i: 51 -> 9998
size cluster i: 52 -> 8798
size cluster i: 53 -> 4192
size cluster i: 54 -> 10000
size cluster i: 55 -> 10001
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 20006
size cluster i: 61 -> 10003
size cluster i: 62 -> 9690
size cluster i: 63 -> 5023
size cluster i: 64 -> 10001
size cluster i: 65 -> 10002
size cluster i: 66 -> 10000
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10000
size cluster i: 70 -> 9920
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 7223
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10001
size cluster i: 78 -> 9405
size cluster i: 79 -> 10000
size cluster i: 80 -> 9518
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 7601
size cluster i: 85 -> 1193
size cluster i: 86 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 2
size cluster i: 95 -> 2
datapoints in clusters: 883911
score: 84.0582


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:51:40 2018
elapsed time: 35.1597s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 6: Exiting... 
Node 8: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node Warning: Node Warning: Node Warning: Node Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:57144] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:57144] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 883911
score: 84.0582
elapsed_time: 35.1597

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 310.01371742112485 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[52171,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 310.014


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.123s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5455s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.11388s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 34700
find clusters duration: 0.259632s
detected clusters: 93
size cluster i: 0 -> 10001
size cluster i: 1 -> 9623
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9820
size cluster i: 8 -> 9974
size cluster i: 9 -> 9987
size cluster i: 10 -> 10001
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 6113
size cluster i: 25 -> 10000
size cluster i: 26 -> 10000
size cluster i: 27 -> 10002
size cluster i: 28 -> 9279
size cluster i: 29 -> 7285
size cluster i: 30 -> 10001
size cluster i: 31 -> 10001
size cluster i: 32 -> 9757
size cluster i: 33 -> 10002
size cluster i: 34 -> 9993
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10000
size cluster i: 38 -> 10001
size cluster i: 39 -> 9057
size cluster i: 40 -> 19990
size cluster i: 41 -> 9786
size cluster i: 42 -> 10001
size cluster i: 43 -> 10000
size cluster i: 44 -> 10000
size cluster i: 45 -> 10002
size cluster i: 46 -> 9600
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 9796
size cluster i: 50 -> 20002
size cluster i: 51 -> 9996
size cluster i: 52 -> 8620
size cluster i: 53 -> 3850
size cluster i: 54 -> 10000
size cluster i: 55 -> 10001
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 20006
size cluster i: 61 -> 10003
size cluster i: 62 -> 9611
size cluster i: 63 -> 10001
size cluster i: 64 -> 10002
size cluster i: 65 -> 10000
size cluster i: 66 -> 10001
size cluster i: 67 -> 10002
size cluster i: 68 -> 10000
size cluster i: 69 -> 9894
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 6946
size cluster i: 74 -> 4712
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10001
size cluster i: 78 -> 9288
size cluster i: 79 -> 10000
size cluster i: 80 -> 9430
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 7269
size cluster i: 85 -> 962
size cluster i: 88 -> 2
size cluster i: 89 -> 3
size cluster i: 91 -> 2
datapoints in clusters: 880709
score: 80.2999


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:52:26 2018
elapsed time: 35.3979s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:57457] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:57457] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 93
datapoints_clusters: 880709
score: 80.2999
elapsed_time: 35.3979

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 315.50068587105625 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[51739,1],1]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 315.501


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0881s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5506s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.78646s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 34529
find clusters duration: 0.256765s
detected clusters: 97
size cluster i: 0 -> 10001
size cluster i: 1 -> 9548
size cluster i: 2 -> 10000
size cluster i: 3 -> 10001
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9772
size cluster i: 8 -> 9963
size cluster i: 9 -> 9982
size cluster i: 10 -> 10001
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10003
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 5770
size cluster i: 25 -> 10000
size cluster i: 26 -> 9997
size cluster i: 27 -> 10001
size cluster i: 28 -> 9161
size cluster i: 29 -> 7007
size cluster i: 30 -> 10001
size cluster i: 31 -> 10001
size cluster i: 32 -> 9698
size cluster i: 33 -> 10002
size cluster i: 34 -> 9991
size cluster i: 35 -> 10002
size cluster i: 36 -> 10000
size cluster i: 37 -> 10000
size cluster i: 38 -> 10001
size cluster i: 39 -> 8870
size cluster i: 40 -> 19989
size cluster i: 41 -> 9740
size cluster i: 42 -> 10001
size cluster i: 43 -> 10000
size cluster i: 44 -> 10000
size cluster i: 45 -> 10002
size cluster i: 46 -> 9526
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 9726
size cluster i: 50 -> 20002
size cluster i: 51 -> 9990
size cluster i: 52 -> 8420
size cluster i: 53 -> 3515
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 20006
size cluster i: 61 -> 10003
size cluster i: 62 -> 9528
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10000
size cluster i: 66 -> 10001
size cluster i: 67 -> 10001
size cluster i: 68 -> 10000
size cluster i: 69 -> 9864
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 6633
size cluster i: 74 -> 4359
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10001
size cluster i: 78 -> 9125
size cluster i: 79 -> 10000
size cluster i: 80 -> 9344
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 6948
size cluster i: 85 -> 769
size cluster i: 86 -> 3
size cluster i: 87 -> 2
size cluster i: 88 -> 2
size cluster i: 89 -> 2
size cluster i: 92 -> 3
datapoints in clusters: 877305
score: 83.43


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:53:11 2018
elapsed time: 35.0455s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:57761] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:57761] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 877305
score: 83.43
elapsed_time: 35.0455

--------------- END RUN -----------------
thresholds:[ 289.28516994  290.50449627  291.72382259  292.94314891  294.16247523
  295.38180155  296.60112788  297.8204542 ]threshold_step:1.21932632221

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 289.2851699436062 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[51547,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 289.285


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1509s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5099s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.95718s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 35030
find clusters duration: 0.259297s
detected clusters: 110
size cluster i: 0 -> 10001
size cluster i: 1 -> 9848
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9938
size cluster i: 8 -> 9992
size cluster i: 9 -> 9993
size cluster i: 10 -> 10002
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10001
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 7313
size cluster i: 25 -> 9832
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 10002
size cluster i: 29 -> 9650
size cluster i: 30 -> 8329
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 9907
size cluster i: 34 -> 10003
size cluster i: 35 -> 9999
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9589
size cluster i: 41 -> 19999
size cluster i: 42 -> 9913
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 30005
size cluster i: 47 -> 9819
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 9927
size cluster i: 51 -> 10000
size cluster i: 52 -> 9268
size cluster i: 53 -> 5133
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 20007
size cluster i: 61 -> 10003
size cluster i: 62 -> 5892
size cluster i: 63 -> 10001
size cluster i: 64 -> 10002
size cluster i: 65 -> 10000
size cluster i: 66 -> 10001
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 9969
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 8012
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 10002
size cluster i: 77 -> 9689
size cluster i: 78 -> 10000
size cluster i: 79 -> 9707
size cluster i: 80 -> 10001
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 8324
size cluster i: 84 -> 1950
size cluster i: 85 -> 4
size cluster i: 86 -> 28
size cluster i: 87 -> 8
size cluster i: 88 -> 4
size cluster i: 89 -> 2
size cluster i: 90 -> 16
size cluster i: 91 -> 2
size cluster i: 92 -> 14
size cluster i: 93 -> 2
size cluster i: 94 -> 3
size cluster i: 95 -> 2
size cluster i: 96 -> 3
size cluster i: 98 -> 4
size cluster i: 99 -> 4
size cluster i: 100 -> 2
size cluster i: 102 -> 2
size cluster i: 103 -> 7
size cluster i: 104 -> 2
size cluster i: 107 -> 3
datapoints in clusters: 892178
score: 78.7216


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:53:56 2018
elapsed time: 35.1979s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:58081] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:58081] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 110
datapoints_clusters: 892178
score: 78.7216
elapsed_time: 35.1979

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 290.50449626581315 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[53157,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 290.504


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.3442s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5899s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.94245s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 35032
find clusters duration: 0.263067s
detected clusters: 106
size cluster i: 0 -> 10001
size cluster i: 1 -> 9840
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9933
size cluster i: 8 -> 9992
size cluster i: 9 -> 9993
size cluster i: 10 -> 10002
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10001
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 7246
size cluster i: 25 -> 9825
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 10002
size cluster i: 29 -> 9626
size cluster i: 30 -> 8269
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 9902
size cluster i: 34 -> 10003
size cluster i: 35 -> 9999
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9571
size cluster i: 41 -> 19999
size cluster i: 42 -> 9906
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 30005
size cluster i: 47 -> 9810
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 9923
size cluster i: 51 -> 10000
size cluster i: 52 -> 9229
size cluster i: 53 -> 5060
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 20007
size cluster i: 61 -> 10003
size cluster i: 62 -> 5821
size cluster i: 63 -> 10001
size cluster i: 64 -> 10002
size cluster i: 65 -> 10000
size cluster i: 66 -> 10001
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 9968
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 7960
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 10002
size cluster i: 77 -> 9661
size cluster i: 78 -> 10000
size cluster i: 79 -> 9695
size cluster i: 80 -> 10001
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 8269
size cluster i: 84 -> 1876
size cluster i: 85 -> 3
size cluster i: 86 -> 22
size cluster i: 87 -> 8
size cluster i: 88 -> 4
size cluster i: 89 -> 15
size cluster i: 90 -> 12
size cluster i: 91 -> 3
size cluster i: 92 -> 3
size cluster i: 93 -> 4
size cluster i: 94 -> 3
size cluster i: 95 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 100 -> 4
size cluster i: 101 -> 3
datapoints in clusters: 891537
score: 82.1613


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:54:41 2018
elapsed time: 35.4693s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 1: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:58399] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:58399] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 106
datapoints_clusters: 891537
score: 82.1613
elapsed_time: 35.4693

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 291.72382258802014 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[52960,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 291.724


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0996s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.54s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.10977s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 35014
find clusters duration: 0.258221s
detected clusters: 108
size cluster i: 0 -> 10001
size cluster i: 1 -> 9834
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9928
size cluster i: 8 -> 9992
size cluster i: 9 -> 9993
size cluster i: 10 -> 10002
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10001
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 7172
size cluster i: 25 -> 9817
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 10002
size cluster i: 29 -> 9612
size cluster i: 30 -> 8200
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 9897
size cluster i: 34 -> 10003
size cluster i: 35 -> 9999
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9542
size cluster i: 41 -> 19998
size cluster i: 42 -> 9903
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 10002
size cluster i: 47 -> 9806
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 9916
size cluster i: 51 -> 20002
size cluster i: 52 -> 9999
size cluster i: 53 -> 9191
size cluster i: 54 -> 4985
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 20006
size cluster i: 62 -> 10003
size cluster i: 63 -> 5754
size cluster i: 64 -> 10001
size cluster i: 65 -> 10002
size cluster i: 66 -> 10000
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 9964
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 7903
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10002
size cluster i: 78 -> 9643
size cluster i: 79 -> 10000
size cluster i: 80 -> 9683
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 8221
size cluster i: 85 -> 1819
size cluster i: 86 -> 2
size cluster i: 87 -> 19
size cluster i: 88 -> 8
size cluster i: 89 -> 4
size cluster i: 90 -> 12
size cluster i: 91 -> 3
size cluster i: 92 -> 12
size cluster i: 93 -> 3
size cluster i: 94 -> 3
size cluster i: 95 -> 2
size cluster i: 96 -> 3
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 101 -> 4
size cluster i: 102 -> 3
size cluster i: 104 -> 2
datapoints in clusters: 890928
score: 80.3582


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:55:26 2018
elapsed time: 35.3588s


Finishing: 
---------- 
Beginning cleanup...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config! is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:58714] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:58714] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 108
datapoints_clusters: 890928
score: 80.3582
elapsed_time: 35.3588

--------------- END RUN -----------------
-> new best_score:0.8340588235294117new best_threshold:291.723822588

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 292.94314891022714 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[52512,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 292.943


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1197s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.6076s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.84494s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 35002
find clusters duration: 0.260665s
detected clusters: 107
size cluster i: 0 -> 10001
size cluster i: 1 -> 9823
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9925
size cluster i: 8 -> 9991
size cluster i: 9 -> 9993
size cluster i: 10 -> 10001
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10001
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 7096
size cluster i: 25 -> 9807
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 10002
size cluster i: 29 -> 9597
size cluster i: 30 -> 8154
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 9894
size cluster i: 34 -> 10003
size cluster i: 35 -> 9999
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9515
size cluster i: 41 -> 19998
size cluster i: 42 -> 9894
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 10002
size cluster i: 47 -> 9795
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 9914
size cluster i: 51 -> 20002
size cluster i: 52 -> 9999
size cluster i: 53 -> 9153
size cluster i: 54 -> 4912
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 20006
size cluster i: 62 -> 10003
size cluster i: 63 -> 5669
size cluster i: 64 -> 10001
size cluster i: 65 -> 10002
size cluster i: 66 -> 10000
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 9960
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 7848
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10002
size cluster i: 78 -> 9624
size cluster i: 79 -> 10000
size cluster i: 80 -> 9666
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 8167
size cluster i: 85 -> 1766
size cluster i: 86 -> 2
size cluster i: 87 -> 19
size cluster i: 88 -> 8
size cluster i: 89 -> 4
size cluster i: 90 -> 10
size cluster i: 91 -> 3
size cluster i: 92 -> 7
size cluster i: 93 -> 3
size cluster i: 94 -> 3
size cluster i: 95 -> 3
size cluster i: 96 -> 2
size cluster i: 97 -> 3
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 101 -> 4
size cluster i: 102 -> 3
size cluster i: 104 -> 2
datapoints in clusters: 890308
score: 81.1751


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:56:11 2018
elapsed time: 35.1795s


Finishing: 
---------- 
Node 1 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:59034] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:59034] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 107
datapoints_clusters: 890308
score: 81.1751
elapsed_time: 35.1795

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 294.1624752324341 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[52331,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 294.162


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2154s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.538s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.84486s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 34971
find clusters duration: 0.260415s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 9807
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9919
size cluster i: 8 -> 9990
size cluster i: 9 -> 9993
size cluster i: 10 -> 10001
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10001
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 7022
size cluster i: 25 -> 9795
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 10002
size cluster i: 29 -> 9579
size cluster i: 30 -> 8096
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 9891
size cluster i: 34 -> 10003
size cluster i: 35 -> 9999
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9480
size cluster i: 41 -> 19998
size cluster i: 42 -> 9888
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 10002
size cluster i: 47 -> 9781
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 9910
size cluster i: 51 -> 20002
size cluster i: 52 -> 9999
size cluster i: 53 -> 9125
size cluster i: 54 -> 4837
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 20006
size cluster i: 62 -> 10003
size cluster i: 63 -> 5593
size cluster i: 64 -> 10001
size cluster i: 65 -> 10002
size cluster i: 66 -> 10000
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 9957
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 7797
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10001
size cluster i: 78 -> 9599
size cluster i: 79 -> 10000
size cluster i: 80 -> 9653
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 8107
size cluster i: 85 -> 1705
size cluster i: 86 -> 18
size cluster i: 87 -> 7
size cluster i: 88 -> 3
size cluster i: 89 -> 2
size cluster i: 90 -> 8
size cluster i: 91 -> 3
size cluster i: 92 -> 5
size cluster i: 93 -> 3
size cluster i: 94 -> 3
size cluster i: 95 -> 2
size cluster i: 96 -> 3
size cluster i: 97 -> 2
size cluster i: 99 -> 4
size cluster i: 100 -> 3
datapoints in clusters: 889654
score: 83.7321


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:56:57 2018
elapsed time: 35.1978s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Beginning cleanup...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:59345] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:59345] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 889654
score: 83.7321
elapsed_time: 35.1978

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 295.3818015546411 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[49849,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 295.382


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2703s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5391s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.82563s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 34934
find clusters duration: 0.264425s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 9791
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9911
size cluster i: 8 -> 9990
size cluster i: 9 -> 9993
size cluster i: 10 -> 10001
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10001
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 6958
size cluster i: 25 -> 9781
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 10002
size cluster i: 29 -> 9565
size cluster i: 30 -> 8041
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 9879
size cluster i: 34 -> 10003
size cluster i: 35 -> 9998
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9452
size cluster i: 41 -> 19998
size cluster i: 42 -> 9884
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 10002
size cluster i: 47 -> 9772
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 9902
size cluster i: 51 -> 20002
size cluster i: 52 -> 9999
size cluster i: 53 -> 9091
size cluster i: 54 -> 4759
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 20006
size cluster i: 62 -> 10003
size cluster i: 63 -> 5517
size cluster i: 64 -> 10001
size cluster i: 65 -> 10002
size cluster i: 66 -> 10000
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 9954
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 7736
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10001
size cluster i: 78 -> 9576
size cluster i: 79 -> 10000
size cluster i: 80 -> 9633
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 8063
size cluster i: 85 -> 1640
size cluster i: 86 -> 17
size cluster i: 87 -> 3
size cluster i: 88 -> 3
size cluster i: 89 -> 7
size cluster i: 90 -> 2
size cluster i: 91 -> 3
size cluster i: 92 -> 3
size cluster i: 93 -> 3
size cluster i: 94 -> 3
size cluster i: 95 -> 2
size cluster i: 96 -> 3
size cluster i: 98 -> 3
size cluster i: 99 -> 3
datapoints in clusters: 889007
score: 83.6712


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:57:42 2018
elapsed time: 35.234s


Finishing: 
---------- 
Beginning cleanup...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 7: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:59651] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:59651] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 889007
score: 83.6712
elapsed_time: 35.234

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 296.60112787684807 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[49539,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 296.601


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1647s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5578s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.81879s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 34914
find clusters duration: 0.26915s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 9775
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9907
size cluster i: 8 -> 9990
size cluster i: 9 -> 9992
size cluster i: 10 -> 10001
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10001
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 6890
size cluster i: 25 -> 9773
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 10002
size cluster i: 29 -> 9544
size cluster i: 30 -> 7978
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 9866
size cluster i: 34 -> 10003
size cluster i: 35 -> 9998
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9420
size cluster i: 41 -> 19997
size cluster i: 42 -> 9875
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 10002
size cluster i: 47 -> 9762
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 9896
size cluster i: 51 -> 20002
size cluster i: 52 -> 9999
size cluster i: 53 -> 9049
size cluster i: 54 -> 4672
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 20006
size cluster i: 62 -> 10003
size cluster i: 63 -> 5453
size cluster i: 64 -> 10001
size cluster i: 65 -> 10002
size cluster i: 66 -> 10000
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 9952
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 7660
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10001
size cluster i: 78 -> 9557
size cluster i: 79 -> 10000
size cluster i: 80 -> 9616
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 8004
size cluster i: 85 -> 1559
size cluster i: 86 -> 17
size cluster i: 87 -> 2
size cluster i: 88 -> 3
size cluster i: 89 -> 7
size cluster i: 90 -> 3
size cluster i: 91 -> 3
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 3
size cluster i: 99 -> 3
size cluster i: 100 -> 3
datapoints in clusters: 888305
score: 83.6052


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:58:27 2018
elapsed time: 35.1405s


Finishing: 
---------- 
Node 4 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 1: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:59961] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:59961] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 888305
score: 83.6052
elapsed_time: 35.1405

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 297.820454199055 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[49206,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 297.82


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2167s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5165s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.91437s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 34895
find clusters duration: 0.267503s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 9761
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9904
size cluster i: 8 -> 9989
size cluster i: 9 -> 9991
size cluster i: 10 -> 10001
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 6817
size cluster i: 25 -> 9759
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 10002
size cluster i: 29 -> 9529
size cluster i: 30 -> 7915
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 9862
size cluster i: 34 -> 10003
size cluster i: 35 -> 9998
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9392
size cluster i: 41 -> 19997
size cluster i: 42 -> 9868
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 10002
size cluster i: 47 -> 9754
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 9888
size cluster i: 51 -> 20002
size cluster i: 52 -> 9999
size cluster i: 53 -> 9002
size cluster i: 54 -> 4593
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 20006
size cluster i: 62 -> 10003
size cluster i: 63 -> 5401
size cluster i: 64 -> 10001
size cluster i: 65 -> 10002
size cluster i: 66 -> 10000
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 9946
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 7596
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10001
size cluster i: 78 -> 9546
size cluster i: 79 -> 10000
size cluster i: 80 -> 9601
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 7942
size cluster i: 85 -> 1490
size cluster i: 86 -> 2
size cluster i: 87 -> 3
size cluster i: 88 -> 7
size cluster i: 89 -> 3
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 92 -> 2
size cluster i: 93 -> 13
size cluster i: 96 -> 3
size cluster i: 98 -> 3
size cluster i: 99 -> 3
datapoints in clusters: 887653
score: 83.5438


Runtimes: 
--------- 
finished computation at Fri Dec 28 12:59:12 2018
elapsed time: 35.2443s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:60300] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:60300] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 887653
score: 83.5438
elapsed_time: 35.2443

--------------- END RUN -----------------
thresholds:[0.0, 111.11111111111111, 222.22222222222223, 333.33333333333337, 444.44444444444446, 555.55555555555554, 666.66666666666674, 777.77777777777783, 888.88888888888891, 1000.0]threshold_step:111.111111111

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 0.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[50956,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 0


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.379s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.86134s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37747
find clusters duration: 63.2508s
detected clusters: 1
size cluster i: 0 -> 1020000
datapoints in clusters: 1020000
score: 1


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:01:20 2018
elapsed time: 117.916s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 7 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 3: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:60598] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:60598] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 1
datapoints_clusters: 1020000
score: 1.0
elapsed_time: 117.916

--------------- END RUN -----------------
-> new best_score:0.00980392156862745new best_threshold:0.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 111.11111111111111 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[50611,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 111.111


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0639s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.3346s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.8208s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39414
find clusters duration: 8.23658s
detected clusters: 129
size cluster i: 0 -> 681222
size cluster i: 1 -> 10015
size cluster i: 2 -> 20033
size cluster i: 3 -> 100147
size cluster i: 4 -> 10006
size cluster i: 5 -> 10020
size cluster i: 6 -> 30062
size cluster i: 7 -> 10010
size cluster i: 8 -> 10007
size cluster i: 9 -> 10011
size cluster i: 10 -> 10005
size cluster i: 11 -> 10005
size cluster i: 12 -> 20029
size cluster i: 13 -> 10009
size cluster i: 14 -> 10012
size cluster i: 15 -> 10019
size cluster i: 16 -> 10001
size cluster i: 17 -> 10002
size cluster i: 18 -> 10006
size cluster i: 19 -> 10003
size cluster i: 20 -> 2
size cluster i: 21 -> 4
size cluster i: 22 -> 4
size cluster i: 23 -> 4
size cluster i: 24 -> 2
size cluster i: 25 -> 2
size cluster i: 26 -> 5
size cluster i: 27 -> 3
size cluster i: 28 -> 2
size cluster i: 29 -> 2
size cluster i: 30 -> 4
size cluster i: 31 -> 2
size cluster i: 32 -> 4
size cluster i: 33 -> 2
size cluster i: 35 -> 3
size cluster i: 36 -> 2
size cluster i: 37 -> 3
size cluster i: 38 -> 2
size cluster i: 39 -> 3
size cluster i: 40 -> 2
size cluster i: 41 -> 5
size cluster i: 42 -> 3
size cluster i: 43 -> 3
size cluster i: 44 -> 2
size cluster i: 45 -> 2
size cluster i: 46 -> 6
size cluster i: 47 -> 5
size cluster i: 48 -> 3
size cluster i: 49 -> 3
size cluster i: 50 -> 2
size cluster i: 52 -> 7
size cluster i: 53 -> 4
size cluster i: 55 -> 3
size cluster i: 56 -> 3
size cluster i: 57 -> 3
size cluster i: 58 -> 2
size cluster i: 60 -> 4
size cluster i: 61 -> 3
size cluster i: 62 -> 6
size cluster i: 64 -> 2
size cluster i: 65 -> 3
size cluster i: 66 -> 2
size cluster i: 68 -> 3
size cluster i: 69 -> 3
size cluster i: 70 -> 2
size cluster i: 71 -> 2
size cluster i: 72 -> 2
size cluster i: 73 -> 3
size cluster i: 74 -> 2
size cluster i: 77 -> 3
size cluster i: 79 -> 2
size cluster i: 80 -> 2
size cluster i: 82 -> 4
size cluster i: 85 -> 2
size cluster i: 86 -> 2
size cluster i: 87 -> 3
size cluster i: 89 -> 3
size cluster i: 90 -> 2
size cluster i: 91 -> 4
size cluster i: 92 -> 3
size cluster i: 93 -> 2
size cluster i: 94 -> 3
size cluster i: 95 -> 2
size cluster i: 96 -> 3
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 2
size cluster i: 103 -> 2
size cluster i: 106 -> 2
size cluster i: 109 -> 2
size cluster i: 110 -> 2
size cluster i: 111 -> 2
size cluster i: 112 -> 3
size cluster i: 120 -> 2
size cluster i: 123 -> 2
datapoints in clusters: 1001872
score: 69.7381


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:02:33 2018
elapsed time: 62.7862s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:60937] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:60937] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 129
datapoints_clusters: 1001872
score: 69.7381
elapsed_time: 62.7862

--------------- END RUN -----------------
-> new best_score:0.21385098039215686new best_threshold:111.111111111

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 222.22222222222223 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[50430,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 222.222


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1941s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.3909s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.88629s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39014
find clusters duration: 0.435776s
detected clusters: 90
size cluster i: 0 -> 120091
size cluster i: 1 -> 10004
size cluster i: 2 -> 20017
size cluster i: 3 -> 30026
size cluster i: 4 -> 10000
size cluster i: 5 -> 10006
size cluster i: 6 -> 50046
size cluster i: 7 -> 10007
size cluster i: 8 -> 10004
size cluster i: 9 -> 10006
size cluster i: 10 -> 10007
size cluster i: 11 -> 10006
size cluster i: 12 -> 40022
size cluster i: 13 -> 140140
size cluster i: 14 -> 10004
size cluster i: 15 -> 10002
size cluster i: 16 -> 10012
size cluster i: 17 -> 20025
size cluster i: 18 -> 10002
size cluster i: 19 -> 10003
size cluster i: 20 -> 20019
size cluster i: 21 -> 10011
size cluster i: 22 -> 10007
size cluster i: 23 -> 10005
size cluster i: 24 -> 9999
size cluster i: 25 -> 30027
size cluster i: 26 -> 10008
size cluster i: 27 -> 10003
size cluster i: 28 -> 30035
size cluster i: 29 -> 20016
size cluster i: 30 -> 10001
size cluster i: 31 -> 10006
size cluster i: 32 -> 10009
size cluster i: 33 -> 10002
size cluster i: 34 -> 10004
size cluster i: 35 -> 20016
size cluster i: 36 -> 10003
size cluster i: 37 -> 10005
size cluster i: 38 -> 10004
size cluster i: 39 -> 10003
size cluster i: 40 -> 10011
size cluster i: 41 -> 10003
size cluster i: 42 -> 10010
size cluster i: 43 -> 10007
size cluster i: 44 -> 10005
size cluster i: 45 -> 10009
size cluster i: 46 -> 10005
size cluster i: 47 -> 10005
size cluster i: 48 -> 10004
size cluster i: 49 -> 10005
size cluster i: 50 -> 10001
size cluster i: 51 -> 9960
size cluster i: 52 -> 10007
size cluster i: 53 -> 10007
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10003
size cluster i: 58 -> 3
size cluster i: 59 -> 2
size cluster i: 60 -> 2
size cluster i: 61 -> 2
size cluster i: 62 -> 3
size cluster i: 63 -> 3
size cluster i: 64 -> 4
size cluster i: 65 -> 2
size cluster i: 66 -> 5
size cluster i: 68 -> 2
size cluster i: 69 -> 2
size cluster i: 70 -> 2
size cluster i: 71 -> 2
size cluster i: 72 -> 2
size cluster i: 74 -> 3
size cluster i: 75 -> 3
size cluster i: 77 -> 2
size cluster i: 78 -> 2
size cluster i: 80 -> 2
size cluster i: 82 -> 3
size cluster i: 83 -> 2
size cluster i: 85 -> 2
size cluster i: 86 -> 2
size cluster i: 88 -> 2
size cluster i: 89 -> 2
datapoints in clusters: 1000723
score: 88.2991


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:03:38 2018
elapsed time: 55.2289s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:61252] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:61252] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 90
datapoints_clusters: 1000723
score: 88.2991
elapsed_time: 55.2289

--------------- END RUN -----------------
-> new best_score:0.5874441176470588new best_threshold:222.222222222

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 333.33333333333337 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[56117,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 333.333


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1817s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.3914s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.82298s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38748
find clusters duration: 0.319214s
detected clusters: 84
size cluster i: 0 -> 60039
size cluster i: 1 -> 9993
size cluster i: 2 -> 20008
size cluster i: 3 -> 30016
size cluster i: 4 -> 10000
size cluster i: 5 -> 10002
size cluster i: 6 -> 50028
size cluster i: 7 -> 10003
size cluster i: 8 -> 10002
size cluster i: 9 -> 10004
size cluster i: 10 -> 10001
size cluster i: 11 -> 10003
size cluster i: 12 -> 20011
size cluster i: 13 -> 50029
size cluster i: 14 -> 10002
size cluster i: 15 -> 10001
size cluster i: 16 -> 10006
size cluster i: 17 -> 10009
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10006
size cluster i: 21 -> 10002
size cluster i: 22 -> 10005
size cluster i: 23 -> 30021
size cluster i: 24 -> 10006
size cluster i: 25 -> 10004
size cluster i: 26 -> 10002
size cluster i: 27 -> 9620
size cluster i: 28 -> 20009
size cluster i: 29 -> 10005
size cluster i: 30 -> 10000
size cluster i: 31 -> 10003
size cluster i: 32 -> 30025
size cluster i: 33 -> 10003
size cluster i: 34 -> 10005
size cluster i: 35 -> 20011
size cluster i: 36 -> 9725
size cluster i: 37 -> 10003
size cluster i: 38 -> 10003
size cluster i: 39 -> 9833
size cluster i: 40 -> 10002
size cluster i: 41 -> 20011
size cluster i: 42 -> 10002
size cluster i: 43 -> 10003
size cluster i: 44 -> 10003
size cluster i: 45 -> 10004
size cluster i: 46 -> 10001
size cluster i: 47 -> 10002
size cluster i: 48 -> 10005
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10005
size cluster i: 54 -> 10002
size cluster i: 55 -> 10005
size cluster i: 56 -> 10004
size cluster i: 57 -> 20010
size cluster i: 58 -> 10004
size cluster i: 59 -> 10005
size cluster i: 60 -> 10000
size cluster i: 61 -> 10005
size cluster i: 62 -> 10003
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10003
size cluster i: 66 -> 10005
size cluster i: 67 -> 10001
size cluster i: 68 -> 8233
size cluster i: 69 -> 10000
size cluster i: 70 -> 10002
size cluster i: 71 -> 10000
size cluster i: 72 -> 9998
size cluster i: 73 -> 9950
size cluster i: 74 -> 10001
size cluster i: 75 -> 3
size cluster i: 76 -> 2
size cluster i: 77 -> 2
size cluster i: 78 -> 2
size cluster i: 79 -> 3
size cluster i: 81 -> 2
datapoints in clusters: 997743
score: 82.1671


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:04:43 2018
elapsed time: 55.0525s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:61583] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:61583] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 TiWarning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
 and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 84
datapoints_clusters: 997743
score: 82.1671
elapsed_time: 55.0525

--------------- END RUN -----------------
-> new best_score:0.7519186274509804new best_threshold:333.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 444.44444444444446 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[55904,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 444.444


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2169s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4483s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.95921s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38453
find clusters duration: 0.29409s
detected clusters: 85
size cluster i: 0 -> 20010
size cluster i: 1 -> 20003
size cluster i: 2 -> 20008
size cluster i: 3 -> 10000
size cluster i: 4 -> 10002
size cluster i: 5 -> 30015
size cluster i: 6 -> 10001
size cluster i: 7 -> 9992
size cluster i: 8 -> 10003
size cluster i: 9 -> 10000
size cluster i: 10 -> 10001
size cluster i: 11 -> 10007
size cluster i: 12 -> 10003
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10003
size cluster i: 16 -> 40011
size cluster i: 17 -> 10007
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10002
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20008
size cluster i: 24 -> 10005
size cluster i: 25 -> 10002
size cluster i: 26 -> 10001
size cluster i: 27 -> 20008
size cluster i: 28 -> 10005
size cluster i: 29 -> 20004
size cluster i: 30 -> 10000
size cluster i: 31 -> 10003
size cluster i: 32 -> 10002
size cluster i: 33 -> 10002
size cluster i: 34 -> 10002
size cluster i: 35 -> 10006
size cluster i: 36 -> 10003
size cluster i: 37 -> 10003
size cluster i: 38 -> 10002
size cluster i: 39 -> 10002
size cluster i: 40 -> 8030
size cluster i: 41 -> 10002
size cluster i: 42 -> 20006
size cluster i: 43 -> 10001
size cluster i: 44 -> 10003
size cluster i: 45 -> 10003
size cluster i: 46 -> 10003
size cluster i: 47 -> 10001
size cluster i: 48 -> 30014
size cluster i: 49 -> 10001
size cluster i: 50 -> 10005
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10005
size cluster i: 56 -> 10003
size cluster i: 57 -> 10001
size cluster i: 58 -> 10004
size cluster i: 59 -> 10002
size cluster i: 60 -> 20010
size cluster i: 61 -> 20007
size cluster i: 62 -> 10003
size cluster i: 63 -> 10003
size cluster i: 64 -> 10000
size cluster i: 65 -> 7850
size cluster i: 66 -> 10001
size cluster i: 67 -> 10003
size cluster i: 68 -> 10000
size cluster i: 69 -> 10000
size cluster i: 70 -> 9728
size cluster i: 71 -> 10002
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 8117
size cluster i: 75 -> 10000
size cluster i: 76 -> 10001
size cluster i: 77 -> 10001
size cluster i: 78 -> 10000
size cluster i: 79 -> 4143
size cluster i: 80 -> 10002
size cluster i: 81 -> 9873
size cluster i: 82 -> 9198
size cluster i: 83 -> 10000
datapoints in clusters: 987167
score: 82.2639


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:05:48 2018
elapsed time: 55.2545s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7: Exiting... 
Node 1: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:61914] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:61914] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 85
datapoints_clusters: 987167
score: 82.2639
elapsed_time: 55.2545

--------------- END RUN -----------------
-> new best_score:0.8300892156862745new best_threshold:444.444444444

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 555.5555555555555 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[55450,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 555.556


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1963s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4215s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.88819s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37975
find clusters duration: 0.269391s
detected clusters: 98
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9778
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9995
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10003
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10006
size cluster i: 25 -> 10001
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10004
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10001
size cluster i: 34 -> 10002
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10005
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10001
size cluster i: 42 -> 4609
size cluster i: 43 -> 10001
size cluster i: 44 -> 20004
size cluster i: 45 -> 9999
size cluster i: 46 -> 10000
size cluster i: 47 -> 10002
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10002
size cluster i: 51 -> 10001
size cluster i: 52 -> 10004
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 20007
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10003
size cluster i: 59 -> 10003
size cluster i: 60 -> 10000
size cluster i: 61 -> 10002
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 20009
size cluster i: 65 -> 10003
size cluster i: 66 -> 10002
size cluster i: 67 -> 10002
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10000
size cluster i: 71 -> 10004
size cluster i: 72 -> 10001
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 8378
size cluster i: 77 -> 10002
size cluster i: 78 -> 5118
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10003
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10002
size cluster i: 88 -> 8969
size cluster i: 89 -> 5221
size cluster i: 90 -> 7256
size cluster i: 91 -> 10000
size cluster i: 92 -> 1192
size cluster i: 93 -> 2
size cluster i: 94 -> 3
datapoints in clusters: 970661
score: 93.2596


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:06:53 2018
elapsed time: 55.1041s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 2: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:62240] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:62240] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 970661
score: 93.2596
elapsed_time: 55.1041

--------------- END RUN -----------------
-> new best_score:0.9023264705882353new best_threshold:555.555555556

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 666.6666666666667 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[57332,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 666.667


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.3156s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.3972s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.93203s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37343
find clusters duration: 0.265915s
detected clusters: 103
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 8762
size cluster i: 8 -> 10001
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9938
size cluster i: 15 -> 10000
size cluster i: 16 -> 20003
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10002
size cluster i: 22 -> 10002
size cluster i: 23 -> 20005
size cluster i: 24 -> 10003
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10003
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 1743
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9902
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20003
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 9999
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 6077
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 9996
size cluster i: 88 -> 10001
size cluster i: 89 -> 6993
size cluster i: 90 -> 4713
size cluster i: 91 -> 10000
size cluster i: 92 -> 2574
size cluster i: 93 -> 2549
size cluster i: 94 -> 155
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 6
size cluster i: 98 -> 2
size cluster i: 99 -> 3
size cluster i: 101 -> 2
size cluster i: 102 -> 3
datapoints in clusters: 953514
score: 90.6773


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:07:58 2018
elapsed time: 55.2394s


Finishing: 
---------- 
Node 6 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:62542] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:62542] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 103
datapoints_clusters: 953514
score: 90.6773
elapsed_time: 55.2394

--------------- END RUN -----------------
-> new best_score:0.9052049019607843new best_threshold:666.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 777.7777777777778 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[56894,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 777.778


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2022s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4017s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.93451s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 36787
find clusters duration: 0.257409s
detected clusters: 119
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 9987
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10002
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9600
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 20003
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10000
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 9995
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10003
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10001
size cluster i: 43 -> 10000
size cluster i: 44 -> 20002
size cluster i: 45 -> 9404
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 20002
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10001
size cluster i: 59 -> 10001
size cluster i: 60 -> 10000
size cluster i: 61 -> 10001
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10003
size cluster i: 66 -> 10001
size cluster i: 67 -> 10000
size cluster i: 68 -> 10000
size cluster i: 69 -> 10001
size cluster i: 70 -> 9986
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 3500
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 9894
size cluster i: 87 -> 10000
size cluster i: 88 -> 7012
size cluster i: 89 -> 4585
size cluster i: 90 -> 2475
size cluster i: 91 -> 10000
size cluster i: 92 -> 371
size cluster i: 93 -> 922
size cluster i: 94 -> 982
size cluster i: 95 -> 3
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 3
size cluster i: 99 -> 4
size cluster i: 100 -> 2
size cluster i: 104 -> 2
size cluster i: 105 -> 2
size cluster i: 108 -> 2
datapoints in clusters: 938808
score: 74.5524


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:09:03 2018
elapsed time: 55.1407s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 2: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:62852] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:62852] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 119
datapoints_clusters: 938808
score: 74.5524
elapsed_time: 55.1407

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 888.8888888888889 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[56691,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 888.889


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1174s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4181s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.90408s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 36223
find clusters duration: 0.258356s
detected clusters: 130
size cluster i: 0 -> 10001
size cluster i: 1 -> 9999
size cluster i: 2 -> 10000
size cluster i: 3 -> 9915
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 8761
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 9997
size cluster i: 20 -> 10000
size cluster i: 21 -> 10000
size cluster i: 22 -> 10002
size cluster i: 23 -> 20003
size cluster i: 24 -> 10001
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10000
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 9924
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10002
size cluster i: 38 -> 10001
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10001
size cluster i: 43 -> 10000
size cluster i: 44 -> 20002
size cluster i: 45 -> 8189
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 9999
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 20001
size cluster i: 56 -> 10000
size cluster i: 57 -> 9986
size cluster i: 58 -> 10001
size cluster i: 59 -> 10000
size cluster i: 60 -> 10000
size cluster i: 61 -> 10000
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10004
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10000
size cluster i: 68 -> 10000
size cluster i: 69 -> 10001
size cluster i: 70 -> 9896
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10001
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 4694
size cluster i: 87 -> 9490
size cluster i: 88 -> 1573
size cluster i: 89 -> 973
size cluster i: 90 -> 10000
size cluster i: 91 -> 2498
size cluster i: 92 -> 263
size cluster i: 93 -> 2
size cluster i: 94 -> 5
size cluster i: 95 -> 7
size cluster i: 96 -> 3
size cluster i: 97 -> 2
size cluster i: 98 -> 3
size cluster i: 99 -> 191
size cluster i: 100 -> 2
size cluster i: 101 -> 3
size cluster i: 102 -> 4
size cluster i: 103 -> 2
size cluster i: 104 -> 2
size cluster i: 105 -> 4
size cluster i: 106 -> 2
size cluster i: 107 -> 2
size cluster i: 108 -> 12
size cluster i: 110 -> 2
size cluster i: 112 -> 2
size cluster i: 113 -> 2
size cluster i: 114 -> 2
size cluster i: 115 -> 2
size cluster i: 117 -> 2
size cluster i: 118 -> 2
size cluster i: 119 -> 5
size cluster i: 120 -> 3
size cluster i: 121 -> 2
size cluster i: 122 -> 4
size cluster i: 123 -> 2
size cluster i: 124 -> 2
size cluster i: 125 -> 2
size cluster i: 126 -> 2
datapoints in clusters: 926489
score: 63.5826


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:10:08 2018
elapsed time: 55.0423s


Finishing: 
---------- 
Node 7 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 8: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 1: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 43 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:63177] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:63177] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 130
datapoints_clusters: 926489
score: 63.5826
elapsed_time: 55.0423

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 1000.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[56390,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 1000


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0534s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4148s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.85689s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 35792
find clusters duration: 0.253012s
detected clusters: 124
size cluster i: 0 -> 10001
size cluster i: 1 -> 9996
size cluster i: 2 -> 10000
size cluster i: 3 -> 9606
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10000
size cluster i: 10 -> 9999
size cluster i: 11 -> 10001
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 10000
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10002
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 10000
size cluster i: 25 -> 10000
size cluster i: 26 -> 9991
size cluster i: 27 -> 10000
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 9994
size cluster i: 31 -> 9608
size cluster i: 32 -> 10000
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10002
size cluster i: 36 -> 10000
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10000
size cluster i: 40 -> 10001
size cluster i: 41 -> 9992
size cluster i: 42 -> 20002
size cluster i: 43 -> 10000
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 10000
size cluster i: 47 -> 10000
size cluster i: 48 -> 10000
size cluster i: 49 -> 9992
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 9999
size cluster i: 53 -> 20001
size cluster i: 54 -> 10000
size cluster i: 55 -> 9915
size cluster i: 56 -> 7509
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10000
size cluster i: 61 -> 10000
size cluster i: 62 -> 10001
size cluster i: 63 -> 10004
size cluster i: 64 -> 10001
size cluster i: 65 -> 6477
size cluster i: 66 -> 10001
size cluster i: 67 -> 10000
size cluster i: 68 -> 10001
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 9583
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 9983
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 2603
size cluster i: 89 -> 8593
size cluster i: 90 -> 342
size cluster i: 91 -> 9981
size cluster i: 92 -> 1027
size cluster i: 93 -> 562
size cluster i: 94 -> 10
size cluster i: 95 -> 2
size cluster i: 96 -> 26
size cluster i: 97 -> 18
size cluster i: 98 -> 2
size cluster i: 100 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 2
size cluster i: 103 -> 2
size cluster i: 105 -> 2
size cluster i: 106 -> 2
size cluster i: 107 -> 10
size cluster i: 110 -> 3
size cluster i: 112 -> 2
size cluster i: 113 -> 2
size cluster i: 114 -> 2
size cluster i: 115 -> 2
size cluster i: 116 -> 2
size cluster i: 117 -> 2
datapoints in clusters: 915888
score: 68.2426


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:11:13 2018
elapsed time: 54.9161s


Finishing: 
---------- 
Beginning cleanup...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:63484] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:63484] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 124
datapoints_clusters: 915888
score: 68.2426
elapsed_time: 54.9161

--------------- END RUN -----------------
thresholds:[ 580.24691358  604.9382716   629.62962963  654.32098765  679.01234568
  703.7037037   728.39506173  753.08641975]threshold_step:24.6913580247

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 580.246913580247 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[54008,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 580.247


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1124s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4723s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.89168s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37852
find clusters duration: 0.26529s
detected clusters: 101
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9645
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9993
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10003
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10006
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10001
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 3870
size cluster i: 43 -> 10001
size cluster i: 44 -> 20004
size cluster i: 45 -> 9994
size cluster i: 46 -> 10000
size cluster i: 47 -> 10002
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10002
size cluster i: 51 -> 10001
size cluster i: 52 -> 10004
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 20005
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10003
size cluster i: 59 -> 10003
size cluster i: 60 -> 10000
size cluster i: 61 -> 10002
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 20008
size cluster i: 65 -> 10003
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10000
size cluster i: 71 -> 10004
size cluster i: 72 -> 10001
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 7912
size cluster i: 77 -> 10002
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10003
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10002
size cluster i: 87 -> 8578
size cluster i: 88 -> 4503
size cluster i: 89 -> 4577
size cluster i: 90 -> 6699
size cluster i: 91 -> 10000
size cluster i: 92 -> 803
size cluster i: 93 -> 2
size cluster i: 94 -> 2
size cluster i: 95 -> 4
size cluster i: 96 -> 2
datapoints in clusters: 966712
score: 93.8279


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:12:18 2018
elapsed time: 55.0737s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 1: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:63810] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:63810] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 101
datapoints_clusters: 966712
score: 93.8279
elapsed_time: 55.0737

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 604.9382716049383 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[53558,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 604.938


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.141s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4588s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.8767s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37696
find clusters duration: 0.267143s
detected clusters: 101
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9463
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9988
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10002
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10005
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 3170
size cluster i: 43 -> 10001
size cluster i: 44 -> 20004
size cluster i: 45 -> 9983
size cluster i: 46 -> 10000
size cluster i: 47 -> 10002
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10002
size cluster i: 51 -> 10001
size cluster i: 52 -> 10004
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 20004
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10002
size cluster i: 59 -> 10002
size cluster i: 60 -> 10000
size cluster i: 61 -> 10002
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 20007
size cluster i: 65 -> 10003
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10000
size cluster i: 71 -> 10004
size cluster i: 72 -> 10001
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 7447
size cluster i: 77 -> 10002
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10003
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10002
size cluster i: 87 -> 8177
size cluster i: 88 -> 3887
size cluster i: 89 -> 3931
size cluster i: 90 -> 6161
size cluster i: 91 -> 10000
size cluster i: 92 -> 504
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 3
size cluster i: 99 -> 3
datapoints in clusters: 962841
score: 93.4522


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:13:23 2018
elapsed time: 55.0797s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!


Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:64140] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:64140] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 101
datapoints_clusters: 962841
score: 93.4522
elapsed_time: 55.0797

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 629.6296296296297 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[53356,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 629.63


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0828s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.5337s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.86747s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37552
find clusters duration: 0.26275s
detected clusters: 106
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9226
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9968
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10004
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 2543
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9963
size cluster i: 47 -> 10000
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20003
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10004
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 6924
size cluster i: 79 -> 10001
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 7749
size cluster i: 90 -> 3362
size cluster i: 91 -> 5590
size cluster i: 92 -> 10000
size cluster i: 93 -> 3345
size cluster i: 94 -> 324
size cluster i: 95 -> 3
size cluster i: 96 -> 4
size cluster i: 97 -> 2
size cluster i: 98 -> 5
size cluster i: 100 -> 3
size cluster i: 101 -> 2
size cluster i: 103 -> 2
size cluster i: 105 -> 2
datapoints in clusters: 959124
score: 88.3899


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:14:28 2018
elapsed time: 55.1001s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 5: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:64470] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:64470] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 106
datapoints_clusters: 959124
score: 88.3899
elapsed_time: 55.1001

--------------- END RUN -----------------
-> new best_score:0.9106764705882353new best_threshold:629.62962963

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 654.320987654321 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[54953,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 654.321


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0795s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4475s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.83489s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37415
find clusters duration: 0.262051s
detected clusters: 101
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 8930
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9954
size cluster i: 15 -> 10000
size cluster i: 16 -> 20003
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10002
size cluster i: 23 -> 20005
size cluster i: 24 -> 10003
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 1961
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9928
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20003
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 9999
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 6356
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 9998
size cluster i: 88 -> 10001
size cluster i: 89 -> 7274
size cluster i: 90 -> 4993
size cluster i: 91 -> 10000
size cluster i: 92 -> 2829
size cluster i: 93 -> 2787
size cluster i: 94 -> 204
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 4
datapoints in clusters: 955320
score: 92.7222


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:15:32 2018
elapsed time: 54.9604s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 4: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 5: Exiting... 
Node 1: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:64787] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:64787] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 101
datapoints_clusters: 955320
score: 92.7222
elapsed_time: 54.9604

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 679.0123456790125 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[54755,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 679.012


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0059s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4399s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.90878s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37269
find clusters duration: 0.270655s
detected clusters: 110
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 8631
size cluster i: 8 -> 10001
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9913
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10002
size cluster i: 22 -> 10002
size cluster i: 23 -> 20004
size cluster i: 24 -> 10003
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10003
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 1510
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9872
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20003
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 9999
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 5790
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 9993
size cluster i: 88 -> 10001
size cluster i: 89 -> 6738
size cluster i: 90 -> 4442
size cluster i: 91 -> 10000
size cluster i: 92 -> 2356
size cluster i: 93 -> 2313
size cluster i: 94 -> 2
size cluster i: 95 -> 97
size cluster i: 96 -> 5
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 100 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 5
size cluster i: 103 -> 3
size cluster i: 104 -> 3
size cluster i: 106 -> 2
size cluster i: 107 -> 2
datapoints in clusters: 951777
score: 83.9803


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:16:37 2018
elapsed time: 55.0451s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 5: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 3: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:65113] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:65113] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 110
datapoints_clusters: 951777
score: 83.9803
elapsed_time: 55.0451

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 703.7037037037038 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[54309,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 703.704


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1122s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4516s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.89477s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37143
find clusters duration: 0.262206s
detected clusters: 117
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 8303
size cluster i: 8 -> 10001
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10004
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9854
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10002
size cluster i: 22 -> 10002
size cluster i: 23 -> 20003
size cluster i: 24 -> 10003
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 9999
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10003
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 1085
size cluster i: 44 -> 10001
size cluster i: 45 -> 20003
size cluster i: 46 -> 9805
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20003
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 9999
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 5203
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 9986
size cluster i: 88 -> 10001
size cluster i: 89 -> 6178
size cluster i: 90 -> 3864
size cluster i: 91 -> 10000
size cluster i: 92 -> 1874
size cluster i: 93 -> 1927
size cluster i: 94 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 3
size cluster i: 97 -> 50
size cluster i: 98 -> 2
size cluster i: 99 -> 4
size cluster i: 100 -> 2
size cluster i: 101 -> 3
size cluster i: 102 -> 3
size cluster i: 103 -> 2
size cluster i: 105 -> 3
size cluster i: 107 -> 3
size cluster i: 108 -> 2
size cluster i: 109 -> 3
size cluster i: 116 -> 2
datapoints in clusters: 948253
score: 77.1618


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:17:42 2018
elapsed time: 55.1299s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 7: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:65439] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:65439] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 117
datapoints_clusters: 948253
score: 77.1618
elapsed_time: 55.1299

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 728.3950617283951 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[11099,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 728.395


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1447s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.54s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.99485s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37005
find clusters duration: 0.257223s
detected clusters: 116
size cluster i: 0 -> 10001
size cluster i: 1 -> 10001
size cluster i: 2 -> 10000
size cluster i: 3 -> 9999
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 10001
size cluster i: 8 -> 10000
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10003
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9794
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10002
size cluster i: 22 -> 10002
size cluster i: 23 -> 20003
size cluster i: 24 -> 10003
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 9999
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10003
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 10000
size cluster i: 44 -> 20003
size cluster i: 45 -> 9694
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10002
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 20003
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10002
size cluster i: 59 -> 10002
size cluster i: 60 -> 10000
size cluster i: 61 -> 10001
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10003
size cluster i: 66 -> 10001
size cluster i: 67 -> 10001
size cluster i: 68 -> 10000
size cluster i: 69 -> 10001
size cluster i: 70 -> 9997
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 4650
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10003
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 9969
size cluster i: 87 -> 7897
size cluster i: 88 -> 10001
size cluster i: 89 -> 5688
size cluster i: 90 -> 3357
size cluster i: 91 -> 10000
size cluster i: 92 -> 785
size cluster i: 93 -> 1473
size cluster i: 94 -> 1565
size cluster i: 95 -> 9
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 99 -> 5
size cluster i: 100 -> 2
size cluster i: 101 -> 3
size cluster i: 102 -> 2
size cluster i: 103 -> 3
size cluster i: 104 -> 4
size cluster i: 105 -> 3
size cluster i: 108 -> 2
size cluster i: 109 -> 3
datapoints in clusters: 944993
score: 77.823


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:18:48 2018
elapsed time: 55.2967s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 4: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 7: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:65760] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:65760] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 116
datapoints_clusters: 944993
score: 77.823
elapsed_time: 55.2967

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 753.0864197530865 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[10642,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 753.086


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1198s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.512s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.93316s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 36885
find clusters duration: 0.260984s
detected clusters: 111
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 9997
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 10001
size cluster i: 8 -> 10000
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10003
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9696
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 20003
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10000
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 9998
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10003
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10002
size cluster i: 43 -> 10000
size cluster i: 44 -> 20003
size cluster i: 45 -> 9565
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 20003
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10000
size cluster i: 61 -> 10001
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10003
size cluster i: 66 -> 10001
size cluster i: 67 -> 10001
size cluster i: 68 -> 10000
size cluster i: 69 -> 10001
size cluster i: 70 -> 9991
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 4065
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 9948
size cluster i: 87 -> 7444
size cluster i: 88 -> 10001
size cluster i: 89 -> 5155
size cluster i: 90 -> 2901
size cluster i: 91 -> 10000
size cluster i: 92 -> 536
size cluster i: 93 -> 1166
size cluster i: 94 -> 1254
size cluster i: 95 -> 5
size cluster i: 96 -> 2
size cluster i: 97 -> 5
size cluster i: 98 -> 3
size cluster i: 99 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 2
size cluster i: 102 -> 2
size cluster i: 103 -> 3
size cluster i: 104 -> 4
size cluster i: 105 -> 2
size cluster i: 110 -> 2
datapoints in clusters: 941822
score: 82.1786


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:19:53 2018
elapsed time: 55.1594s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:66089] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:66089] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 111
datapoints_clusters: 941822
score: 82.1786
elapsed_time: 55.1594

--------------- END RUN -----------------
thresholds:[ 610.42524005  615.9122085   621.39917695  626.8861454   632.37311385
  637.8600823   643.34705075  648.8340192 ]threshold_step:5.48696844993

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 610.4252400548697 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[10456,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 610.425


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0926s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4504s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.0234s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37659
find clusters duration: 0.265825s
detected clusters: 103
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9414
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9984
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10005
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 3033
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9982
size cluster i: 47 -> 10000
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20004
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 20007
size cluster i: 66 -> 10003
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10004
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 7342
size cluster i: 78 -> 10002
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10003
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 8082
size cluster i: 89 -> 3754
size cluster i: 90 -> 3795
size cluster i: 91 -> 6036
size cluster i: 92 -> 10000
size cluster i: 93 -> 456
size cluster i: 94 -> 4
size cluster i: 95 -> 4
size cluster i: 98 -> 2
size cluster i: 99 -> 3
size cluster i: 100 -> 3
datapoints in clusters: 962008
score: 91.4851


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:20:58 2018
elapsed time: 55.1601s


Finishing: 
---------- 
Beginning cleanup...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:66403] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:66403] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 103
datapoints_clusters: 962008
score: 91.4851
elapsed_time: 55.1601

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 615.9122085048011 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[12052,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 615.912


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1899s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4154s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.95974s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37616
find clusters duration: 0.270638s
detected clusters: 103
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9370
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9979
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10005
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 2887
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9981
size cluster i: 47 -> 10000
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20004
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10004
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 7210
size cluster i: 79 -> 10002
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 7982
size cluster i: 90 -> 3629
size cluster i: 91 -> 3672
size cluster i: 92 -> 5898
size cluster i: 93 -> 10000
size cluster i: 94 -> 414
size cluster i: 95 -> 4
size cluster i: 97 -> 3
size cluster i: 99 -> 3
size cluster i: 100 -> 3
datapoints in clusters: 961147
score: 91.4032


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:22:03 2018
elapsed time: 55.1703s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 8: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:66735] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:66735] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 103
datapoints_clusters: 961147
score: 91.4032
elapsed_time: 55.1703

--------------- END RUN -----------------
-> new best_score:0.9126607843137254new best_threshold:615.912208505

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 621.3991769547325 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[11846,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 621.399


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0932s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4495s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.05555s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37594
find clusters duration: 0.270304s
detected clusters: 107
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9314
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9975
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10005
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 2749
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9969
size cluster i: 47 -> 10000
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20004
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10004
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 7090
size cluster i: 79 -> 10001
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 7908
size cluster i: 90 -> 3517
size cluster i: 91 -> 3555
size cluster i: 92 -> 5793
size cluster i: 93 -> 10000
size cluster i: 94 -> 379
size cluster i: 95 -> 4
size cluster i: 96 -> 3
size cluster i: 98 -> 3
size cluster i: 100 -> 3
size cluster i: 101 -> 2
size cluster i: 104 -> 3
size cluster i: 106 -> 2
datapoints in clusters: 960381
score: 87.5641


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:23:08 2018
elapsed time: 55.1998s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 6: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 5: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:67069] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:67069] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 107
datapoints_clusters: 960381
score: 87.5641
elapsed_time: 55.1998

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 626.8861454046639 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[11517,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 626.886


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1948s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4734s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.88929s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37558
find clusters duration: 0.267124s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9256
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9972
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10004
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 2601
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9967
size cluster i: 47 -> 10000
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20003
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10004
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 6985
size cluster i: 79 -> 10001
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 7799
size cluster i: 90 -> 3402
size cluster i: 91 -> 3418
size cluster i: 92 -> 5660
size cluster i: 93 -> 10000
size cluster i: 94 -> 343
size cluster i: 95 -> 4
size cluster i: 96 -> 2
size cluster i: 97 -> 5
size cluster i: 99 -> 3
size cluster i: 100 -> 2
size cluster i: 101 -> 3
size cluster i: 103 -> 2
datapoints in clusters: 959531
score: 90.3088


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:24:13 2018
elapsed time: 55.1589s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Node 8: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Cleanup done!
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:67398] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:67398] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with the
value given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 959531
score: 90.3088
elapsed_time: 55.1589

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 632.3731138545954 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[9167,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 632.373


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1221s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4089s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.97553s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37552
find clusters duration: 0.270967s
detected clusters: 110
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9194
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9968
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10004
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 2477
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9958
size cluster i: 47 -> 10000
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20003
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10004
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 6868
size cluster i: 79 -> 10001
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 7692
size cluster i: 90 -> 3297
size cluster i: 91 -> 5518
size cluster i: 92 -> 10000
size cluster i: 93 -> 3270
size cluster i: 94 -> 311
size cluster i: 95 -> 3
size cluster i: 96 -> 4
size cluster i: 97 -> 2
size cluster i: 98 -> 5
size cluster i: 99 -> 2
size cluster i: 101 -> 2
size cluster i: 103 -> 3
size cluster i: 104 -> 2
size cluster i: 106 -> 2
size cluster i: 108 -> 2
datapoints in clusters: 958689
score: 84.5902


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:25:18 2018
elapsed time: 55.1097s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:67700] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:67700] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 110
datapoints_clusters: 958689
score: 84.5902
elapsed_time: 55.1097

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 637.8600823045268 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[8819,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 637.86


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2359s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4361s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.90496s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37520
find clusters duration: 0.268123s
detected clusters: 105
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9118
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9967
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10004
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 2334
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9951
size cluster i: 47 -> 10000
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20003
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 9999
size cluster i: 72 -> 10004
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 6738
size cluster i: 79 -> 10001
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 9999
size cluster i: 88 -> 10001
size cluster i: 89 -> 7591
size cluster i: 90 -> 3190
size cluster i: 91 -> 5393
size cluster i: 92 -> 10000
size cluster i: 93 -> 3131
size cluster i: 94 -> 286
size cluster i: 95 -> 2
size cluster i: 96 -> 4
size cluster i: 97 -> 4
size cluster i: 101 -> 2
size cluster i: 102 -> 2
size cluster i: 103 -> 2
datapoints in clusters: 957820
score: 89.2087


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:26:23 2018
elapsed time: 55.1808s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:68040] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:68040] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 105
datapoints_clusters: 957820
score: 89.2087
elapsed_time: 55.1808

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 643.3470507544582 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[8380,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 643.347


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1678s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4635s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.9996s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37496
find clusters duration: 0.263913s
detected clusters: 107
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9049
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9963
size cluster i: 15 -> 10000
size cluster i: 16 -> 20003
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10004
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 2196
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9944
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20003
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 9999
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 6622
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 9999
size cluster i: 88 -> 10001
size cluster i: 89 -> 7482
size cluster i: 90 -> 3074
size cluster i: 91 -> 5252
size cluster i: 92 -> 10000
size cluster i: 93 -> 3002
size cluster i: 94 -> 257
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 3
size cluster i: 101 -> 3
size cluster i: 102 -> 3
size cluster i: 104 -> 2
size cluster i: 106 -> 2
datapoints in clusters: 956958
score: 87.2521


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:27:28 2018
elapsed time: 55.2227s


Finishing: 
---------- 
Node 6 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:68359] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:68359] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 107
datapoints_clusters: 956958
score: 87.2521
elapsed_time: 55.2227

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 648.8340192043896 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[10226,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 648.834


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2408s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4671s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.96453s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37449
find clusters duration: 0.266333s
detected clusters: 101
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 8996
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9961
size cluster i: 15 -> 10000
size cluster i: 16 -> 20003
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10003
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 2078
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9935
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20003
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 9999
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 6498
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 9998
size cluster i: 88 -> 10001
size cluster i: 89 -> 7368
size cluster i: 90 -> 5126
size cluster i: 91 -> 10000
size cluster i: 92 -> 2950
size cluster i: 93 -> 2888
size cluster i: 94 -> 224
size cluster i: 96 -> 2
size cluster i: 98 -> 2
size cluster i: 99 -> 4
datapoints in clusters: 956128
score: 92.8007


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:28:33 2018
elapsed time: 55.2629s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1: Exiting... 
Node 7: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:68681] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:68681] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 101
datapoints_clusters: 956128
score: 92.8007
elapsed_time: 55.2629

--------------- END RUN -----------------
thresholds:[ 611.64456638  612.8638927   614.08321902  615.30254534  616.52187167
  617.74119799  618.96052431  620.17985063]threshold_step:1.21932632221

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 611.6445663770767 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[9924,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 611.645


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1956s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.3873s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.07255s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37641
find clusters duration: 0.264357s
detected clusters: 103
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9404
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9983
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10005
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 3007
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9982
size cluster i: 47 -> 10000
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20004
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 20007
size cluster i: 66 -> 10003
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10004
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 7312
size cluster i: 78 -> 10002
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10003
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 8062
size cluster i: 89 -> 3731
size cluster i: 90 -> 3762
size cluster i: 91 -> 5996
size cluster i: 92 -> 10000
size cluster i: 93 -> 450
size cluster i: 94 -> 4
size cluster i: 95 -> 4
size cluster i: 98 -> 2
size cluster i: 99 -> 3
size cluster i: 100 -> 3
datapoints in clusters: 961819
score: 91.4671


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:29:38 2018
elapsed time: 55.2617s


Finishing: 
---------- 
Node 3 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:68991] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:68991] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 103
datapoints_clusters: 961819
score: 91.4671
elapsed_time: 55.2617

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 612.8638926992837 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[9477,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 612.864


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1619s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.391s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.08943s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37625
find clusters duration: 0.267857s
detected clusters: 102
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9392
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9983
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10005
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 2973
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9982
size cluster i: 47 -> 10000
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20004
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 20007
size cluster i: 66 -> 10003
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10004
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 7284
size cluster i: 78 -> 10002
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10003
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 8045
size cluster i: 89 -> 3699
size cluster i: 90 -> 3738
size cluster i: 91 -> 5968
size cluster i: 92 -> 10000
size cluster i: 93 -> 434
size cluster i: 94 -> 4
size cluster i: 96 -> 4
size cluster i: 98 -> 3
size cluster i: 99 -> 3
datapoints in clusters: 961626
score: 92.3915


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:30:44 2018
elapsed time: 55.2406s


Finishing: 
---------- 
Beginning cleanup...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 4: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 6: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:69310] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:69310] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 102
datapoints_clusters: 961626
score: 92.3915
elapsed_time: 55.2406

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 614.0832190214907 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[15282,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 614.083


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0354s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4321s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.89476s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37623
find clusters duration: 0.262881s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9383
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9980
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10005
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 2936
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9981
size cluster i: 47 -> 10000
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20004
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10004
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 7252
size cluster i: 79 -> 10002
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 8018
size cluster i: 90 -> 3677
size cluster i: 91 -> 3714
size cluster i: 92 -> 5934
size cluster i: 93 -> 10000
size cluster i: 94 -> 427
size cluster i: 95 -> 4
size cluster i: 97 -> 4
size cluster i: 99 -> 3
size cluster i: 100 -> 3
datapoints in clusters: 961429
score: 90.4874


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:31:49 2018
elapsed time: 54.9784s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:69641] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:69641] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 961429
score: 90.4874
elapsed_time: 54.9784

--------------- END RUN -----------------
-> new best_score:0.9129352941176471new best_threshold:614.083219021

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 615.3025453436976 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[15075,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 615.303


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1355s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4549s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.91836s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37608
find clusters duration: 0.272585s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9374
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9979
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10005
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 2904
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9981
size cluster i: 47 -> 10000
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20004
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10004
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 7225
size cluster i: 79 -> 10002
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 7993
size cluster i: 90 -> 3643
size cluster i: 91 -> 3696
size cluster i: 92 -> 5908
size cluster i: 93 -> 10000
size cluster i: 94 -> 422
size cluster i: 95 -> 4
size cluster i: 97 -> 3
size cluster i: 99 -> 3
size cluster i: 100 -> 3
datapoints in clusters: 961251
score: 90.4707


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:32:54 2018
elapsed time: 55.1303s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 2: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 
Node 5: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node Warning: Node Warning: Node Warning: Node Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!


8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:69976] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:69976] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 961251
score: 90.4707
elapsed_time: 55.1303

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 616.5218716659047 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[14628,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 616.522


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.082s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4363s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.93954s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37610
find clusters duration: 0.27036s
detected clusters: 102
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9365
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9978
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10005
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 2868
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9980
size cluster i: 47 -> 10000
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20004
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10004
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 7188
size cluster i: 79 -> 10002
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 7976
size cluster i: 90 -> 3621
size cluster i: 91 -> 3655
size cluster i: 92 -> 5889
size cluster i: 93 -> 10000
size cluster i: 94 -> 412
size cluster i: 95 -> 4
size cluster i: 97 -> 3
size cluster i: 99 -> 3
size cluster i: 100 -> 3
datapoints in clusters: 961056
score: 92.3368


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:33:59 2018
elapsed time: 55.0586s


Finishing: 
---------- 
Node 7 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:70303] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:70303] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 102
datapoints_clusters: 961056
score: 92.3368
elapsed_time: 55.0586

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 617.7411979881116 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[14445,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 617.741


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1779s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4641s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.90385s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37601
find clusters duration: 0.264755s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9355
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9977
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10005
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 2836
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9980
size cluster i: 47 -> 10000
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20004
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10004
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 7163
size cluster i: 79 -> 10002
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 7956
size cluster i: 90 -> 3598
size cluster i: 91 -> 3628
size cluster i: 92 -> 5869
size cluster i: 93 -> 10000
size cluster i: 94 -> 397
size cluster i: 95 -> 4
size cluster i: 96 -> 3
size cluster i: 98 -> 3
size cluster i: 100 -> 3
size cluster i: 101 -> 3
datapoints in clusters: 960887
score: 90.4364


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:35:04 2018
elapsed time: 55.1293s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3: Exiting... 
Node 8: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:70614] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:70614] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 960887
score: 90.4364
elapsed_time: 55.1293

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 618.9605243103185 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[16025,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 618.961


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0664s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.457s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.89735s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37605
find clusters duration: 0.265906s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9341
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9975
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10005
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 2804
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9976
size cluster i: 47 -> 10000
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20004
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10004
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 7142
size cluster i: 79 -> 10002
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 7937
size cluster i: 90 -> 3572
size cluster i: 91 -> 3615
size cluster i: 92 -> 5842
size cluster i: 93 -> 10000
size cluster i: 94 -> 390
size cluster i: 95 -> 4
size cluster i: 96 -> 3
size cluster i: 98 -> 3
size cluster i: 100 -> 3
size cluster i: 101 -> 2
datapoints in clusters: 960721
score: 90.4208


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:36:09 2018
elapsed time: 55.0409s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7: Exiting... 
Node 8: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:70946] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:70946] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 960721
score: 90.4208
elapsed_time: 55.0409

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 620.1798506325256 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[15821,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 620.18


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1711s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4747s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.97594s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37609
find clusters duration: 0.274473s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9322
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9975
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10005
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 2781
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9972
size cluster i: 47 -> 10000
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20004
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10004
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 7116
size cluster i: 79 -> 10001
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 7925
size cluster i: 90 -> 3544
size cluster i: 91 -> 3589
size cluster i: 92 -> 5818
size cluster i: 93 -> 10000
size cluster i: 94 -> 384
size cluster i: 95 -> 4
size cluster i: 96 -> 3
size cluster i: 98 -> 3
size cluster i: 100 -> 3
size cluster i: 101 -> 2
datapoints in clusters: 960552
score: 90.4049


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:37:14 2018
elapsed time: 55.2847s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node Warning: Node Warning: Node Warning: Node Warning: Node 3 is going to delete COUNT entry for Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:71286] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:71286] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 960552
score: 90.4049
elapsed_time: 55.2847

--------------- END RUN -----------------
thresholds:[0.0, 111.11111111111111, 222.22222222222223, 333.33333333333337, 444.44444444444446, 555.55555555555554, 666.66666666666674, 777.77777777777783, 888.88888888888891, 1000.0]threshold_step:111.111111111

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 0.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[15487,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 0


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1525s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.2711s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.07705s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37747
find clusters duration: 63.8268s
detected clusters: 1
size cluster i: 0 -> 1020000
datapoints in clusters: 1020000
score: 1


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:40:17 2018
elapsed time: 173.662s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5: Exiting... 
Node 4: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:71620] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:71620] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 1
datapoints_clusters: 1020000
score: 1.0
elapsed_time: 173.662

--------------- END RUN -----------------
-> new best_score:0.00980392156862745new best_threshold:0.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 111.11111111111111 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[12979,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 111.111


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.157s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.0436s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.02955s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39632
find clusters duration: 16.0615s
detected clusters: 447
size cluster i: 0 -> 801918
size cluster i: 1 -> 10026
size cluster i: 2 -> 10015
size cluster i: 3 -> 10027
size cluster i: 4 -> 10014
size cluster i: 5 -> 10016
size cluster i: 6 -> 10018
size cluster i: 7 -> 10011
size cluster i: 8 -> 10018
size cluster i: 9 -> 10024
size cluster i: 10 -> 10019
size cluster i: 11 -> 10013
size cluster i: 12 -> 10011
size cluster i: 13 -> 10016
size cluster i: 14 -> 10015
size cluster i: 15 -> 10024
size cluster i: 16 -> 10024
size cluster i: 17 -> 10005
size cluster i: 18 -> 10015
size cluster i: 19 -> 10020
size cluster i: 20 -> 10023
size cluster i: 21 -> 3
size cluster i: 22 -> 3
size cluster i: 23 -> 3
size cluster i: 24 -> 3
size cluster i: 25 -> 2
size cluster i: 26 -> 3
size cluster i: 27 -> 2
size cluster i: 28 -> 2
size cluster i: 29 -> 6
size cluster i: 30 -> 2
size cluster i: 31 -> 2
size cluster i: 32 -> 2
size cluster i: 33 -> 6
size cluster i: 34 -> 2
size cluster i: 35 -> 4
size cluster i: 36 -> 4
size cluster i: 37 -> 4
size cluster i: 38 -> 2
size cluster i: 39 -> 3
size cluster i: 40 -> 3
size cluster i: 41 -> 2
size cluster i: 42 -> 4
size cluster i: 43 -> 5
size cluster i: 44 -> 5
size cluster i: 45 -> 5
size cluster i: 46 -> 2
size cluster i: 47 -> 4
size cluster i: 48 -> 2
size cluster i: 49 -> 2
size cluster i: 50 -> 2
size cluster i: 51 -> 2
size cluster i: 52 -> 2
size cluster i: 53 -> 4
size cluster i: 54 -> 3
size cluster i: 55 -> 8
size cluster i: 56 -> 2
size cluster i: 57 -> 2
size cluster i: 58 -> 3
size cluster i: 59 -> 2
size cluster i: 60 -> 2
size cluster i: 61 -> 2
size cluster i: 62 -> 6
size cluster i: 63 -> 2
size cluster i: 64 -> 2
size cluster i: 65 -> 2
size cluster i: 66 -> 2
size cluster i: 67 -> 3
size cluster i: 68 -> 3
size cluster i: 69 -> 7
size cluster i: 70 -> 4
size cluster i: 71 -> 4
size cluster i: 72 -> 2
size cluster i: 73 -> 2
size cluster i: 74 -> 3
size cluster i: 75 -> 3
size cluster i: 76 -> 3
size cluster i: 77 -> 3
size cluster i: 78 -> 2
size cluster i: 79 -> 2
size cluster i: 81 -> 2
size cluster i: 82 -> 4
size cluster i: 83 -> 3
size cluster i: 84 -> 2
size cluster i: 85 -> 2
size cluster i: 86 -> 3
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 3
size cluster i: 92 -> 2
size cluster i: 93 -> 3
size cluster i: 94 -> 2
size cluster i: 95 -> 3
size cluster i: 96 -> 4
size cluster i: 97 -> 3
size cluster i: 98 -> 7
size cluster i: 99 -> 3
size cluster i: 100 -> 3
size cluster i: 101 -> 4
size cluster i: 102 -> 2
size cluster i: 103 -> 2
size cluster i: 104 -> 5
size cluster i: 105 -> 2
size cluster i: 106 -> 3
size cluster i: 107 -> 5
size cluster i: 108 -> 4
size cluster i: 109 -> 2
size cluster i: 110 -> 2
size cluster i: 111 -> 3
size cluster i: 112 -> 2
size cluster i: 113 -> 5
size cluster i: 114 -> 3
size cluster i: 115 -> 5
size cluster i: 116 -> 3
size cluster i: 117 -> 2
size cluster i: 118 -> 4
size cluster i: 119 -> 2
size cluster i: 120 -> 3
size cluster i: 121 -> 3
size cluster i: 122 -> 3
size cluster i: 124 -> 6
size cluster i: 125 -> 3
size cluster i: 126 -> 4
size cluster i: 127 -> 2
size cluster i: 128 -> 2
size cluster i: 129 -> 5
size cluster i: 130 -> 3
size cluster i: 131 -> 2
size cluster i: 134 -> 3
size cluster i: 135 -> 7
size cluster i: 136 -> 5
size cluster i: 138 -> 2
size cluster i: 139 -> 3
size cluster i: 140 -> 2
size cluster i: 141 -> 5
size cluster i: 142 -> 4
size cluster i: 143 -> 2
size cluster i: 144 -> 2
size cluster i: 146 -> 3
size cluster i: 147 -> 4
size cluster i: 148 -> 5
size cluster i: 149 -> 8
size cluster i: 150 -> 4
size cluster i: 151 -> 8
size cluster i: 152 -> 4
size cluster i: 153 -> 2
size cluster i: 154 -> 3
size cluster i: 156 -> 2
size cluster i: 157 -> 3
size cluster i: 158 -> 2
size cluster i: 159 -> 2
size cluster i: 160 -> 3
size cluster i: 161 -> 2
size cluster i: 162 -> 3
size cluster i: 163 -> 4
size cluster i: 164 -> 2
size cluster i: 165 -> 2
size cluster i: 166 -> 4
size cluster i: 168 -> 2
size cluster i: 169 -> 2
size cluster i: 170 -> 2
size cluster i: 171 -> 5
size cluster i: 172 -> 5
size cluster i: 173 -> 4
size cluster i: 174 -> 2
size cluster i: 175 -> 2
size cluster i: 176 -> 2
size cluster i: 177 -> 3
size cluster i: 178 -> 3
size cluster i: 179 -> 2
size cluster i: 180 -> 3
size cluster i: 181 -> 2
size cluster i: 182 -> 3
size cluster i: 183 -> 4
size cluster i: 184 -> 2
size cluster i: 185 -> 2
size cluster i: 187 -> 2
size cluster i: 188 -> 2
size cluster i: 189 -> 3
size cluster i: 190 -> 2
size cluster i: 191 -> 3
size cluster i: 192 -> 2
size cluster i: 193 -> 3
size cluster i: 194 -> 2
size cluster i: 195 -> 5
size cluster i: 196 -> 2
size cluster i: 197 -> 3
size cluster i: 198 -> 2
size cluster i: 201 -> 3
size cluster i: 202 -> 5
size cluster i: 203 -> 2
size cluster i: 205 -> 2
size cluster i: 206 -> 2
size cluster i: 207 -> 2
size cluster i: 209 -> 5
size cluster i: 211 -> 2
size cluster i: 213 -> 2
size cluster i: 214 -> 3
size cluster i: 218 -> 2
size cluster i: 219 -> 5
size cluster i: 220 -> 4
size cluster i: 221 -> 2
size cluster i: 222 -> 2
size cluster i: 223 -> 2
size cluster i: 224 -> 2
size cluster i: 225 -> 2
size cluster i: 226 -> 3
size cluster i: 227 -> 2
size cluster i: 228 -> 2
size cluster i: 229 -> 3
size cluster i: 230 -> 2
size cluster i: 231 -> 2
size cluster i: 232 -> 2
size cluster i: 233 -> 3
size cluster i: 234 -> 3
size cluster i: 235 -> 2
size cluster i: 236 -> 2
size cluster i: 237 -> 3
size cluster i: 238 -> 2
size cluster i: 239 -> 2
size cluster i: 240 -> 2
size cluster i: 241 -> 2
size cluster i: 242 -> 2
size cluster i: 244 -> 4
size cluster i: 245 -> 2
size cluster i: 246 -> 5
size cluster i: 248 -> 4
size cluster i: 249 -> 2
size cluster i: 251 -> 2
size cluster i: 252 -> 2
size cluster i: 254 -> 2
size cluster i: 255 -> 2
size cluster i: 256 -> 2
size cluster i: 257 -> 3
size cluster i: 258 -> 2
size cluster i: 259 -> 2
size cluster i: 262 -> 4
size cluster i: 263 -> 4
size cluster i: 266 -> 2
size cluster i: 267 -> 2
size cluster i: 268 -> 3
size cluster i: 269 -> 4
size cluster i: 270 -> 2
size cluster i: 271 -> 3
size cluster i: 273 -> 3
size cluster i: 274 -> 3
size cluster i: 277 -> 5
size cluster i: 278 -> 3
size cluster i: 280 -> 2
size cluster i: 281 -> 3
size cluster i: 282 -> 2
size cluster i: 283 -> 2
size cluster i: 286 -> 3
size cluster i: 287 -> 2
size cluster i: 288 -> 2
size cluster i: 290 -> 2
size cluster i: 291 -> 2
size cluster i: 292 -> 3
size cluster i: 293 -> 2
size cluster i: 297 -> 3
size cluster i: 298 -> 2
size cluster i: 300 -> 2
size cluster i: 302 -> 2
size cluster i: 303 -> 2
size cluster i: 305 -> 2
size cluster i: 307 -> 2
size cluster i: 308 -> 2
size cluster i: 309 -> 3
size cluster i: 311 -> 2
size cluster i: 312 -> 2
size cluster i: 313 -> 3
size cluster i: 314 -> 3
size cluster i: 317 -> 2
size cluster i: 319 -> 2
size cluster i: 320 -> 2
size cluster i: 322 -> 4
size cluster i: 324 -> 2
size cluster i: 326 -> 2
size cluster i: 328 -> 2
size cluster i: 330 -> 2
size cluster i: 331 -> 2
size cluster i: 334 -> 2
size cluster i: 335 -> 2
size cluster i: 336 -> 2
size cluster i: 337 -> 2
size cluster i: 338 -> 2
size cluster i: 339 -> 2
size cluster i: 340 -> 2
size cluster i: 342 -> 3
size cluster i: 344 -> 3
size cluster i: 345 -> 2
size cluster i: 346 -> 4
size cluster i: 347 -> 4
size cluster i: 349 -> 2
size cluster i: 354 -> 4
size cluster i: 360 -> 2
size cluster i: 362 -> 2
size cluster i: 367 -> 2
size cluster i: 368 -> 2
size cluster i: 369 -> 2
size cluster i: 374 -> 2
size cluster i: 375 -> 2
size cluster i: 376 -> 5
size cluster i: 377 -> 2
size cluster i: 379 -> 2
size cluster i: 380 -> 4
size cluster i: 381 -> 2
size cluster i: 384 -> 2
size cluster i: 386 -> 2
size cluster i: 389 -> 2
size cluster i: 403 -> 2
size cluster i: 406 -> 2
size cluster i: 412 -> 2
size cluster i: 413 -> 2
size cluster i: 416 -> 2
size cluster i: 426 -> 2
size cluster i: 437 -> 2
size cluster i: 444 -> 2
datapoints in clusters: 1003239
score: 0


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:42:34 2018
elapsed time: 125.621s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:71944] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:71944] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 447
datapoints_clusters: 1003239
score: 0.0
elapsed_time: 125.621

--------------- END RUN -----------------
-> new best_score:0.22231470588235294new best_threshold:111.111111111

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 222.22222222222223 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[12780,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 222.222


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1712s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.1886s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.03862s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39239
find clusters duration: 0.606365s
detected clusters: 206
size cluster i: 0 -> 270390
size cluster i: 1 -> 10013
size cluster i: 2 -> 20026
size cluster i: 3 -> 30035
size cluster i: 4 -> 10002
size cluster i: 5 -> 10013
size cluster i: 6 -> 10019
size cluster i: 7 -> 10007
size cluster i: 8 -> 10013
size cluster i: 9 -> 10013
size cluster i: 10 -> 10011
size cluster i: 11 -> 50054
size cluster i: 12 -> 10009
size cluster i: 13 -> 10011
size cluster i: 14 -> 10005
size cluster i: 15 -> 90119
size cluster i: 16 -> 10018
size cluster i: 17 -> 10004
size cluster i: 18 -> 10006
size cluster i: 19 -> 10010
size cluster i: 20 -> 10021
size cluster i: 21 -> 10007
size cluster i: 22 -> 10010
size cluster i: 23 -> 10014
size cluster i: 24 -> 10010
size cluster i: 25 -> 30039
size cluster i: 26 -> 10009
size cluster i: 27 -> 20027
size cluster i: 28 -> 10004
size cluster i: 29 -> 10010
size cluster i: 30 -> 10008
size cluster i: 31 -> 10004
size cluster i: 32 -> 10013
size cluster i: 33 -> 20018
size cluster i: 34 -> 10006
size cluster i: 35 -> 10009
size cluster i: 36 -> 10007
size cluster i: 37 -> 10012
size cluster i: 38 -> 10009
size cluster i: 39 -> 10012
size cluster i: 40 -> 10008
size cluster i: 41 -> 10003
size cluster i: 42 -> 10016
size cluster i: 43 -> 10011
size cluster i: 44 -> 10018
size cluster i: 45 -> 10015
size cluster i: 46 -> 10009
size cluster i: 47 -> 10003
size cluster i: 48 -> 10001
size cluster i: 49 -> 10011
size cluster i: 50 -> 10013
size cluster i: 51 -> 10002
size cluster i: 52 -> 10006
size cluster i: 53 -> 10004
size cluster i: 54 -> 10014
size cluster i: 55 -> 2
size cluster i: 56 -> 2
size cluster i: 57 -> 2
size cluster i: 58 -> 6
size cluster i: 59 -> 2
size cluster i: 60 -> 4
size cluster i: 61 -> 2
size cluster i: 62 -> 2
size cluster i: 63 -> 2
size cluster i: 64 -> 2
size cluster i: 65 -> 3
size cluster i: 66 -> 4
size cluster i: 67 -> 4
size cluster i: 68 -> 3
size cluster i: 69 -> 2
size cluster i: 70 -> 3
size cluster i: 71 -> 2
size cluster i: 72 -> 2
size cluster i: 73 -> 2
size cluster i: 75 -> 3
size cluster i: 76 -> 4
size cluster i: 77 -> 3
size cluster i: 78 -> 4
size cluster i: 79 -> 2
size cluster i: 80 -> 4
size cluster i: 81 -> 4
size cluster i: 82 -> 2
size cluster i: 83 -> 2
size cluster i: 84 -> 2
size cluster i: 85 -> 2
size cluster i: 86 -> 4
size cluster i: 87 -> 4
size cluster i: 88 -> 4
size cluster i: 89 -> 4
size cluster i: 90 -> 3
size cluster i: 91 -> 2
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 2
size cluster i: 95 -> 4
size cluster i: 96 -> 4
size cluster i: 98 -> 2
size cluster i: 100 -> 4
size cluster i: 102 -> 2
size cluster i: 103 -> 3
size cluster i: 104 -> 3
size cluster i: 105 -> 3
size cluster i: 106 -> 2
size cluster i: 107 -> 2
size cluster i: 108 -> 2
size cluster i: 109 -> 3
size cluster i: 110 -> 2
size cluster i: 112 -> 2
size cluster i: 114 -> 4
size cluster i: 116 -> 3
size cluster i: 117 -> 2
size cluster i: 118 -> 5
size cluster i: 119 -> 3
size cluster i: 120 -> 2
size cluster i: 122 -> 2
size cluster i: 124 -> 4
size cluster i: 126 -> 2
size cluster i: 127 -> 2
size cluster i: 128 -> 4
size cluster i: 129 -> 3
size cluster i: 131 -> 2
size cluster i: 132 -> 2
size cluster i: 133 -> 3
size cluster i: 135 -> 3
size cluster i: 136 -> 3
size cluster i: 137 -> 2
size cluster i: 138 -> 3
size cluster i: 140 -> 3
size cluster i: 142 -> 2
size cluster i: 143 -> 2
size cluster i: 146 -> 2
size cluster i: 147 -> 2
size cluster i: 148 -> 2
size cluster i: 150 -> 2
size cluster i: 151 -> 4
size cluster i: 152 -> 2
size cluster i: 154 -> 3
size cluster i: 155 -> 2
size cluster i: 156 -> 2
size cluster i: 158 -> 3
size cluster i: 159 -> 2
size cluster i: 161 -> 2
size cluster i: 162 -> 2
size cluster i: 164 -> 2
size cluster i: 166 -> 2
size cluster i: 168 -> 3
size cluster i: 171 -> 2
size cluster i: 173 -> 2
size cluster i: 177 -> 2
size cluster i: 186 -> 2
size cluster i: 193 -> 2
datapoints in clusters: 1001472
score: 0


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:44:35 2018
elapsed time: 110.339s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 6: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:72279] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:72279] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 206
datapoints_clusters: 1001472
score: 0.0
elapsed_time: 110.339

--------------- END RUN -----------------
-> new best_score:0.5573803921568627new best_threshold:222.222222222

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 333.33333333333337 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[12297,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 333.333


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1887s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.383s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.96419s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39020
find clusters duration: 0.344633s
detected clusters: 137
size cluster i: 0 -> 100102
size cluster i: 1 -> 10004
size cluster i: 2 -> 20018
size cluster i: 3 -> 30026
size cluster i: 4 -> 10002
size cluster i: 5 -> 10007
size cluster i: 6 -> 30035
size cluster i: 7 -> 10007
size cluster i: 8 -> 10005
size cluster i: 9 -> 10009
size cluster i: 10 -> 10006
size cluster i: 11 -> 10007
size cluster i: 12 -> 30019
size cluster i: 13 -> 10007
size cluster i: 14 -> 10004
size cluster i: 15 -> 10001
size cluster i: 16 -> 10010
size cluster i: 17 -> 60058
size cluster i: 18 -> 10012
size cluster i: 19 -> 10002
size cluster i: 20 -> 10004
size cluster i: 21 -> 20020
size cluster i: 22 -> 10004
size cluster i: 23 -> 10006
size cluster i: 24 -> 10012
size cluster i: 25 -> 10007
size cluster i: 26 -> 10007
size cluster i: 27 -> 10004
size cluster i: 28 -> 20018
size cluster i: 29 -> 10009
size cluster i: 30 -> 20016
size cluster i: 31 -> 10004
size cluster i: 32 -> 30032
size cluster i: 33 -> 20011
size cluster i: 34 -> 10008
size cluster i: 35 -> 20019
size cluster i: 36 -> 10003
size cluster i: 37 -> 10006
size cluster i: 38 -> 10006
size cluster i: 39 -> 10003
size cluster i: 40 -> 10005
size cluster i: 41 -> 20014
size cluster i: 42 -> 10003
size cluster i: 43 -> 10008
size cluster i: 44 -> 10005
size cluster i: 45 -> 10004
size cluster i: 46 -> 10004
size cluster i: 47 -> 10007
size cluster i: 48 -> 10008
size cluster i: 49 -> 10009
size cluster i: 50 -> 10007
size cluster i: 51 -> 10002
size cluster i: 52 -> 10008
size cluster i: 53 -> 10006
size cluster i: 54 -> 10009
size cluster i: 55 -> 10006
size cluster i: 56 -> 10005
size cluster i: 57 -> 10003
size cluster i: 58 -> 10019
size cluster i: 59 -> 10005
size cluster i: 60 -> 10006
size cluster i: 61 -> 10005
size cluster i: 62 -> 10007
size cluster i: 63 -> 10002
size cluster i: 64 -> 10000
size cluster i: 65 -> 10007
size cluster i: 66 -> 10008
size cluster i: 67 -> 10001
size cluster i: 68 -> 10003
size cluster i: 69 -> 10001
size cluster i: 70 -> 10008
size cluster i: 71 -> 2
size cluster i: 72 -> 4
size cluster i: 73 -> 2
size cluster i: 74 -> 3
size cluster i: 75 -> 2
size cluster i: 76 -> 3
size cluster i: 77 -> 2
size cluster i: 78 -> 2
size cluster i: 79 -> 5
size cluster i: 80 -> 2
size cluster i: 81 -> 3
size cluster i: 82 -> 2
size cluster i: 83 -> 2
size cluster i: 84 -> 4
size cluster i: 85 -> 4
size cluster i: 86 -> 3
size cluster i: 87 -> 5
size cluster i: 88 -> 2
size cluster i: 89 -> 2
size cluster i: 90 -> 3
size cluster i: 92 -> 3
size cluster i: 93 -> 2
size cluster i: 95 -> 3
size cluster i: 98 -> 2
size cluster i: 100 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 2
size cluster i: 103 -> 2
size cluster i: 105 -> 2
size cluster i: 107 -> 2
size cluster i: 108 -> 3
size cluster i: 109 -> 3
size cluster i: 110 -> 2
size cluster i: 112 -> 4
size cluster i: 115 -> 2
size cluster i: 116 -> 2
size cluster i: 117 -> 3
size cluster i: 119 -> 2
size cluster i: 120 -> 2
size cluster i: 121 -> 2
size cluster i: 123 -> 2
size cluster i: 124 -> 2
size cluster i: 131 -> 2
size cluster i: 134 -> 2
datapoints in clusters: 1000859
score: 61.8178


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:46:35 2018
elapsed time: 110.21s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:72626] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:72626] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 137
datapoints_clusters: 1000859
score: 61.8178
elapsed_time: 110.21

--------------- END RUN -----------------
-> new best_score:0.7148441176470588new best_threshold:333.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 444.44444444444446 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[14013,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 444.444


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1496s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4018s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.07374s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38915
find clusters duration: 0.301967s
detected clusters: 111
size cluster i: 0 -> 50042
size cluster i: 1 -> 10004
size cluster i: 2 -> 20014
size cluster i: 3 -> 20017
size cluster i: 4 -> 10000
size cluster i: 5 -> 10005
size cluster i: 6 -> 20014
size cluster i: 7 -> 10004
size cluster i: 8 -> 10004
size cluster i: 9 -> 10005
size cluster i: 10 -> 10005
size cluster i: 11 -> 10005
size cluster i: 12 -> 30018
size cluster i: 13 -> 10007
size cluster i: 14 -> 10003
size cluster i: 15 -> 10001
size cluster i: 16 -> 10008
size cluster i: 17 -> 40029
size cluster i: 18 -> 10009
size cluster i: 19 -> 10001
size cluster i: 20 -> 10002
size cluster i: 21 -> 10007
size cluster i: 22 -> 10004
size cluster i: 23 -> 10006
size cluster i: 24 -> 10008
size cluster i: 25 -> 10005
size cluster i: 26 -> 10004
size cluster i: 27 -> 10003
size cluster i: 28 -> 20010
size cluster i: 29 -> 10007
size cluster i: 30 -> 10004
size cluster i: 31 -> 10003
size cluster i: 32 -> 10012
size cluster i: 33 -> 20010
size cluster i: 34 -> 10008
size cluster i: 35 -> 20014
size cluster i: 36 -> 10001
size cluster i: 37 -> 10005
size cluster i: 38 -> 10003
size cluster i: 39 -> 10004
size cluster i: 40 -> 10003
size cluster i: 41 -> 10004
size cluster i: 42 -> 20010
size cluster i: 43 -> 10002
size cluster i: 44 -> 10004
size cluster i: 45 -> 10007
size cluster i: 46 -> 10005
size cluster i: 47 -> 10001
size cluster i: 48 -> 10003
size cluster i: 49 -> 10002
size cluster i: 50 -> 10006
size cluster i: 51 -> 10002
size cluster i: 52 -> 10003
size cluster i: 53 -> 20024
size cluster i: 54 -> 10006
size cluster i: 55 -> 10002
size cluster i: 56 -> 10004
size cluster i: 57 -> 10004
size cluster i: 58 -> 10008
size cluster i: 59 -> 10005
size cluster i: 60 -> 20012
size cluster i: 61 -> 20016
size cluster i: 62 -> 10004
size cluster i: 63 -> 10005
size cluster i: 64 -> 10008
size cluster i: 65 -> 10002
size cluster i: 66 -> 10012
size cluster i: 67 -> 10004
size cluster i: 68 -> 10004
size cluster i: 69 -> 10003
size cluster i: 70 -> 10006
size cluster i: 71 -> 10002
size cluster i: 72 -> 9998
size cluster i: 73 -> 10005
size cluster i: 74 -> 10008
size cluster i: 75 -> 10005
size cluster i: 76 -> 10001
size cluster i: 77 -> 10005
size cluster i: 78 -> 10001
size cluster i: 79 -> 10000
size cluster i: 80 -> 10003
size cluster i: 81 -> 2
size cluster i: 82 -> 2
size cluster i: 83 -> 2
size cluster i: 84 -> 4
size cluster i: 85 -> 2
size cluster i: 86 -> 3
size cluster i: 87 -> 3
size cluster i: 88 -> 4
size cluster i: 89 -> 3
size cluster i: 91 -> 3
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 2
size cluster i: 95 -> 4
size cluster i: 96 -> 3
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 100 -> 2
size cluster i: 103 -> 2
size cluster i: 105 -> 2
size cluster i: 106 -> 2
datapoints in clusters: 1000586
score: 87.306


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:48:36 2018
elapsed time: 110.266s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:72966] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:72966] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 111
datapoints_clusters: 1000586
score: 87.306
elapsed_time: 110.266

--------------- END RUN -----------------
-> new best_score:0.8131470588235294new best_threshold:444.444444444

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 555.5555555555555 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[13785,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 555.556


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0846s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4291s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.99133s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38830
find clusters duration: 0.29383s
detected clusters: 99
size cluster i: 0 -> 20014
size cluster i: 1 -> 10004
size cluster i: 2 -> 20009
size cluster i: 3 -> 20014
size cluster i: 4 -> 10000
size cluster i: 5 -> 10003
size cluster i: 6 -> 20010
size cluster i: 7 -> 10004
size cluster i: 8 -> 10004
size cluster i: 9 -> 10004
size cluster i: 10 -> 10004
size cluster i: 11 -> 10004
size cluster i: 12 -> 30016
size cluster i: 13 -> 10003
size cluster i: 14 -> 10003
size cluster i: 15 -> 10001
size cluster i: 16 -> 10006
size cluster i: 17 -> 40020
size cluster i: 18 -> 10007
size cluster i: 19 -> 10001
size cluster i: 20 -> 10001
size cluster i: 21 -> 10005
size cluster i: 22 -> 10003
size cluster i: 23 -> 10004
size cluster i: 24 -> 30018
size cluster i: 25 -> 10005
size cluster i: 26 -> 10003
size cluster i: 27 -> 10003
size cluster i: 28 -> 10001
size cluster i: 29 -> 20009
size cluster i: 30 -> 10007
size cluster i: 31 -> 10004
size cluster i: 32 -> 10003
size cluster i: 33 -> 10009
size cluster i: 34 -> 10004
size cluster i: 35 -> 10004
size cluster i: 36 -> 20012
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10002
size cluster i: 42 -> 10004
size cluster i: 43 -> 20007
size cluster i: 44 -> 10002
size cluster i: 45 -> 10003
size cluster i: 46 -> 10003
size cluster i: 47 -> 10004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10005
size cluster i: 52 -> 10002
size cluster i: 53 -> 10003
size cluster i: 54 -> 20018
size cluster i: 55 -> 10002
size cluster i: 56 -> 10002
size cluster i: 57 -> 10004
size cluster i: 58 -> 10003
size cluster i: 59 -> 10006
size cluster i: 60 -> 10004
size cluster i: 61 -> 20011
size cluster i: 62 -> 10005
size cluster i: 63 -> 10004
size cluster i: 64 -> 10004
size cluster i: 65 -> 10006
size cluster i: 66 -> 10006
size cluster i: 67 -> 10000
size cluster i: 68 -> 10010
size cluster i: 69 -> 10004
size cluster i: 70 -> 10003
size cluster i: 71 -> 10002
size cluster i: 72 -> 10003
size cluster i: 73 -> 10006
size cluster i: 74 -> 10001
size cluster i: 75 -> 9968
size cluster i: 76 -> 10003
size cluster i: 77 -> 10002
size cluster i: 78 -> 10005
size cluster i: 79 -> 10000
size cluster i: 80 -> 10004
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10002
size cluster i: 84 -> 2
size cluster i: 85 -> 4
size cluster i: 86 -> 2
size cluster i: 87 -> 4
size cluster i: 88 -> 2
size cluster i: 90 -> 3
size cluster i: 91 -> 2
size cluster i: 92 -> 3
size cluster i: 93 -> 2
size cluster i: 94 -> 2
size cluster i: 96 -> 2
datapoints in clusters: 1000396
score: 97.0973


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:50:36 2018
elapsed time: 110.137s


Finishing: 
---------- 
Beginning cleanup...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Node 2: Exiting... 
Cleanup done!
Node 4: Exiting... 
Node 5: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!


Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:73314] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:73314] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 1000396
score: 97.0973
elapsed_time: 110.137

--------------- END RUN -----------------
-> new best_score:0.8426862745098039new best_threshold:555.555555556

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 666.6666666666667 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[13312,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 666.667


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.16s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4271s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.98065s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38774
find clusters duration: 0.30273s
detected clusters: 99
size cluster i: 0 -> 10005
size cluster i: 1 -> 10002
size cluster i: 2 -> 20009
size cluster i: 3 -> 20009
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10001
size cluster i: 7 -> 10002
size cluster i: 8 -> 10003
size cluster i: 9 -> 10004
size cluster i: 10 -> 10002
size cluster i: 11 -> 10003
size cluster i: 12 -> 20010
size cluster i: 13 -> 10002
size cluster i: 14 -> 10002
size cluster i: 15 -> 10001
size cluster i: 16 -> 10006
size cluster i: 17 -> 30006
size cluster i: 18 -> 10007
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10004
size cluster i: 22 -> 10001
size cluster i: 23 -> 10004
size cluster i: 24 -> 30015
size cluster i: 25 -> 10004
size cluster i: 26 -> 10003
size cluster i: 27 -> 10003
size cluster i: 28 -> 9995
size cluster i: 29 -> 20008
size cluster i: 30 -> 10006
size cluster i: 31 -> 10001
size cluster i: 32 -> 10002
size cluster i: 33 -> 10005
size cluster i: 34 -> 10007
size cluster i: 35 -> 10003
size cluster i: 36 -> 10004
size cluster i: 37 -> 10003
size cluster i: 38 -> 9999
size cluster i: 39 -> 10003
size cluster i: 40 -> 10003
size cluster i: 41 -> 10000
size cluster i: 42 -> 10001
size cluster i: 43 -> 10003
size cluster i: 44 -> 20007
size cluster i: 45 -> 10001
size cluster i: 46 -> 10003
size cluster i: 47 -> 10002
size cluster i: 48 -> 10002
size cluster i: 49 -> 10001
size cluster i: 50 -> 10002
size cluster i: 51 -> 10002
size cluster i: 52 -> 10005
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 20013
size cluster i: 56 -> 10002
size cluster i: 57 -> 10001
size cluster i: 58 -> 10006
size cluster i: 59 -> 10004
size cluster i: 60 -> 10003
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10006
size cluster i: 64 -> 20009
size cluster i: 65 -> 10003
size cluster i: 66 -> 10004
size cluster i: 67 -> 10004
size cluster i: 68 -> 10006
size cluster i: 69 -> 10005
size cluster i: 70 -> 10000
size cluster i: 71 -> 10007
size cluster i: 72 -> 10004
size cluster i: 73 -> 10003
size cluster i: 74 -> 10003
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10003
size cluster i: 78 -> 10001
size cluster i: 79 -> 10005
size cluster i: 80 -> 9737
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 10003
size cluster i: 84 -> 10000
size cluster i: 85 -> 10003
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 4
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 1000052
score: 97.0639


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:52:36 2018
elapsed time: 110.192s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:73659] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:73659] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 1000052
score: 97.0639
elapsed_time: 110.192

--------------- END RUN -----------------
-> new best_score:0.8915745098039216new best_threshold:666.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 777.7777777777778 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[2503,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 777.778


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2685s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.476s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.07349s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38718
find clusters duration: 0.28339s
detected clusters: 97
size cluster i: 0 -> 10004
size cluster i: 1 -> 10001
size cluster i: 2 -> 20005
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10001
size cluster i: 8 -> 10002
size cluster i: 9 -> 10004
size cluster i: 10 -> 10001
size cluster i: 11 -> 10002
size cluster i: 12 -> 20008
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10001
size cluster i: 16 -> 10004
size cluster i: 17 -> 30005
size cluster i: 18 -> 10005
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10002
size cluster i: 22 -> 10001
size cluster i: 23 -> 10004
size cluster i: 24 -> 20008
size cluster i: 25 -> 10006
size cluster i: 26 -> 10004
size cluster i: 27 -> 10002
size cluster i: 28 -> 10001
size cluster i: 29 -> 9944
size cluster i: 30 -> 20007
size cluster i: 31 -> 10005
size cluster i: 32 -> 10001
size cluster i: 33 -> 10001
size cluster i: 34 -> 10003
size cluster i: 35 -> 10005
size cluster i: 36 -> 10002
size cluster i: 37 -> 10004
size cluster i: 38 -> 10005
size cluster i: 39 -> 10003
size cluster i: 40 -> 9966
size cluster i: 41 -> 10003
size cluster i: 42 -> 10002
size cluster i: 43 -> 10000
size cluster i: 44 -> 9986
size cluster i: 45 -> 10003
size cluster i: 46 -> 20006
size cluster i: 47 -> 10001
size cluster i: 48 -> 10003
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10002
size cluster i: 53 -> 10002
size cluster i: 54 -> 10004
size cluster i: 55 -> 10002
size cluster i: 56 -> 10002
size cluster i: 57 -> 20009
size cluster i: 58 -> 10001
size cluster i: 59 -> 10001
size cluster i: 60 -> 10006
size cluster i: 61 -> 10004
size cluster i: 62 -> 10001
size cluster i: 63 -> 10006
size cluster i: 64 -> 10003
size cluster i: 65 -> 10004
size cluster i: 66 -> 20009
size cluster i: 67 -> 10003
size cluster i: 68 -> 10003
size cluster i: 69 -> 10002
size cluster i: 70 -> 10006
size cluster i: 71 -> 10005
size cluster i: 72 -> 10000
size cluster i: 73 -> 10006
size cluster i: 74 -> 10001
size cluster i: 75 -> 10003
size cluster i: 76 -> 10002
size cluster i: 77 -> 10000
size cluster i: 78 -> 10003
size cluster i: 79 -> 10002
size cluster i: 80 -> 10001
size cluster i: 81 -> 10004
size cluster i: 82 -> 9095
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10002
size cluster i: 86 -> 10000
size cluster i: 87 -> 10003
size cluster i: 88 -> 10000
size cluster i: 89 -> 9997
size cluster i: 90 -> 10001
size cluster i: 91 -> 2
size cluster i: 92 -> 2
size cluster i: 93 -> 2
datapoints in clusters: 999237
score: 95.0255


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:54:36 2018
elapsed time: 110.427s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:74364] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:74364] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 999237
score: 95.0255
elapsed_time: 110.427

--------------- END RUN -----------------
-> new best_score:0.9105264705882353new best_threshold:777.777777778

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 888.8888888888889 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[2172,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 888.889


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1235s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4517s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.92416s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38666
find clusters duration: 0.278953s
detected clusters: 95
size cluster i: 0 -> 10004
size cluster i: 1 -> 9998
size cluster i: 2 -> 20004
size cluster i: 3 -> 10001
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10001
size cluster i: 8 -> 10001
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 20008
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10003
size cluster i: 17 -> 20002
size cluster i: 18 -> 10003
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20007
size cluster i: 25 -> 10005
size cluster i: 26 -> 10003
size cluster i: 27 -> 10002
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9813
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10005
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10004
size cluster i: 40 -> 10003
size cluster i: 41 -> 9879
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9854
size cluster i: 46 -> 10002
size cluster i: 47 -> 20006
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 10004
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 20009
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10004
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 20009
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10005
size cluster i: 72 -> 10004
size cluster i: 73 -> 10000
size cluster i: 74 -> 10005
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10003
size cluster i: 80 -> 10002
size cluster i: 81 -> 10001
size cluster i: 82 -> 10004
size cluster i: 83 -> 8004
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10002
size cluster i: 87 -> 10000
size cluster i: 88 -> 10002
size cluster i: 89 -> 9999
size cluster i: 90 -> 9962
size cluster i: 91 -> 10001
size cluster i: 92 -> 2
size cluster i: 93 -> 2
datapoints in clusters: 997711
score: 92.9241


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:56:36 2018
elapsed time: 110.107s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 7: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:74695] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:74695] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 95
datapoints_clusters: 997711
score: 92.9241
elapsed_time: 110.107

--------------- END RUN -----------------
-> new best_score:0.9189245098039216new best_threshold:888.888888889

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1000.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[3762,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1000


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1218s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4423s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.99238s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38607
find clusters duration: 0.284515s
detected clusters: 96
size cluster i: 0 -> 10004
size cluster i: 1 -> 9988
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20005
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9556
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9674
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9529
size cluster i: 46 -> 10001
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10004
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 20008
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10005
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10002
size cluster i: 73 -> 10000
size cluster i: 74 -> 10004
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10001
size cluster i: 82 -> 10001
size cluster i: 83 -> 10004
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 6580
size cluster i: 90 -> 10002
size cluster i: 91 -> 9997
size cluster i: 92 -> 9852
size cluster i: 93 -> 10000
size cluster i: 94 -> 2
size cluster i: 95 -> 2
datapoints in clusters: 995326
score: 93.6777


Runtimes: 
--------- 
finished computation at Fri Dec 28 13:58:36 2018
elapsed time: 110.163s


Finishing: 
---------- 
Beginning cleanup...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:75017] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:75017] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 995326
score: 93.6777
elapsed_time: 110.163

--------------- END RUN -----------------
-> new best_score:0.9362980392156863new best_threshold:1000.0
thresholds:[  913.58024691   938.27160494   962.96296296   987.65432099  1012.34567901
  1037.03703704  1061.72839506  1086.41975309]threshold_step:24.6913580247

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 913.5802469135803 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[3566,1],2]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 913.58


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1673s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.3684s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.02219s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38637
find clusters duration: 0.275965s
detected clusters: 95
size cluster i: 0 -> 10004
size cluster i: 1 -> 9996
size cluster i: 2 -> 20004
size cluster i: 3 -> 10001
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10001
size cluster i: 8 -> 10001
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 20008
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20002
size cluster i: 18 -> 10003
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20007
size cluster i: 25 -> 10005
size cluster i: 26 -> 10003
size cluster i: 27 -> 10002
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9764
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10003
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10004
size cluster i: 40 -> 10003
size cluster i: 41 -> 9842
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9810
size cluster i: 46 -> 10002
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10004
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 20009
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10004
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 20009
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10004
size cluster i: 72 -> 10003
size cluster i: 73 -> 10000
size cluster i: 74 -> 10005
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10003
size cluster i: 80 -> 10002
size cluster i: 81 -> 10001
size cluster i: 82 -> 10004
size cluster i: 83 -> 7724
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10002
size cluster i: 89 -> 9999
size cluster i: 90 -> 9948
size cluster i: 91 -> 10001
size cluster i: 92 -> 2
size cluster i: 93 -> 2
datapoints in clusters: 997275
score: 92.8835


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:00:36 2018
elapsed time: 110.176s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:75349] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:75349] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 95
datapoints_clusters: 997275
score: 92.8835
elapsed_time: 110.176

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 938.2716049382716 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[3089,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 938.272


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1763s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4374s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.99214s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38633
find clusters duration: 0.275668s
detected clusters: 94
size cluster i: 0 -> 10004
size cluster i: 1 -> 9994
size cluster i: 2 -> 20004
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 20007
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20002
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20006
size cluster i: 25 -> 10003
size cluster i: 26 -> 10002
size cluster i: 27 -> 10002
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9710
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10002
size cluster i: 39 -> 10004
size cluster i: 40 -> 10002
size cluster i: 41 -> 9789
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9756
size cluster i: 46 -> 10001
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10004
size cluster i: 56 -> 10001
size cluster i: 57 -> 10002
size cluster i: 58 -> 20009
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10004
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 20007
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10003
size cluster i: 73 -> 10000
size cluster i: 74 -> 10005
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10003
size cluster i: 80 -> 10002
size cluster i: 81 -> 10001
size cluster i: 82 -> 10004
size cluster i: 83 -> 7403
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10002
size cluster i: 89 -> 9999
size cluster i: 90 -> 9928
size cluster i: 91 -> 10001
size cluster i: 92 -> 2
size cluster i: 93 -> 2
datapoints in clusters: 996752
score: 91.8575


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:02:36 2018
elapsed time: 110.205s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:75690] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:75690] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 94
datapoints_clusters: 996752
score: 91.8575
elapsed_time: 110.205

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 962.9629629629629 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[840,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 962.963


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1248s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.477s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.87224s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38609
find clusters duration: 0.276298s
detected clusters: 94
size cluster i: 0 -> 10004
size cluster i: 1 -> 9993
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 20007
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20006
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9651
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10002
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9751
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9686
size cluster i: 46 -> 10001
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10004
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 20008
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10004
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 20007
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10003
size cluster i: 73 -> 10000
size cluster i: 74 -> 10004
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10003
size cluster i: 80 -> 10002
size cluster i: 81 -> 10001
size cluster i: 82 -> 10004
size cluster i: 83 -> 7049
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10002
size cluster i: 89 -> 9998
size cluster i: 90 -> 9903
size cluster i: 91 -> 10000
size cluster i: 92 -> 2
size cluster i: 93 -> 2
datapoints in clusters: 996193
score: 91.806


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:04:36 2018
elapsed time: 110.091s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Node 4: Exiting... 
Node 8: Exiting... 
Cleanup done!
Node 6: Exiting... 
Node 3: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:76019] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:76019] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 94
datapoints_clusters: 996193
score: 91.806
elapsed_time: 110.091

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 987.6543209876543 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[507,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 987.654


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1835s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.378s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.18589s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38603
find clusters duration: 0.277066s
detected clusters: 94
size cluster i: 0 -> 10004
size cluster i: 1 -> 9989
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 20007
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20005
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9587
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9703
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9587
size cluster i: 46 -> 10001
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10004
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 20008
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10005
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 20007
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10002
size cluster i: 73 -> 10000
size cluster i: 74 -> 10004
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10002
size cluster i: 80 -> 10001
size cluster i: 81 -> 10001
size cluster i: 82 -> 10004
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 6737
size cluster i: 88 -> 10002
size cluster i: 89 -> 9998
size cluster i: 90 -> 9868
size cluster i: 91 -> 10000
size cluster i: 92 -> 2
size cluster i: 93 -> 2
datapoints in clusters: 995623
score: 91.7535


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:06:36 2018
elapsed time: 110.338s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:76352] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:76352] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 94
datapoints_clusters: 995623
score: 91.7535
elapsed_time: 110.338

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1012.3456790123457 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[34,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1012.35


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1961s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.3615s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.99116s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38599
find clusters duration: 0.276425s
detected clusters: 97
size cluster i: 0 -> 10004
size cluster i: 1 -> 9986
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20005
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9516
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9643
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9466
size cluster i: 46 -> 10001
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 20008
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10005
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10002
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 10004
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 6388
size cluster i: 90 -> 10002
size cluster i: 91 -> 9996
size cluster i: 92 -> 9837
size cluster i: 93 -> 10000
size cluster i: 94 -> 4
size cluster i: 95 -> 2
datapoints in clusters: 994979
score: 94.6206


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:08:36 2018
elapsed time: 110.166s


Finishing: 
---------- 
Beginning cleanup...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 43 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:76697] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:76697] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 994979
score: 94.6206
elapsed_time: 110.166

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1037.037037037037 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[1902,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1037.04


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0866s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.369s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.19515s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38581
find clusters duration: 0.274205s
detected clusters: 96
size cluster i: 0 -> 10004
size cluster i: 1 -> 9980
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9430
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9573
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9349
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 20006
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10002
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 10004
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 6024
size cluster i: 90 -> 10002
size cluster i: 91 -> 9994
size cluster i: 92 -> 9795
size cluster i: 93 -> 10000
size cluster i: 94 -> 3
size cluster i: 95 -> 2
datapoints in clusters: 994283
score: 93.5796


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:10:36 2018
elapsed time: 110.254s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 5: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:77013] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:77013] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 994283
score: 93.5796
elapsed_time: 110.254

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1061.7283950617284 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[1425,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1061.73


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2095s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4513s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.98146s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38552
find clusters duration: 0.269994s
detected clusters: 98
size cluster i: 0 -> 10004
size cluster i: 1 -> 9972
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9328
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9494
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9213
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10001
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10001
size cluster i: 84 -> 10004
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 5670
size cluster i: 91 -> 10002
size cluster i: 92 -> 9993
size cluster i: 93 -> 9746
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 993551
score: 95.4588


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:12:36 2018
elapsed time: 110.236s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:77354] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:77354] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 993551
score: 95.4588
elapsed_time: 110.236

--------------- END RUN -----------------
-> new best_score:0.9443931372549019new best_threshold:1061.72839506

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1086.4197530864199 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[1242,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1086.42


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1428s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4714s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.05817s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38505
find clusters duration: 0.268216s
detected clusters: 97
size cluster i: 0 -> 10004
size cluster i: 1 -> 9955
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9210
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9409
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9074
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10001
size cluster i: 84 -> 10003
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 5295
size cluster i: 91 -> 10002
size cluster i: 92 -> 9986
size cluster i: 93 -> 9693
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
size cluster i: 96 -> 2
datapoints in clusters: 992751
score: 94.4087


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:14:37 2018
elapsed time: 110.269s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Node 8: Exiting... 
Cleanup done!
Node 6: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:77665] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:77665] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 992751
score: 94.4087
elapsed_time: 110.269

--------------- END RUN -----------------
thresholds:[ 1042.52400549  1048.01097394  1053.49794239  1058.98491084  1064.47187929
  1069.95884774  1075.44581619  1080.93278464]threshold_step:5.48696844993

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1042.5240054869685 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[6940,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1042.52


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1695s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.493s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.13451s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38564
find clusters duration: 0.276396s
detected clusters: 97
size cluster i: 0 -> 10004
size cluster i: 1 -> 9978
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9402
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9552
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9322
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10003
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10001
size cluster i: 84 -> 10004
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 5934
size cluster i: 91 -> 10002
size cluster i: 92 -> 9994
size cluster i: 93 -> 9787
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
size cluster i: 96 -> 2
datapoints in clusters: 994104
score: 94.5373


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:16:37 2018
elapsed time: 110.408s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:77991] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:77991] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 994104
score: 94.5373
elapsed_time: 110.408

--------------- END RUN -----------------
-> new best_score:0.9449333333333333new best_threshold:1042.52400549

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1048.0109739368997 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[6726,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1048.01


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1421s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4975s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.98905s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38547
find clusters duration: 0.285575s
detected clusters: 99
size cluster i: 0 -> 10004
size cluster i: 1 -> 9977
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9383
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9537
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9293
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10001
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10001
size cluster i: 84 -> 10004
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 5862
size cluster i: 91 -> 10002
size cluster i: 92 -> 9993
size cluster i: 93 -> 9777
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 993958
score: 96.4724


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:18:37 2018
elapsed time: 110.237s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 6: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 7: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:78333] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:78333] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 993958
score: 96.4724
elapsed_time: 110.237

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1053.4979423868313 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[6382,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1053.5


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1664s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4437s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.9995s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38554
find clusters duration: 0.272806s
detected clusters: 98
size cluster i: 0 -> 10004
size cluster i: 1 -> 9976
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9357
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9526
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9265
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10001
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10001
size cluster i: 84 -> 10004
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 5775
size cluster i: 91 -> 10002
size cluster i: 92 -> 9993
size cluster i: 93 -> 9764
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 993791
score: 95.4819


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:20:37 2018
elapsed time: 110.256s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 7 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 3: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:78677] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:78677] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 993791
score: 95.4819
elapsed_time: 110.256

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1058.9849108367628 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[7945,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1058.98


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1653s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4524s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.82419s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38555
find clusters duration: 0.276825s
detected clusters: 98
size cluster i: 0 -> 10004
size cluster i: 1 -> 9974
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9340
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9503
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9233
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10001
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10001
size cluster i: 84 -> 10004
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 5708
size cluster i: 91 -> 10002
size cluster i: 92 -> 9993
size cluster i: 93 -> 9753
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 993639
score: 95.4673


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:22:37 2018
elapsed time: 110.044s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node Warning: Node Warning: Node Warning: Node Warning: Node Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:79026] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:79026] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 993639
score: 95.4673
elapsed_time: 110.044

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1064.471879286694 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[7767,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1064.47


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1295s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4137s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.05249s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38542
find clusters duration: 0.271901s
detected clusters: 98
size cluster i: 0 -> 10004
size cluster i: 1 -> 9970
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9318
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9483
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9196
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10001
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10001
size cluster i: 84 -> 10004
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 5626
size cluster i: 91 -> 10002
size cluster i: 92 -> 9991
size cluster i: 93 -> 9737
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 993456
score: 95.4497


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:24:37 2018
elapsed time: 110.201s


Finishing: 
---------- 
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Node 2: Exiting... 
Cleanup done!
Node 6: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:79340] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:79340] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 993456
score: 95.4497
elapsed_time: 110.201

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1069.9588477366256 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[7422,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1069.96


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1683s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.3854s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.10354s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38543
find clusters duration: 0.270121s
detected clusters: 97
size cluster i: 0 -> 10004
size cluster i: 1 -> 9966
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9293
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9467
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9169
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10001
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10001
size cluster i: 84 -> 10004
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 5552
size cluster i: 91 -> 10002
size cluster i: 92 -> 9991
size cluster i: 93 -> 9729
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
size cluster i: 96 -> 2
datapoints in clusters: 993299
score: 94.4608


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:26:37 2018
elapsed time: 110.247s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 2: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:79685] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:79685] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 993299
score: 94.4608
elapsed_time: 110.247

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1075.445816186557 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[4902,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1075.45


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2105s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.3509s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.97584s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38525
find clusters duration: 0.275312s
detected clusters: 96
size cluster i: 0 -> 10004
size cluster i: 1 -> 9964
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9262
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9452
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9128
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10001
size cluster i: 84 -> 10004
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 5455
size cluster i: 91 -> 10002
size cluster i: 92 -> 9989
size cluster i: 93 -> 9710
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
datapoints in clusters: 993089
score: 93.4672


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:28:37 2018
elapsed time: 110.133s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 3: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:80029] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:80029] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 993089
score: 93.4672
elapsed_time: 110.133

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1080.9327846364883 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[4700,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1080.93


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1737s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.3749s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.94135s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38508
find clusters duration: 0.273684s
detected clusters: 97
size cluster i: 0 -> 10004
size cluster i: 1 -> 9956
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9231
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9432
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9097
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10001
size cluster i: 84 -> 10003
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 5359
size cluster i: 91 -> 10002
size cluster i: 92 -> 9987
size cluster i: 93 -> 9703
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
size cluster i: 96 -> 2
datapoints in clusters: 992895
score: 94.4224


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:30:38 2018
elapsed time: 110.084s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:80359] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:80359] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 992895
score: 94.4224
elapsed_time: 110.084

--------------- END RUN -----------------
thresholds:[ 1038.25636336  1039.47568968  1040.695016    1041.91434233  1043.13366865
  1044.35299497  1045.57232129  1046.79164761]threshold_step:1.21932632221

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1038.2563633592442 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[4235,1],5]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1038.26


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0926s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.3516s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.06067s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38575
find clusters duration: 0.270247s
detected clusters: 96
size cluster i: 0 -> 10004
size cluster i: 1 -> 9980
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9424
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9570
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9340
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 20006
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10002
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 10004
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 5999
size cluster i: 90 -> 10002
size cluster i: 91 -> 9994
size cluster i: 92 -> 9794
size cluster i: 93 -> 10000
size cluster i: 94 -> 3
size cluster i: 95 -> 2
datapoints in clusters: 994239
score: 93.5754


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:32:38 2018
elapsed time: 110.088s


Finishing: 
---------- 
Beginning cleanup...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 5: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:80688] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:80688] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 994239
score: 93.5754
elapsed_time: 110.088

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1039.475689681451 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[5946,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1039.48


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2087s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4236s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.98301s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38569
find clusters duration: 0.276407s
detected clusters: 96
size cluster i: 0 -> 10004
size cluster i: 1 -> 9979
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9419
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9565
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9336
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 20006
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10002
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 10004
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 5985
size cluster i: 90 -> 10002
size cluster i: 91 -> 9994
size cluster i: 92 -> 9792
size cluster i: 93 -> 10000
size cluster i: 94 -> 2
size cluster i: 95 -> 2
datapoints in clusters: 994207
score: 93.5724


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:34:38 2018
elapsed time: 110.239s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:81025] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:81025] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 994207
score: 93.5724
elapsed_time: 110.239

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1040.695016003658 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[5738,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1040.7


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1319s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.3564s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.00592s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38567
find clusters duration: 0.275689s
detected clusters: 97
size cluster i: 0 -> 10004
size cluster i: 1 -> 9979
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9410
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9558
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9331
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10003
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10001
size cluster i: 84 -> 10004
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 5967
size cluster i: 91 -> 10002
size cluster i: 92 -> 9994
size cluster i: 93 -> 9790
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
size cluster i: 96 -> 2
datapoints in clusters: 994164
score: 94.543


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:36:38 2018
elapsed time: 110.083s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:81361] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:81361] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 994164
score: 94.543
elapsed_time: 110.083

--------------- END RUN -----------------
-> new best_score:0.9449921568627451new best_threshold:1040.695016

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1041.914342325865 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[5267,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1041.91


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1376s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4496s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.95122s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38562
find clusters duration: 0.271969s
detected clusters: 97
size cluster i: 0 -> 10004
size cluster i: 1 -> 9978
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9404
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9552
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9326
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10003
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10001
size cluster i: 84 -> 10004
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 5943
size cluster i: 91 -> 10002
size cluster i: 92 -> 9994
size cluster i: 93 -> 9788
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
size cluster i: 96 -> 2
datapoints in clusters: 994120
score: 94.5389


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:38:38 2018
elapsed time: 110.136s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:81704] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:81704] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 994120
score: 94.5389
elapsed_time: 110.136

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1043.133668648072 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[27592,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1043.13


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1981s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4472s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.99189s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38563
find clusters duration: 0.270833s
detected clusters: 97
size cluster i: 0 -> 10004
size cluster i: 1 -> 9978
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9400
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9549
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9319
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10003
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10001
size cluster i: 84 -> 10004
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 5924
size cluster i: 91 -> 10002
size cluster i: 92 -> 9994
size cluster i: 93 -> 9787
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
size cluster i: 96 -> 2
datapoints in clusters: 994086
score: 94.5356


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:40:38 2018
elapsed time: 110.234s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 8: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:82035] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:82035] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 994086
score: 94.5356
elapsed_time: 110.234

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1044.3529949702788 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[27136,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1044.35


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2283s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4161s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.16518s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38561
find clusters duration: 0.275888s
detected clusters: 98
size cluster i: 0 -> 10004
size cluster i: 1 -> 9977
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9395
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9544
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9316
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10003
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10001
size cluster i: 84 -> 10004
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 5911
size cluster i: 91 -> 10002
size cluster i: 92 -> 9994
size cluster i: 93 -> 9784
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
size cluster i: 96 -> 2
datapoints in clusters: 994057
score: 95.5074


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:42:38 2018
elapsed time: 110.399s


Finishing: 
---------- 
Beginning cleanup...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 8: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:82363] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:82363] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 994057
score: 95.5074
elapsed_time: 110.399

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1045.5723212924859 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[26801,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1045.57


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1531s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4664s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.08958s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38557
find clusters duration: 0.283063s
detected clusters: 98
size cluster i: 0 -> 10004
size cluster i: 1 -> 9977
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9391
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9543
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9309
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10003
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10001
size cluster i: 84 -> 10004
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 5894
size cluster i: 91 -> 10002
size cluster i: 92 -> 9994
size cluster i: 93 -> 9783
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
size cluster i: 96 -> 2
datapoints in clusters: 994027
score: 95.5046


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:44:38 2018
elapsed time: 110.309s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node Warning: Node Warning: Node 6 is going to delete COUNT entry for Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!


3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:82698] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:82698] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 994027
score: 95.5046
elapsed_time: 110.309

--------------- END RUN -----------------

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config config_ocl_float_gtx1080ti.cfg --MPIconfig argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1046.7916476146927 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[28640,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1046.79


Setup:
------ 
Using MPI network config setting: argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.182s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4523s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.15525s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38552
find clusters duration: 0.270092s
detected clusters: 98
size cluster i: 0 -> 10004
size cluster i: 1 -> 9977
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9385
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9539
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9301
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10003
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10001
size cluster i: 84 -> 10004
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 5882
size cluster i: 91 -> 10002
size cluster i: 92 -> 9993
size cluster i: 93 -> 9780
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
size cluster i: 96 -> 2
datapoints in clusters: 993993
score: 95.5013


Runtimes: 
--------- 
finished computation at Fri Dec 28 14:46:39 2018
elapsed time: 110.393s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 8: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:83035] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:83035] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 993993
score: 95.5013
elapsed_time: 110.393

--------------- END RUN -----------------
overall_best_score:0.9449921568627451overall_best_threshold:1040.695016overall_best_lambda_value:1e-07
