lambda_factor: 10.0 lambda_start_lower:1e-08 overall_best_lambda_value: -1
lambda_values:[1e-08, 1e-07, 1e-06, 1e-05, 0.0001]
thresholds:[0.0, 166.66666666666666, 333.33333333333331, 500.0, 666.66666666666663, 833.33333333333326, 1000.0, 1166.6666666666665, 1333.3333333333333, 1500.0], threshold_step:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 0.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[23793,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 0


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0975s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 236.261s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.97411s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37747
find clusters duration: 66.0941s
detected clusters: 1
size cluster i: 0 -> 1020000
datapoints in clusters: 1020000
score: 1


Runtimes: 
--------- 
finished computation at Thu Jan  3 11:37:09 2019
elapsed time: 324.761s


Finishing: 
---------- 
Beginning cleanup...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:227144] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:227144] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 1
datapoints_clusters: 1020000
score: 1.0
elapsed_time: 324.761

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 0.0 percent_correct: 0.00980392156862745
-> new best_percent_correct:0.00980392156862745new best_threshold:0.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 166.66666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[21343,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 166.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1285s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 236.595s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.96309s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39702
find clusters duration: 11.4321s
detected clusters: 628
size cluster i: 0 -> 761962
size cluster i: 1 -> 20035
size cluster i: 2 -> 10024
size cluster i: 3 -> 10011
size cluster i: 4 -> 10030
size cluster i: 5 -> 10016
size cluster i: 6 -> 10024
size cluster i: 7 -> 10042
size cluster i: 8 -> 10016
size cluster i: 9 -> 10035
size cluster i: 10 -> 10021
size cluster i: 11 -> 10014
size cluster i: 12 -> 10020
size cluster i: 13 -> 10019
size cluster i: 14 -> 10011
size cluster i: 15 -> 10028
size cluster i: 16 -> 10022
size cluster i: 17 -> 10019
size cluster i: 18 -> 10032
size cluster i: 19 -> 10010
size cluster i: 20 -> 10003
size cluster i: 21 -> 10004
size cluster i: 22 -> 10009
size cluster i: 23 -> 10013
size cluster i: 24 -> 11
size cluster i: 25 -> 25
size cluster i: 26 -> 3
size cluster i: 27 -> 5
size cluster i: 28 -> 8
size cluster i: 29 -> 2
size cluster i: 30 -> 3
size cluster i: 31 -> 4
size cluster i: 32 -> 8
size cluster i: 33 -> 2
size cluster i: 34 -> 4
size cluster i: 35 -> 2
size cluster i: 36 -> 2
size cluster i: 37 -> 4
size cluster i: 38 -> 6
size cluster i: 39 -> 2
size cluster i: 40 -> 3
size cluster i: 41 -> 8
size cluster i: 42 -> 3
size cluster i: 43 -> 3
size cluster i: 44 -> 3
size cluster i: 45 -> 2
size cluster i: 46 -> 4
size cluster i: 47 -> 14
size cluster i: 48 -> 8
size cluster i: 49 -> 5
size cluster i: 50 -> 4
size cluster i: 51 -> 6
size cluster i: 52 -> 2
size cluster i: 53 -> 2
size cluster i: 54 -> 7
size cluster i: 55 -> 2
size cluster i: 56 -> 5
size cluster i: 57 -> 9
size cluster i: 58 -> 6
size cluster i: 59 -> 2
size cluster i: 60 -> 2
size cluster i: 61 -> 5
size cluster i: 62 -> 2
size cluster i: 63 -> 6
size cluster i: 64 -> 2
size cluster i: 65 -> 2
size cluster i: 66 -> 2
size cluster i: 67 -> 5
size cluster i: 68 -> 3
size cluster i: 69 -> 3
size cluster i: 70 -> 5
size cluster i: 71 -> 2
size cluster i: 72 -> 5
size cluster i: 73 -> 5
size cluster i: 74 -> 3
size cluster i: 75 -> 4
size cluster i: 76 -> 4
size cluster i: 77 -> 2
size cluster i: 78 -> 2
size cluster i: 79 -> 2
size cluster i: 80 -> 2
size cluster i: 81 -> 2
size cluster i: 82 -> 3
size cluster i: 83 -> 3
size cluster i: 84 -> 2
size cluster i: 85 -> 10
size cluster i: 86 -> 6
size cluster i: 87 -> 2
size cluster i: 88 -> 2
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 4
size cluster i: 92 -> 9
size cluster i: 93 -> 2
size cluster i: 94 -> 3
size cluster i: 95 -> 11
size cluster i: 96 -> 3
size cluster i: 97 -> 14
size cluster i: 98 -> 4
size cluster i: 99 -> 3
size cluster i: 100 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 3
size cluster i: 103 -> 5
size cluster i: 104 -> 2
size cluster i: 105 -> 4
size cluster i: 106 -> 2
size cluster i: 107 -> 10
size cluster i: 108 -> 2
size cluster i: 111 -> 11
size cluster i: 112 -> 5
size cluster i: 113 -> 5
size cluster i: 114 -> 4
size cluster i: 116 -> 2
size cluster i: 117 -> 10
size cluster i: 118 -> 3
size cluster i: 119 -> 2
size cluster i: 120 -> 2
size cluster i: 121 -> 2
size cluster i: 123 -> 8
size cluster i: 124 -> 2
size cluster i: 125 -> 2
size cluster i: 126 -> 2
size cluster i: 127 -> 9
size cluster i: 128 -> 2
size cluster i: 129 -> 2
size cluster i: 130 -> 2
size cluster i: 131 -> 3
size cluster i: 132 -> 2
size cluster i: 133 -> 3
size cluster i: 134 -> 4
size cluster i: 135 -> 3
size cluster i: 136 -> 6
size cluster i: 137 -> 4
size cluster i: 138 -> 4
size cluster i: 139 -> 4
size cluster i: 140 -> 3
size cluster i: 141 -> 2
size cluster i: 142 -> 3
size cluster i: 143 -> 2
size cluster i: 144 -> 3
size cluster i: 145 -> 4
size cluster i: 146 -> 2
size cluster i: 147 -> 4
size cluster i: 148 -> 3
size cluster i: 149 -> 2
size cluster i: 150 -> 2
size cluster i: 151 -> 3
size cluster i: 152 -> 2
size cluster i: 153 -> 6
size cluster i: 154 -> 2
size cluster i: 155 -> 7
size cluster i: 156 -> 2
size cluster i: 157 -> 2
size cluster i: 158 -> 4
size cluster i: 159 -> 5
size cluster i: 160 -> 7
size cluster i: 161 -> 4
size cluster i: 162 -> 3
size cluster i: 163 -> 2
size cluster i: 164 -> 4
size cluster i: 165 -> 2
size cluster i: 166 -> 4
size cluster i: 167 -> 2
size cluster i: 168 -> 2
size cluster i: 169 -> 2
size cluster i: 170 -> 5
size cluster i: 171 -> 2
size cluster i: 172 -> 3
size cluster i: 173 -> 2
size cluster i: 174 -> 2
size cluster i: 175 -> 10
size cluster i: 176 -> 4
size cluster i: 177 -> 3
size cluster i: 178 -> 2
size cluster i: 179 -> 4
size cluster i: 180 -> 5
size cluster i: 181 -> 4
size cluster i: 182 -> 5
size cluster i: 183 -> 3
size cluster i: 185 -> 2
size cluster i: 186 -> 5
size cluster i: 187 -> 3
size cluster i: 188 -> 2
size cluster i: 190 -> 2
size cluster i: 193 -> 2
size cluster i: 194 -> 2
size cluster i: 195 -> 2
size cluster i: 196 -> 4
size cluster i: 197 -> 2
size cluster i: 198 -> 3
size cluster i: 199 -> 2
size cluster i: 200 -> 2
size cluster i: 201 -> 2
size cluster i: 204 -> 5
size cluster i: 205 -> 3
size cluster i: 206 -> 3
size cluster i: 207 -> 3
size cluster i: 208 -> 2
size cluster i: 209 -> 7
size cluster i: 210 -> 2
size cluster i: 211 -> 9
size cluster i: 212 -> 2
size cluster i: 214 -> 2
size cluster i: 215 -> 2
size cluster i: 216 -> 2
size cluster i: 217 -> 2
size cluster i: 218 -> 2
size cluster i: 219 -> 3
size cluster i: 220 -> 2
size cluster i: 221 -> 3
size cluster i: 222 -> 4
size cluster i: 223 -> 4
size cluster i: 224 -> 3
size cluster i: 225 -> 3
size cluster i: 226 -> 2
size cluster i: 227 -> 2
size cluster i: 228 -> 5
size cluster i: 229 -> 3
size cluster i: 232 -> 2
size cluster i: 234 -> 2
size cluster i: 236 -> 2
size cluster i: 237 -> 3
size cluster i: 238 -> 4
size cluster i: 239 -> 3
size cluster i: 241 -> 2
size cluster i: 243 -> 4
size cluster i: 244 -> 7
size cluster i: 245 -> 5
size cluster i: 246 -> 3
size cluster i: 247 -> 2
size cluster i: 249 -> 3
size cluster i: 250 -> 3
size cluster i: 251 -> 4
size cluster i: 252 -> 4
size cluster i: 253 -> 2
size cluster i: 254 -> 2
size cluster i: 255 -> 4
size cluster i: 256 -> 3
size cluster i: 257 -> 2
size cluster i: 258 -> 8
size cluster i: 259 -> 2
size cluster i: 260 -> 3
size cluster i: 261 -> 3
size cluster i: 262 -> 4
size cluster i: 263 -> 2
size cluster i: 264 -> 6
size cluster i: 265 -> 2
size cluster i: 267 -> 2
size cluster i: 268 -> 3
size cluster i: 270 -> 3
size cluster i: 271 -> 2
size cluster i: 272 -> 2
size cluster i: 273 -> 2
size cluster i: 274 -> 3
size cluster i: 275 -> 3
size cluster i: 277 -> 2
size cluster i: 279 -> 2
size cluster i: 280 -> 3
size cluster i: 282 -> 2
size cluster i: 283 -> 6
size cluster i: 284 -> 4
size cluster i: 285 -> 4
size cluster i: 287 -> 2
size cluster i: 289 -> 5
size cluster i: 290 -> 4
size cluster i: 291 -> 2
size cluster i: 293 -> 3
size cluster i: 294 -> 3
size cluster i: 295 -> 4
size cluster i: 296 -> 2
size cluster i: 297 -> 2
size cluster i: 298 -> 3
size cluster i: 300 -> 2
size cluster i: 302 -> 3
size cluster i: 303 -> 2
size cluster i: 304 -> 10
size cluster i: 306 -> 2
size cluster i: 308 -> 2
size cluster i: 309 -> 2
size cluster i: 310 -> 3
size cluster i: 311 -> 2
size cluster i: 312 -> 2
size cluster i: 313 -> 2
size cluster i: 314 -> 3
size cluster i: 315 -> 4
size cluster i: 317 -> 3
size cluster i: 318 -> 2
size cluster i: 320 -> 4
size cluster i: 321 -> 2
size cluster i: 322 -> 2
size cluster i: 323 -> 2
size cluster i: 324 -> 2
size cluster i: 325 -> 3
size cluster i: 326 -> 3
size cluster i: 328 -> 2
size cluster i: 329 -> 2
size cluster i: 330 -> 3
size cluster i: 331 -> 2
size cluster i: 332 -> 2
size cluster i: 333 -> 4
size cluster i: 334 -> 4
size cluster i: 335 -> 2
size cluster i: 336 -> 3
size cluster i: 337 -> 2
size cluster i: 339 -> 5
size cluster i: 340 -> 2
size cluster i: 341 -> 2
size cluster i: 342 -> 3
size cluster i: 344 -> 2
size cluster i: 345 -> 4
size cluster i: 347 -> 2
size cluster i: 348 -> 2
size cluster i: 349 -> 5
size cluster i: 350 -> 2
size cluster i: 351 -> 3
size cluster i: 352 -> 2
size cluster i: 354 -> 2
size cluster i: 355 -> 2
size cluster i: 356 -> 2
size cluster i: 357 -> 2
size cluster i: 358 -> 2
size cluster i: 359 -> 3
size cluster i: 361 -> 2
size cluster i: 363 -> 3
size cluster i: 364 -> 3
size cluster i: 367 -> 3
size cluster i: 368 -> 3
size cluster i: 369 -> 3
size cluster i: 371 -> 3
size cluster i: 372 -> 2
size cluster i: 373 -> 4
size cluster i: 376 -> 2
size cluster i: 377 -> 4
size cluster i: 378 -> 2
size cluster i: 379 -> 2
size cluster i: 380 -> 3
size cluster i: 381 -> 2
size cluster i: 384 -> 5
size cluster i: 385 -> 2
size cluster i: 386 -> 2
size cluster i: 387 -> 2
size cluster i: 388 -> 3
size cluster i: 390 -> 2
size cluster i: 391 -> 5
size cluster i: 392 -> 2
size cluster i: 393 -> 2
size cluster i: 394 -> 2
size cluster i: 400 -> 3
size cluster i: 402 -> 2
size cluster i: 408 -> 2
size cluster i: 409 -> 2
size cluster i: 410 -> 2
size cluster i: 412 -> 4
size cluster i: 413 -> 2
size cluster i: 414 -> 2
size cluster i: 416 -> 2
size cluster i: 417 -> 3
size cluster i: 419 -> 2
size cluster i: 421 -> 2
size cluster i: 423 -> 5
size cluster i: 424 -> 2
size cluster i: 425 -> 2
size cluster i: 426 -> 2
size cluster i: 427 -> 2
size cluster i: 428 -> 2
size cluster i: 430 -> 3
size cluster i: 431 -> 3
size cluster i: 432 -> 2
size cluster i: 436 -> 2
size cluster i: 437 -> 4
size cluster i: 441 -> 3
size cluster i: 442 -> 2
size cluster i: 444 -> 2
size cluster i: 445 -> 2
size cluster i: 446 -> 2
size cluster i: 447 -> 3
size cluster i: 449 -> 3
size cluster i: 450 -> 3
size cluster i: 451 -> 2
size cluster i: 453 -> 3
size cluster i: 456 -> 3
size cluster i: 458 -> 4
size cluster i: 460 -> 3
size cluster i: 461 -> 2
size cluster i: 462 -> 4
size cluster i: 463 -> 2
size cluster i: 464 -> 2
size cluster i: 470 -> 3
size cluster i: 471 -> 2
size cluster i: 472 -> 3
size cluster i: 474 -> 2
size cluster i: 477 -> 3
size cluster i: 479 -> 3
size cluster i: 480 -> 2
size cluster i: 481 -> 2
size cluster i: 482 -> 2
size cluster i: 485 -> 2
size cluster i: 489 -> 2
size cluster i: 490 -> 2
size cluster i: 491 -> 3
size cluster i: 492 -> 4
size cluster i: 493 -> 5
size cluster i: 496 -> 2
size cluster i: 498 -> 2
size cluster i: 499 -> 2
size cluster i: 500 -> 2
size cluster i: 501 -> 2
size cluster i: 503 -> 2
size cluster i: 509 -> 2
size cluster i: 512 -> 5
size cluster i: 513 -> 2
size cluster i: 516 -> 2
size cluster i: 518 -> 2
size cluster i: 524 -> 2
size cluster i: 525 -> 2
size cluster i: 526 -> 2
size cluster i: 528 -> 2
size cluster i: 532 -> 2
size cluster i: 535 -> 2
size cluster i: 537 -> 2
size cluster i: 541 -> 2
size cluster i: 542 -> 2
size cluster i: 543 -> 2
size cluster i: 544 -> 2
size cluster i: 545 -> 2
size cluster i: 555 -> 2
size cluster i: 558 -> 2
size cluster i: 560 -> 3
size cluster i: 562 -> 2
size cluster i: 563 -> 2
size cluster i: 572 -> 2
size cluster i: 580 -> 2
size cluster i: 582 -> 2
size cluster i: 602 -> 2
datapoints in clusters: 1003943
score: 0


Runtimes: 
--------- 
finished computation at Thu Jan  3 11:41:50 2019
elapsed time: 270.465s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 1: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:227558] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:227558] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 628
datapoints_clusters: 1003943
score: 0.0
elapsed_time: 270.465

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 166.666666667 percent_correct: 0.2510362745098039
-> new best_percent_correct:0.2510362745098039new best_threshold:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[20929,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 333.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1576s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 237.354s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.03608s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39140
find clusters duration: 0.338913s
detected clusters: 334
size cluster i: 0 -> 60090
size cluster i: 1 -> 10010
size cluster i: 2 -> 20027
size cluster i: 3 -> 20025
size cluster i: 4 -> 10002
size cluster i: 5 -> 10007
size cluster i: 6 -> 10007
size cluster i: 7 -> 20027
size cluster i: 8 -> 10010
size cluster i: 9 -> 10010
size cluster i: 10 -> 10011
size cluster i: 11 -> 10011
size cluster i: 12 -> 50049
size cluster i: 13 -> 10009
size cluster i: 14 -> 20019
size cluster i: 15 -> 10002
size cluster i: 16 -> 10012
size cluster i: 17 -> 40037
size cluster i: 18 -> 10012
size cluster i: 19 -> 10002
size cluster i: 20 -> 10005
size cluster i: 21 -> 20029
size cluster i: 22 -> 10005
size cluster i: 23 -> 10008
size cluster i: 24 -> 60078
size cluster i: 25 -> 10017
size cluster i: 26 -> 10011
size cluster i: 27 -> 10009
size cluster i: 28 -> 20023
size cluster i: 29 -> 10016
size cluster i: 30 -> 10005
size cluster i: 31 -> 10005
size cluster i: 32 -> 30042
size cluster i: 33 -> 20015
size cluster i: 34 -> 10009
size cluster i: 35 -> 10008
size cluster i: 36 -> 10003
size cluster i: 37 -> 10011
size cluster i: 38 -> 10006
size cluster i: 39 -> 10004
size cluster i: 40 -> 10009
size cluster i: 41 -> 20018
size cluster i: 42 -> 10004
size cluster i: 43 -> 10014
size cluster i: 44 -> 10007
size cluster i: 45 -> 10006
size cluster i: 46 -> 10009
size cluster i: 47 -> 10008
size cluster i: 48 -> 10006
size cluster i: 49 -> 10011
size cluster i: 50 -> 10010
size cluster i: 51 -> 10009
size cluster i: 52 -> 10009
size cluster i: 53 -> 10009
size cluster i: 54 -> 10009
size cluster i: 55 -> 10011
size cluster i: 56 -> 10006
size cluster i: 57 -> 10013
size cluster i: 58 -> 10019
size cluster i: 59 -> 10003
size cluster i: 60 -> 10011
size cluster i: 61 -> 10024
size cluster i: 62 -> 10012
size cluster i: 63 -> 10008
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10011
size cluster i: 67 -> 10010
size cluster i: 68 -> 10002
size cluster i: 69 -> 10005
size cluster i: 70 -> 10007
size cluster i: 71 -> 10005
size cluster i: 72 -> 10013
size cluster i: 73 -> 2
size cluster i: 74 -> 2
size cluster i: 75 -> 3
size cluster i: 76 -> 9
size cluster i: 77 -> 4
size cluster i: 78 -> 2
size cluster i: 79 -> 2
size cluster i: 80 -> 4
size cluster i: 81 -> 2
size cluster i: 82 -> 2
size cluster i: 83 -> 2
size cluster i: 84 -> 2
size cluster i: 85 -> 4
size cluster i: 86 -> 2
size cluster i: 87 -> 3
size cluster i: 88 -> 3
size cluster i: 89 -> 2
size cluster i: 90 -> 3
size cluster i: 91 -> 2
size cluster i: 92 -> 3
size cluster i: 93 -> 3
size cluster i: 94 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 4
size cluster i: 97 -> 2
size cluster i: 98 -> 3
size cluster i: 99 -> 3
size cluster i: 100 -> 2
size cluster i: 101 -> 8
size cluster i: 102 -> 2
size cluster i: 103 -> 2
size cluster i: 104 -> 4
size cluster i: 105 -> 2
size cluster i: 106 -> 3
size cluster i: 107 -> 3
size cluster i: 108 -> 2
size cluster i: 109 -> 2
size cluster i: 110 -> 2
size cluster i: 111 -> 2
size cluster i: 112 -> 2
size cluster i: 113 -> 5
size cluster i: 115 -> 2
size cluster i: 116 -> 2
size cluster i: 117 -> 3
size cluster i: 118 -> 2
size cluster i: 120 -> 5
size cluster i: 121 -> 3
size cluster i: 122 -> 4
size cluster i: 123 -> 4
size cluster i: 124 -> 3
size cluster i: 125 -> 4
size cluster i: 126 -> 3
size cluster i: 127 -> 2
size cluster i: 128 -> 3
size cluster i: 129 -> 11
size cluster i: 130 -> 2
size cluster i: 131 -> 4
size cluster i: 132 -> 5
size cluster i: 133 -> 2
size cluster i: 134 -> 2
size cluster i: 135 -> 3
size cluster i: 136 -> 3
size cluster i: 137 -> 2
size cluster i: 138 -> 2
size cluster i: 139 -> 4
size cluster i: 140 -> 4
size cluster i: 141 -> 2
size cluster i: 142 -> 3
size cluster i: 143 -> 2
size cluster i: 145 -> 2
size cluster i: 146 -> 2
size cluster i: 147 -> 5
size cluster i: 148 -> 2
size cluster i: 149 -> 3
size cluster i: 150 -> 3
size cluster i: 151 -> 3
size cluster i: 152 -> 3
size cluster i: 153 -> 2
size cluster i: 154 -> 2
size cluster i: 155 -> 4
size cluster i: 157 -> 2
size cluster i: 158 -> 4
size cluster i: 160 -> 2
size cluster i: 161 -> 2
size cluster i: 162 -> 4
size cluster i: 163 -> 2
size cluster i: 165 -> 2
size cluster i: 167 -> 2
size cluster i: 168 -> 4
size cluster i: 169 -> 3
size cluster i: 170 -> 2
size cluster i: 171 -> 2
size cluster i: 172 -> 3
size cluster i: 173 -> 3
size cluster i: 174 -> 2
size cluster i: 175 -> 3
size cluster i: 176 -> 2
size cluster i: 177 -> 2
size cluster i: 178 -> 2
size cluster i: 180 -> 2
size cluster i: 181 -> 3
size cluster i: 183 -> 2
size cluster i: 184 -> 2
size cluster i: 186 -> 3
size cluster i: 187 -> 2
size cluster i: 189 -> 3
size cluster i: 190 -> 3
size cluster i: 192 -> 2
size cluster i: 193 -> 2
size cluster i: 194 -> 2
size cluster i: 195 -> 2
size cluster i: 197 -> 2
size cluster i: 198 -> 2
size cluster i: 199 -> 2
size cluster i: 200 -> 3
size cluster i: 201 -> 6
size cluster i: 202 -> 3
size cluster i: 203 -> 2
size cluster i: 204 -> 2
size cluster i: 205 -> 2
size cluster i: 206 -> 3
size cluster i: 207 -> 2
size cluster i: 208 -> 2
size cluster i: 209 -> 4
size cluster i: 210 -> 2
size cluster i: 212 -> 2
size cluster i: 213 -> 2
size cluster i: 215 -> 2
size cluster i: 216 -> 2
size cluster i: 217 -> 2
size cluster i: 218 -> 3
size cluster i: 220 -> 2
size cluster i: 221 -> 4
size cluster i: 222 -> 2
size cluster i: 223 -> 3
size cluster i: 224 -> 2
size cluster i: 225 -> 3
size cluster i: 226 -> 3
size cluster i: 227 -> 2
size cluster i: 229 -> 3
size cluster i: 230 -> 2
size cluster i: 231 -> 3
size cluster i: 233 -> 3
size cluster i: 234 -> 2
size cluster i: 235 -> 3
size cluster i: 236 -> 3
size cluster i: 239 -> 2
size cluster i: 240 -> 2
size cluster i: 241 -> 4
size cluster i: 242 -> 2
size cluster i: 243 -> 2
size cluster i: 244 -> 3
size cluster i: 246 -> 2
size cluster i: 247 -> 3
size cluster i: 248 -> 2
size cluster i: 249 -> 2
size cluster i: 252 -> 2
size cluster i: 255 -> 2
size cluster i: 256 -> 2
size cluster i: 257 -> 2
size cluster i: 260 -> 2
size cluster i: 264 -> 2
size cluster i: 265 -> 3
size cluster i: 266 -> 4
size cluster i: 267 -> 2
size cluster i: 269 -> 2
size cluster i: 273 -> 2
size cluster i: 274 -> 2
size cluster i: 279 -> 2
size cluster i: 280 -> 3
size cluster i: 281 -> 2
size cluster i: 282 -> 2
size cluster i: 290 -> 3
size cluster i: 296 -> 2
size cluster i: 300 -> 2
size cluster i: 302 -> 2
size cluster i: 308 -> 2
size cluster i: 309 -> 2
size cluster i: 311 -> 2
size cluster i: 322 -> 2
size cluster i: 324 -> 2
size cluster i: 330 -> 2
datapoints in clusters: 1001556
score: 0


Runtimes: 
--------- 
finished computation at Thu Jan  3 11:46:22 2019
elapsed time: 260.26s


Finishing: 
---------- 
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:227960] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:227960] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 334
datapoints_clusters: 1001556
score: 0.0
elapsed_time: 260.26

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 333.333333333 percent_correct: 0.7337686274509804
-> new best_percent_correct:0.7337686274509804new best_threshold:333.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[20559,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.115s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 237.651s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.02857s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38955
find clusters duration: 0.304804s
detected clusters: 190
size cluster i: 0 -> 10008
size cluster i: 1 -> 10005
size cluster i: 2 -> 20021
size cluster i: 3 -> 20018
size cluster i: 4 -> 10002
size cluster i: 5 -> 10004
size cluster i: 6 -> 10005
size cluster i: 7 -> 10012
size cluster i: 8 -> 10006
size cluster i: 9 -> 10007
size cluster i: 10 -> 10006
size cluster i: 11 -> 10008
size cluster i: 12 -> 30021
size cluster i: 13 -> 10006
size cluster i: 14 -> 10003
size cluster i: 15 -> 10001
size cluster i: 16 -> 10006
size cluster i: 17 -> 40028
size cluster i: 18 -> 10010
size cluster i: 19 -> 10001
size cluster i: 20 -> 10003
size cluster i: 21 -> 10008
size cluster i: 22 -> 10003
size cluster i: 23 -> 10006
size cluster i: 24 -> 40031
size cluster i: 25 -> 10008
size cluster i: 26 -> 10007
size cluster i: 27 -> 10005
size cluster i: 28 -> 10004
size cluster i: 29 -> 20016
size cluster i: 30 -> 10009
size cluster i: 31 -> 10005
size cluster i: 32 -> 10004
size cluster i: 33 -> 10011
size cluster i: 34 -> 20013
size cluster i: 35 -> 10009
size cluster i: 36 -> 10007
size cluster i: 37 -> 10001
size cluster i: 38 -> 10005
size cluster i: 39 -> 10005
size cluster i: 40 -> 10003
size cluster i: 41 -> 10003
size cluster i: 42 -> 10006
size cluster i: 43 -> 20009
size cluster i: 44 -> 10003
size cluster i: 45 -> 10006
size cluster i: 46 -> 30028
size cluster i: 47 -> 10004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10003
size cluster i: 51 -> 10005
size cluster i: 52 -> 10009
size cluster i: 53 -> 10007
size cluster i: 54 -> 10015
size cluster i: 55 -> 10006
size cluster i: 56 -> 10002
size cluster i: 57 -> 10008
size cluster i: 58 -> 10005
size cluster i: 59 -> 10004
size cluster i: 60 -> 10008
size cluster i: 61 -> 10006
size cluster i: 62 -> 20010
size cluster i: 63 -> 10007
size cluster i: 64 -> 10005
size cluster i: 65 -> 10011
size cluster i: 66 -> 10008
size cluster i: 67 -> 10002
size cluster i: 68 -> 10009
size cluster i: 69 -> 10015
size cluster i: 70 -> 10004
size cluster i: 71 -> 10005
size cluster i: 72 -> 10005
size cluster i: 73 -> 10006
size cluster i: 74 -> 10002
size cluster i: 75 -> 10000
size cluster i: 76 -> 10005
size cluster i: 77 -> 10008
size cluster i: 78 -> 10009
size cluster i: 79 -> 10001
size cluster i: 80 -> 10005
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10005
size cluster i: 84 -> 2
size cluster i: 85 -> 4
size cluster i: 86 -> 2
size cluster i: 87 -> 2
size cluster i: 88 -> 3
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 92 -> 2
size cluster i: 93 -> 4
size cluster i: 94 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 4
size cluster i: 98 -> 2
size cluster i: 99 -> 3
size cluster i: 100 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 4
size cluster i: 103 -> 2
size cluster i: 104 -> 3
size cluster i: 105 -> 3
size cluster i: 106 -> 2
size cluster i: 107 -> 2
size cluster i: 108 -> 4
size cluster i: 109 -> 3
size cluster i: 110 -> 3
size cluster i: 111 -> 3
size cluster i: 112 -> 2
size cluster i: 113 -> 5
size cluster i: 114 -> 3
size cluster i: 116 -> 3
size cluster i: 118 -> 2
size cluster i: 119 -> 2
size cluster i: 120 -> 2
size cluster i: 121 -> 2
size cluster i: 123 -> 5
size cluster i: 124 -> 2
size cluster i: 126 -> 2
size cluster i: 127 -> 2
size cluster i: 128 -> 2
size cluster i: 131 -> 2
size cluster i: 133 -> 2
size cluster i: 134 -> 2
size cluster i: 137 -> 3
size cluster i: 138 -> 2
size cluster i: 140 -> 2
size cluster i: 143 -> 3
size cluster i: 144 -> 2
size cluster i: 146 -> 4
size cluster i: 147 -> 2
size cluster i: 148 -> 2
size cluster i: 149 -> 2
size cluster i: 150 -> 2
size cluster i: 151 -> 2
size cluster i: 153 -> 3
size cluster i: 155 -> 4
size cluster i: 156 -> 2
size cluster i: 157 -> 2
size cluster i: 158 -> 3
size cluster i: 162 -> 2
size cluster i: 164 -> 2
size cluster i: 169 -> 2
size cluster i: 176 -> 2
size cluster i: 181 -> 2
size cluster i: 186 -> 2
datapoints in clusters: 1000812
score: 9.81188


Runtimes: 
--------- 
finished computation at Thu Jan  3 11:50:53 2019
elapsed time: 260.428s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:228342] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:228342] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 190
datapoints_clusters: 1000812
score: 9.81188
elapsed_time: 260.428

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 500.0 percent_correct: 0.8423411764705883
-> new best_percent_correct:0.8423411764705883new best_threshold:500.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 666.6666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[21786,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 666.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1101s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 237.77s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.00101s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38855
find clusters duration: 0.291985s
detected clusters: 139
size cluster i: 0 -> 10006
size cluster i: 1 -> 10004
size cluster i: 2 -> 20010
size cluster i: 3 -> 20016
size cluster i: 4 -> 10001
size cluster i: 5 -> 10003
size cluster i: 6 -> 10002
size cluster i: 7 -> 10004
size cluster i: 8 -> 10004
size cluster i: 9 -> 10004
size cluster i: 10 -> 10005
size cluster i: 11 -> 10004
size cluster i: 12 -> 30017
size cluster i: 13 -> 10005
size cluster i: 14 -> 10003
size cluster i: 15 -> 10001
size cluster i: 16 -> 10005
size cluster i: 17 -> 30012
size cluster i: 18 -> 10005
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10006
size cluster i: 22 -> 10003
size cluster i: 23 -> 10004
size cluster i: 24 -> 30021
size cluster i: 25 -> 10006
size cluster i: 26 -> 10005
size cluster i: 27 -> 10003
size cluster i: 28 -> 10003
size cluster i: 29 -> 20010
size cluster i: 30 -> 10007
size cluster i: 31 -> 10003
size cluster i: 32 -> 10004
size cluster i: 33 -> 10011
size cluster i: 34 -> 10005
size cluster i: 35 -> 10008
size cluster i: 36 -> 10005
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10005
size cluster i: 40 -> 10001
size cluster i: 41 -> 10003
size cluster i: 42 -> 10004
size cluster i: 43 -> 20007
size cluster i: 44 -> 10002
size cluster i: 45 -> 10004
size cluster i: 46 -> 10005
size cluster i: 47 -> 10003
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10005
size cluster i: 52 -> 10003
size cluster i: 53 -> 10005
size cluster i: 54 -> 10012
size cluster i: 55 -> 10006
size cluster i: 56 -> 10002
size cluster i: 57 -> 10007
size cluster i: 58 -> 10005
size cluster i: 59 -> 10002
size cluster i: 60 -> 10006
size cluster i: 61 -> 10005
size cluster i: 62 -> 10007
size cluster i: 63 -> 20009
size cluster i: 64 -> 10007
size cluster i: 65 -> 10004
size cluster i: 66 -> 20014
size cluster i: 67 -> 10009
size cluster i: 68 -> 10005
size cluster i: 69 -> 10000
size cluster i: 70 -> 10007
size cluster i: 71 -> 10009
size cluster i: 72 -> 10003
size cluster i: 73 -> 10004
size cluster i: 74 -> 10003
size cluster i: 75 -> 10004
size cluster i: 76 -> 10006
size cluster i: 77 -> 10002
size cluster i: 78 -> 10006
size cluster i: 79 -> 10000
size cluster i: 80 -> 10004
size cluster i: 81 -> 10006
size cluster i: 82 -> 10005
size cluster i: 83 -> 10001
size cluster i: 84 -> 10004
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10003
size cluster i: 88 -> 2
size cluster i: 89 -> 4
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 4
size cluster i: 98 -> 3
size cluster i: 99 -> 3
size cluster i: 102 -> 3
size cluster i: 104 -> 2
size cluster i: 106 -> 2
size cluster i: 108 -> 2
size cluster i: 110 -> 2
size cluster i: 112 -> 2
size cluster i: 113 -> 2
size cluster i: 114 -> 3
size cluster i: 115 -> 2
size cluster i: 116 -> 2
size cluster i: 118 -> 2
size cluster i: 119 -> 3
size cluster i: 121 -> 2
size cluster i: 122 -> 3
size cluster i: 124 -> 2
size cluster i: 128 -> 2
size cluster i: 131 -> 2
size cluster i: 132 -> 2
size cluster i: 136 -> 2
datapoints in clusters: 1000533
score: 59.8358


Runtimes: 
--------- 
finished computation at Thu Jan  3 11:55:23 2019
elapsed time: 260.523s


Finishing: 
---------- 
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:229027] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:229027] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 139
datapoints_clusters: 1000533
score: 59.8358
elapsed_time: 260.523

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 666.666666667 percent_correct: 0.8818303921568628
-> new best_percent_correct:0.8818303921568628new best_threshold:666.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 833.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[43937,1],1]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 833.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1788s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 237.699s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.07203s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38780
find clusters duration: 0.291472s
detected clusters: 119
size cluster i: 0 -> 10006
size cluster i: 1 -> 10003
size cluster i: 2 -> 20009
size cluster i: 3 -> 20009
size cluster i: 4 -> 10001
size cluster i: 5 -> 10003
size cluster i: 6 -> 10001
size cluster i: 7 -> 10002
size cluster i: 8 -> 10004
size cluster i: 9 -> 10004
size cluster i: 10 -> 10003
size cluster i: 11 -> 10004
size cluster i: 12 -> 20010
size cluster i: 13 -> 10003
size cluster i: 14 -> 10002
size cluster i: 15 -> 10001
size cluster i: 16 -> 10004
size cluster i: 17 -> 20003
size cluster i: 18 -> 10003
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10003
size cluster i: 22 -> 10002
size cluster i: 23 -> 10004
size cluster i: 24 -> 30018
size cluster i: 25 -> 10005
size cluster i: 26 -> 10003
size cluster i: 27 -> 10006
size cluster i: 28 -> 10002
size cluster i: 29 -> 10001
size cluster i: 30 -> 20009
size cluster i: 31 -> 10006
size cluster i: 32 -> 10002
size cluster i: 33 -> 10002
size cluster i: 34 -> 10004
size cluster i: 35 -> 10010
size cluster i: 36 -> 10003
size cluster i: 37 -> 10004
size cluster i: 38 -> 10003
size cluster i: 39 -> 10001
size cluster i: 40 -> 10004
size cluster i: 41 -> 10004
size cluster i: 42 -> 10001
size cluster i: 43 -> 10002
size cluster i: 44 -> 10004
size cluster i: 45 -> 20007
size cluster i: 46 -> 10002
size cluster i: 47 -> 10003
size cluster i: 48 -> 10002
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10002
size cluster i: 52 -> 10002
size cluster i: 53 -> 10004
size cluster i: 54 -> 10002
size cluster i: 55 -> 10003
size cluster i: 56 -> 10009
size cluster i: 57 -> 10003
size cluster i: 58 -> 10002
size cluster i: 59 -> 10007
size cluster i: 60 -> 10004
size cluster i: 61 -> 10002
size cluster i: 62 -> 10006
size cluster i: 63 -> 10002
size cluster i: 64 -> 10005
size cluster i: 65 -> 20008
size cluster i: 66 -> 10003
size cluster i: 67 -> 10004
size cluster i: 68 -> 20009
size cluster i: 69 -> 10006
size cluster i: 70 -> 10004
size cluster i: 71 -> 10000
size cluster i: 72 -> 10007
size cluster i: 73 -> 10007
size cluster i: 74 -> 10003
size cluster i: 75 -> 10003
size cluster i: 76 -> 10002
size cluster i: 77 -> 10003
size cluster i: 78 -> 10004
size cluster i: 79 -> 10001
size cluster i: 80 -> 10006
size cluster i: 81 -> 9996
size cluster i: 82 -> 10002
size cluster i: 83 -> 10002
size cluster i: 84 -> 10003
size cluster i: 85 -> 10000
size cluster i: 86 -> 10003
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10002
size cluster i: 90 -> 3
size cluster i: 91 -> 2
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 2
size cluster i: 95 -> 4
size cluster i: 96 -> 3
size cluster i: 99 -> 3
size cluster i: 100 -> 2
size cluster i: 101 -> 3
size cluster i: 102 -> 2
size cluster i: 104 -> 2
size cluster i: 105 -> 3
size cluster i: 106 -> 2
size cluster i: 107 -> 2
size cluster i: 108 -> 2
size cluster i: 109 -> 2
size cluster i: 111 -> 2
size cluster i: 113 -> 2
size cluster i: 115 -> 2
datapoints in clusters: 1000383
score: 79.4422


Runtimes: 
--------- 
finished computation at Thu Jan  3 11:59:54 2019
elapsed time: 260.589s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Node 2: Exiting... 
Cleanup done!
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 43 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:229400] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:229400] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 119
datapoints_clusters: 1000383
score: 79.4422
elapsed_time: 260.589

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 833.333333333 percent_correct: 0.9015774509803921
-> new best_percent_correct:0.9015774509803921new best_threshold:833.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 1000.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[43714,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 1000


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2597s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 237.728s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.94232s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38732
find clusters duration: 0.273409s
detected clusters: 108
size cluster i: 0 -> 10005
size cluster i: 1 -> 10002
size cluster i: 2 -> 20005
size cluster i: 3 -> 10003
size cluster i: 4 -> 10000
size cluster i: 5 -> 10002
size cluster i: 6 -> 10000
size cluster i: 7 -> 10001
size cluster i: 8 -> 10003
size cluster i: 9 -> 10004
size cluster i: 10 -> 10002
size cluster i: 11 -> 10003
size cluster i: 12 -> 20010
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10001
size cluster i: 16 -> 10004
size cluster i: 17 -> 20002
size cluster i: 18 -> 10003
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10003
size cluster i: 22 -> 10002
size cluster i: 23 -> 10004
size cluster i: 24 -> 20006
size cluster i: 25 -> 10005
size cluster i: 26 -> 10004
size cluster i: 27 -> 10002
size cluster i: 28 -> 10004
size cluster i: 29 -> 10002
size cluster i: 30 -> 10001
size cluster i: 31 -> 20008
size cluster i: 32 -> 10006
size cluster i: 33 -> 10002
size cluster i: 34 -> 10001
size cluster i: 35 -> 10003
size cluster i: 36 -> 10005
size cluster i: 37 -> 10003
size cluster i: 38 -> 10004
size cluster i: 39 -> 10004
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 10003
size cluster i: 44 -> 10001
size cluster i: 45 -> 10001
size cluster i: 46 -> 10003
size cluster i: 47 -> 20006
size cluster i: 48 -> 10002
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 10003
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 10005
size cluster i: 59 -> 10002
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10004
size cluster i: 63 -> 10002
size cluster i: 64 -> 10006
size cluster i: 65 -> 10002
size cluster i: 66 -> 10005
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10003
size cluster i: 70 -> 10003
size cluster i: 71 -> 10006
size cluster i: 72 -> 10004
size cluster i: 73 -> 10000
size cluster i: 74 -> 10004
size cluster i: 75 -> 10002
size cluster i: 76 -> 10003
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10001
size cluster i: 80 -> 10003
size cluster i: 81 -> 10003
size cluster i: 82 -> 10003
size cluster i: 83 -> 10001
size cluster i: 84 -> 10004
size cluster i: 85 -> 9953
size cluster i: 86 -> 10001
size cluster i: 87 -> 10002
size cluster i: 88 -> 10003
size cluster i: 89 -> 10000
size cluster i: 90 -> 10003
size cluster i: 91 -> 10000
size cluster i: 92 -> 10000
size cluster i: 93 -> 10001
size cluster i: 94 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 3
size cluster i: 98 -> 2
size cluster i: 100 -> 3
size cluster i: 102 -> 2
size cluster i: 103 -> 2
size cluster i: 104 -> 2
datapoints in clusters: 1000230
score: 90.2168


Runtimes: 
--------- 
finished computation at Thu Jan  3 12:04:25 2019
elapsed time: 260.544s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:229755] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:229755] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 108
datapoints_clusters: 1000230
score: 90.2168
elapsed_time: 260.544

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 1000.0 percent_correct: 0.9408588235294117
-> new best_percent_correct:0.9408588235294117new best_threshold:1000.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 1166.6666666666665 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[43359,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 1166.67


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2955s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 237.822s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.02085s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38698
find clusters duration: 0.277042s
detected clusters: 102
size cluster i: 0 -> 10005
size cluster i: 1 -> 10001
size cluster i: 2 -> 20005
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10002
size cluster i: 9 -> 10003
size cluster i: 10 -> 10001
size cluster i: 11 -> 10003
size cluster i: 12 -> 20006
size cluster i: 13 -> 10002
size cluster i: 14 -> 10000
size cluster i: 15 -> 10001
size cluster i: 16 -> 10002
size cluster i: 17 -> 20001
size cluster i: 18 -> 10003
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10002
size cluster i: 22 -> 10002
size cluster i: 23 -> 10003
size cluster i: 24 -> 20005
size cluster i: 25 -> 10005
size cluster i: 26 -> 10002
size cluster i: 27 -> 10002
size cluster i: 28 -> 10002
size cluster i: 29 -> 10001
size cluster i: 30 -> 9997
size cluster i: 31 -> 20007
size cluster i: 32 -> 10006
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10003
size cluster i: 36 -> 10004
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9995
size cluster i: 42 -> 10003
size cluster i: 43 -> 10003
size cluster i: 44 -> 10001
size cluster i: 45 -> 10001
size cluster i: 46 -> 10003
size cluster i: 47 -> 20006
size cluster i: 48 -> 10002
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 10003
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 10004
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10004
size cluster i: 63 -> 10001
size cluster i: 64 -> 10006
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10003
size cluster i: 70 -> 10002
size cluster i: 71 -> 10004
size cluster i: 72 -> 10004
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10002
size cluster i: 76 -> 10003
size cluster i: 77 -> 10001
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10003
size cluster i: 81 -> 10002
size cluster i: 82 -> 10003
size cluster i: 83 -> 10001
size cluster i: 84 -> 10003
size cluster i: 85 -> 9700
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10002
size cluster i: 89 -> 10000
size cluster i: 90 -> 10003
size cluster i: 91 -> 10000
size cluster i: 92 -> 10000
size cluster i: 93 -> 10001
size cluster i: 94 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 3
size cluster i: 98 -> 2
size cluster i: 100 -> 2
datapoints in clusters: 999907
score: 96.0695


Runtimes: 
--------- 
finished computation at Thu Jan  3 12:08:55 2019
elapsed time: 260.738s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 6: Exiting... 
Node 3: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:230118] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:230118] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 102
datapoints_clusters: 999907
score: 96.0695
elapsed_time: 260.738

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 1166.66666667 percent_correct: 0.9406637254901961

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 1333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[45035,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 1333.33


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1352s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 237.905s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.92451s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38651
find clusters duration: 0.271235s
detected clusters: 101
size cluster i: 0 -> 10003
size cluster i: 1 -> 10001
size cluster i: 2 -> 20004
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10002
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10002
size cluster i: 12 -> 20006
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 10000
size cluster i: 25 -> 10004
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10002
size cluster i: 29 -> 10001
size cluster i: 30 -> 9974
size cluster i: 31 -> 20007
size cluster i: 32 -> 10006
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10002
size cluster i: 36 -> 10004
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9974
size cluster i: 42 -> 10003
size cluster i: 43 -> 10003
size cluster i: 44 -> 10001
size cluster i: 45 -> 9994
size cluster i: 46 -> 10002
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 10003
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 10003
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10003
size cluster i: 72 -> 10003
size cluster i: 73 -> 10003
size cluster i: 74 -> 10000
size cluster i: 75 -> 10002
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10003
size cluster i: 84 -> 10001
size cluster i: 85 -> 10003
size cluster i: 86 -> 9149
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10002
size cluster i: 90 -> 10000
size cluster i: 91 -> 10003
size cluster i: 92 -> 10000
size cluster i: 93 -> 9999
size cluster i: 94 -> 10001
size cluster i: 95 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 2
datapoints in clusters: 999264
score: 96.9874


Runtimes: 
--------- 
finished computation at Thu Jan  3 12:13:26 2019
elapsed time: 260.596s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:230482] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:230482] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 101
datapoints_clusters: 999264
score: 96.9874
elapsed_time: 260.596

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 1333.33333333 percent_correct: 0.9499156862745098
-> new best_percent_correct:0.9499156862745098new best_threshold:1333.33333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 1500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[44561,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 1500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1786s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 237.698s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.97403s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38626
find clusters duration: 0.280746s
detected clusters: 99
size cluster i: 0 -> 10003
size cluster i: 1 -> 9999
size cluster i: 2 -> 20003
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10001
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 20005
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9912
size cluster i: 31 -> 20007
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10003
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9917
size cluster i: 42 -> 10003
size cluster i: 43 -> 10003
size cluster i: 44 -> 10000
size cluster i: 45 -> 9943
size cluster i: 46 -> 10001
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10002
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10002
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10004
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10003
size cluster i: 73 -> 10003
size cluster i: 74 -> 10000
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10003
size cluster i: 84 -> 10001
size cluster i: 85 -> 10003
size cluster i: 86 -> 8293
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10002
size cluster i: 92 -> 9999
size cluster i: 93 -> 9980
size cluster i: 94 -> 10001
size cluster i: 95 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 998183
score: 96.8825


Runtimes: 
--------- 
finished computation at Thu Jan  3 12:17:56 2019
elapsed time: 260.459s


Finishing: 
---------- 
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 2: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 1: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:230824] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:230824] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 998183
score: 96.8825
elapsed_time: 260.459

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 1500.0 percent_correct: 0.9489205882352941
thresholds:[ 1203.7037037   1240.74074074  1277.77777778  1314.81481481  1351.85185185
  1388.88888889  1425.92592593  1462.96296296], threshold_step:37.037037037

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 1203.7037037037035 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[44206,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 1203.7


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1321s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 237.598s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.00884s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38677
find clusters duration: 0.27422s
detected clusters: 102
size cluster i: 0 -> 10004
size cluster i: 1 -> 10001
size cluster i: 2 -> 20005
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10002
size cluster i: 9 -> 10003
size cluster i: 10 -> 10001
size cluster i: 11 -> 10003
size cluster i: 12 -> 20006
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10002
size cluster i: 17 -> 20001
size cluster i: 18 -> 10003
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10002
size cluster i: 22 -> 10002
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10005
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10002
size cluster i: 29 -> 10001
size cluster i: 30 -> 9993
size cluster i: 31 -> 20007
size cluster i: 32 -> 10006
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10003
size cluster i: 36 -> 10004
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9993
size cluster i: 42 -> 10003
size cluster i: 43 -> 10003
size cluster i: 44 -> 10001
size cluster i: 45 -> 10001
size cluster i: 46 -> 10003
size cluster i: 47 -> 20006
size cluster i: 48 -> 10002
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 10003
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 10004
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10004
size cluster i: 63 -> 10001
size cluster i: 64 -> 10006
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10003
size cluster i: 70 -> 10002
size cluster i: 71 -> 10004
size cluster i: 72 -> 10004
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10001
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10003
size cluster i: 81 -> 10002
size cluster i: 82 -> 10003
size cluster i: 83 -> 10001
size cluster i: 84 -> 10003
size cluster i: 85 -> 9607
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10002
size cluster i: 89 -> 10000
size cluster i: 90 -> 10003
size cluster i: 91 -> 10000
size cluster i: 92 -> 10000
size cluster i: 93 -> 10001
size cluster i: 94 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 3
size cluster i: 98 -> 2
size cluster i: 100 -> 2
datapoints in clusters: 999800
score: 96.0592


Runtimes: 
--------- 
finished computation at Thu Jan  3 12:22:26 2019
elapsed time: 260.356s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 2: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 3: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:231191] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:231191] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 102
datapoints_clusters: 999800
score: 96.0592
elapsed_time: 260.356

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 1203.7037037 percent_correct: 0.9405745098039215

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 1240.7407407407406 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[41772,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 1240.74


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0886s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 237.738s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.19864s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38670
find clusters duration: 0.274482s
detected clusters: 101
size cluster i: 0 -> 10004
size cluster i: 1 -> 10001
size cluster i: 2 -> 20004
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10002
size cluster i: 9 -> 10003
size cluster i: 10 -> 10001
size cluster i: 11 -> 10002
size cluster i: 12 -> 20006
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10003
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10005
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10002
size cluster i: 29 -> 10001
size cluster i: 30 -> 9987
size cluster i: 31 -> 20007
size cluster i: 32 -> 10006
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10003
size cluster i: 36 -> 10004
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9990
size cluster i: 42 -> 10003
size cluster i: 43 -> 10003
size cluster i: 44 -> 10001
size cluster i: 45 -> 9999
size cluster i: 46 -> 10003
size cluster i: 47 -> 20006
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 10003
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 10003
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10001
size cluster i: 64 -> 10006
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10003
size cluster i: 70 -> 10002
size cluster i: 71 -> 10004
size cluster i: 72 -> 10003
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10001
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10002
size cluster i: 82 -> 10003
size cluster i: 83 -> 10001
size cluster i: 84 -> 10003
size cluster i: 85 -> 9499
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10002
size cluster i: 89 -> 10000
size cluster i: 90 -> 10003
size cluster i: 91 -> 10000
size cluster i: 92 -> 10000
size cluster i: 93 -> 10001
size cluster i: 94 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 99 -> 2
datapoints in clusters: 999669
score: 97.0267


Runtimes: 
--------- 
finished computation at Thu Jan  3 12:26:57 2019
elapsed time: 260.637s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:231573] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:231573] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 101
datapoints_clusters: 999669
score: 97.0267
elapsed_time: 260.637

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 1240.74074074 percent_correct: 0.9404696078431373

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 1277.7777777777776 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[41542,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 1277.78


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1115s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 237.495s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.06305s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38664
find clusters duration: 0.272759s
detected clusters: 100
size cluster i: 0 -> 10004
size cluster i: 1 -> 10001
size cluster i: 2 -> 20004
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10002
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10002
size cluster i: 12 -> 20006
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10004
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10002
size cluster i: 29 -> 10001
size cluster i: 30 -> 9981
size cluster i: 31 -> 20007
size cluster i: 32 -> 10006
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10002
size cluster i: 36 -> 10004
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9988
size cluster i: 42 -> 10003
size cluster i: 43 -> 10003
size cluster i: 44 -> 10001
size cluster i: 45 -> 9998
size cluster i: 46 -> 10003
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 10003
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 10003
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10003
size cluster i: 72 -> 10003
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10001
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10002
size cluster i: 82 -> 10003
size cluster i: 83 -> 10001
size cluster i: 84 -> 10003
size cluster i: 85 -> 9374
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10002
size cluster i: 89 -> 10000
size cluster i: 90 -> 10003
size cluster i: 91 -> 10000
size cluster i: 92 -> 10000
size cluster i: 93 -> 10001
size cluster i: 94 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 999520
score: 97.9922


Runtimes: 
--------- 
finished computation at Thu Jan  3 12:31:27 2019
elapsed time: 260.317s


Finishing: 
---------- 
Beginning cleanup...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:231935] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:231935] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 100
datapoints_clusters: 999520
score: 97.9922
elapsed_time: 260.317

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 1277.77777778 percent_correct: 0.9403529411764706

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 1314.8148148148148 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[41175,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 1314.81


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.154s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 237.737s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.01903s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38652
find clusters duration: 0.271203s
detected clusters: 101
size cluster i: 0 -> 10003
size cluster i: 1 -> 10001
size cluster i: 2 -> 20004
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10002
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10002
size cluster i: 12 -> 20006
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 10000
size cluster i: 25 -> 10004
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10002
size cluster i: 29 -> 10001
size cluster i: 30 -> 9975
size cluster i: 31 -> 20007
size cluster i: 32 -> 10006
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10002
size cluster i: 36 -> 10004
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9978
size cluster i: 42 -> 10003
size cluster i: 43 -> 10003
size cluster i: 44 -> 10001
size cluster i: 45 -> 9996
size cluster i: 46 -> 10003
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 10003
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 10003
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10003
size cluster i: 72 -> 10003
size cluster i: 73 -> 10003
size cluster i: 74 -> 10000
size cluster i: 75 -> 10002
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10003
size cluster i: 84 -> 10001
size cluster i: 85 -> 10003
size cluster i: 86 -> 9228
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10002
size cluster i: 90 -> 10000
size cluster i: 91 -> 10003
size cluster i: 92 -> 10000
size cluster i: 93 -> 9999
size cluster i: 94 -> 10001
size cluster i: 95 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 2
datapoints in clusters: 999351
score: 96.9958


Runtimes: 
--------- 
finished computation at Thu Jan  3 12:35:57 2019
elapsed time: 260.538s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 7: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:232302] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:232302] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 101
datapoints_clusters: 999351
score: 96.9958
elapsed_time: 260.538

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 1314.81481481 percent_correct: 0.9499990196078432
-> new best_percent_correct:0.9499990196078432new best_threshold:1314.81481481

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 1351.8518518518517 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[42840,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 1351.85


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1744s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 237.731s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.05241s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38649
find clusters duration: 0.275404s
detected clusters: 100
size cluster i: 0 -> 10003
size cluster i: 1 -> 10001
size cluster i: 2 -> 20004
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10002
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10002
size cluster i: 12 -> 20006
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 10000
size cluster i: 25 -> 10004
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10002
size cluster i: 29 -> 10001
size cluster i: 30 -> 9970
size cluster i: 31 -> 20007
size cluster i: 32 -> 10006
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10004
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9965
size cluster i: 42 -> 10003
size cluster i: 43 -> 10003
size cluster i: 44 -> 10001
size cluster i: 45 -> 9993
size cluster i: 46 -> 10002
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 10003
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10003
size cluster i: 72 -> 10003
size cluster i: 73 -> 10003
size cluster i: 74 -> 10000
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10003
size cluster i: 84 -> 10001
size cluster i: 85 -> 10003
size cluster i: 86 -> 9075
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10002
size cluster i: 90 -> 10000
size cluster i: 91 -> 10002
size cluster i: 92 -> 10000
size cluster i: 93 -> 9999
size cluster i: 94 -> 10001
size cluster i: 95 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 2
datapoints in clusters: 999171
score: 97.9579


Runtimes: 
--------- 
finished computation at Thu Jan  3 12:40:28 2019
elapsed time: 260.59s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Beginning cleanup...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 8: Exiting... 
Node 4: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:232673] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:232673] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 100
datapoints_clusters: 999171
score: 97.9579
elapsed_time: 260.59

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 1351.85185185 percent_correct: 0.9498343137254902

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 1388.888888888889 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[42477,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 1388.89


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1629s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 237.693s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.08447s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38654
find clusters duration: 0.274031s
detected clusters: 99
size cluster i: 0 -> 10003
size cluster i: 1 -> 10001
size cluster i: 2 -> 20004
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10002
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10002
size cluster i: 12 -> 20006
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 10000
size cluster i: 25 -> 10004
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10002
size cluster i: 29 -> 10001
size cluster i: 30 -> 9961
size cluster i: 31 -> 20007
size cluster i: 32 -> 10006
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10004
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9954
size cluster i: 42 -> 10003
size cluster i: 43 -> 10003
size cluster i: 44 -> 10001
size cluster i: 45 -> 9989
size cluster i: 46 -> 10002
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10002
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 10003
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10005
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10003
size cluster i: 72 -> 10003
size cluster i: 73 -> 10003
size cluster i: 74 -> 10000
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10003
size cluster i: 84 -> 10001
size cluster i: 85 -> 10003
size cluster i: 86 -> 8901
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10002
size cluster i: 92 -> 10000
size cluster i: 93 -> 9997
size cluster i: 94 -> 10001
size cluster i: 95 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 998965
score: 96.9584


Runtimes: 
--------- 
finished computation at Thu Jan  3 12:44:58 2019
elapsed time: 260.559s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 5: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:233044] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:233044] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 998965
score: 96.9584
elapsed_time: 260.559

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 1388.88888889 percent_correct: 0.9496441176470588

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 1425.9259259259259 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[42097,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 1425.93


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1705s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 237.44s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.19569s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38644
find clusters duration: 0.271133s
detected clusters: 99
size cluster i: 0 -> 10003
size cluster i: 1 -> 10001
size cluster i: 2 -> 20004
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10002
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 20005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 10000
size cluster i: 25 -> 10004
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9942
size cluster i: 31 -> 20007
size cluster i: 32 -> 10006
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10004
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9940
size cluster i: 42 -> 10003
size cluster i: 43 -> 10003
size cluster i: 44 -> 10000
size cluster i: 45 -> 9979
size cluster i: 46 -> 10001
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10002
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 10002
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10005
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10004
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10003
size cluster i: 72 -> 10003
size cluster i: 73 -> 10003
size cluster i: 74 -> 10000
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10003
size cluster i: 84 -> 10001
size cluster i: 85 -> 10003
size cluster i: 86 -> 8710
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10002
size cluster i: 92 -> 10000
size cluster i: 93 -> 9993
size cluster i: 94 -> 10001
size cluster i: 95 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 998720
score: 96.9346


Runtimes: 
--------- 
finished computation at Thu Jan  3 12:49:28 2019
elapsed time: 260.421s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:233416] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:233416] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 998720
score: 96.9346
elapsed_time: 260.421

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 1425.92592593 percent_correct: 0.9494176470588235

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-08 --k 6 --epsilon 0.001 --threshold 1462.962962962963 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[47746,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-08
k: 6
threshold: 1462.96


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1664s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228253
delta: 0.0547116
delta: 0.0449019
delta: 0.0325578
delta: 0.0392336
delta: 0.0637129
delta: 0.0383647
delta: 0.029344
delta: 0.0439586
delta: 0.0189976
delta: 0.0938873
delta: 0.0718092
delta: 0.026054
delta: 0.0124393
delta: 0.0194542
delta: 0.0735284
delta: 0.0201988
delta: 0.0208183
delta: 0.0475186
delta: 0.0361077
delta: 0.010549
delta: 0.161856
delta: 0.0188203
delta: 0.0439934
delta: 0.0105509
delta: 0.0250405
delta: 0.12436
delta: 0.012757
delta: 0.0209551
delta: 0.00874869
delta: 0.0063427
delta: 0.0842125
delta: 0.00797435
delta: 0.0143201
delta: 0.0531446
delta: 0.00785014
delta: 0.122226
delta: 0.0105153
delta: 0.0084594
delta: 0.00565801
delta: 0.00523491
delta: 0.0337527
delta: 0.0123491
delta: 0.0383729
delta: 0.00318719
delta: 0.00822046
delta: 0.0337101
delta: 0.00505696
delta: 0.0149511
delta: 0.00507251
delta: 0.0104817
delta: 0.0056341
delta: 0.0574327
delta: 0.00392711
delta: 0.0137299
delta: 0.00163272
delta: 0.0365787
delta: 0.00412995
delta: 0.0101945
delta: 0.00497234
delta: 0.00393071
delta: 0.0158254
delta: 0.00216232
delta: 0.00300419
delta: 0.00999327
delta: 0.00335477
delta: 0.00259958
delta: 0.00346769
delta: 0.00465468
delta: 0.00431648
delta: 0.00167871
delta: 0.0145707
delta: 0.00223547
delta: 0.0154565
delta: 0.00131827
delta: 0.00350762
delta: 0.0170344
delta: 0.00325877
delta: 0.00171101
delta: 0.00254689
delta: 0.00330057
delta: 0.0079185
delta: 0.00239183
delta: 0.00388644
delta: 0.00110882
delta: 0.00115932
delta: 0.0178028
delta: 0.00507955
delta: 0.00139833
delta: 0.00106752
delta: 0.000692976
delta: 0.0206034
delta: 0.00147943
delta: 0.000797272
delta: 0.00816141
delta: 0.00206287
delta: 0.009206
delta: 0.000707611
delta: 0.00115673
delta: 0.00116218
delta: 0.00152754
delta: 0.00146284
delta: 0.00516918
delta: 0.000378284
delta: 0.00262637
delta: 0.00043341
delta: 0.000437646
delta: 0.000530168
delta: 0.00237396
delta: 0.00115763
delta: 0.00121084
delta: 0.00319916
delta: 0.000404975
delta: 0.00100275
delta: 0.000753359
delta: 0.00112104
delta: 0.000677965
delta: 0.0022626
delta: 0.000557778
delta: 0.000281821
delta: 0.00503373
delta: 0.00146478
delta: 0.000781644
delta: 0.000887576
delta: 0.00024026
delta: 0.000790223
delta: 0.000747556
delta: 0.000415111
delta: 0.000349213
delta: 0.00027993
delta: 0.000476564
delta: 0.000483562
delta: 0.00229017
delta: 0.000435526
delta: 0.000104817
delta: 0.0037682
delta: 0.000363757
delta: 0.0003984
delta: 0.000304273
delta: 0.000456281
delta: 0.00218232
delta: 0.000206587
delta: 0.000710172
delta: 0.000213135
delta: 0.000241099
delta: 0.00141568
delta: 0.000788072
delta: 0.000746614
delta: 0.000150109
delta: 0.000310495
delta: 0.00134424
delta: 9.2051e-05
delta: 0.000425639
delta: 6.77059e-05
delta: 0.000245597
delta: 0.000603052
delta: 0.000651168
delta: 9.64863e-05
delta: 0.000316765
delta: 6.36364e-05
delta: 0.000416388
delta: 0.00149863
delta: 0.000108779
delta: 9.78798e-05
delta: 6.3234e-05
delta: 9.59627e-05
delta: 0.000214655
delta: 0.000354116
delta: 0.000115226
delta: 7.45752e-05
delta: 0.00016339
delta: 0.000440991
delta: 0.000127404
delta: 8.97589e-05
delta: 0.000163254
delta: 8.67449e-05
delta: 0.000119403
delta: 0.00115314
delta: 6.54013e-05
delta: 9.15124e-05
delta: 0.000120331
delta: 5.76165e-05
delta: 0.00011864
delta: 0.000184019
delta: 4.59383e-05
delta: 6.142e-05
delta: 0.000195506
delta: 0.000197833
delta: 4.96987e-05
delta: 0.000222866
delta: 0.000148648
delta: 3.12387e-05
delta: 3.68801e-05
delta: 0.000341629
delta: 2.64117e-05
delta: 7.84742e-05
delta: 3.99569e-05
delta: 1.39954e-05
delta: 8.54664e-05
delta: 0.000395148
delta: 1.9033e-05
delta: 8.30021e-05
delta: 2.71035e-05
delta: 0.000176768
delta: 0.000226329
delta: 4.76046e-05
delta: 2.22119e-05
delta: 0.000110228
delta: 7.28028e-05
delta: 3.34017e-05
delta: 3.56882e-05
delta: 0.00010974
delta: 4.36122e-05
delta: 2.50137e-05
delta: 8.89697e-05
delta: 1.61201e-05
delta: 3.00728e-05
delta: 3.10846e-05
delta: 0.000245985
delta: 0.000145434
delta: 4.60954e-05
delta: 1.51681e-05
delta: 1.47512e-05
delta: 0.000197476
delta: 3.44087e-05
delta: 2.44904e-05
delta: 3.82113e-05
delta: 1.44818e-05
delta: 6.6556e-05
delta: 3.61251e-05
delta: 3.61569e-05
delta: 5.2673e-06
delta: 6.5185e-06
delta: 0.000114347
delta: 3.44184e-05
delta: 2.785e-05
delta: 5.818e-06
delta: 5.73267e-06
delta: 2.65919e-05
delta: 5.60895e-06
delta: 5.98192e-05
delta: 1.71464e-05
delta: 5.81895e-06
delta: 4.96227e-05
delta: 8.44596e-06
delta: 9.27738e-06
delta: 8.47178e-06
delta: 4.11506e-05
delta: 0.000204706
delta: 2.63067e-05
delta: 1.01424e-05
delta: 9.68402e-06
delta: 5.19812e-06
delta: 7.21882e-06
delta: 7.4366e-05
delta: 7.24973e-06
delta: 4.92302e-06
delta: 3.03281e-05
delta: 2.0551e-05
delta: 7.3957e-06
delta: 2.50983e-05
delta: 1.90249e-05
delta: 5.16421e-06
delta: 4.1213e-06
delta: 6.36259e-06
delta: 2.10345e-06
delta: 4.19819e-06
delta: 1.78204e-05
delta: 6.52428e-06
delta: 6.39905e-06
delta: 2.99821e-06
delta: 7.65055e-06
delta: 3.21679e-05
delta: 3.75215e-06
delta: 6.41536e-06
delta: 6.05622e-06
delta: 9.88853e-06
delta: 3.23754e-05
delta: 9.34947e-06
delta: 2.2158e-06
delta: 4.13172e-06
delta: 4.3939e-06
delta: 3.49057e-05
delta: 2.06929e-06
delta: 1.29066e-05
delta: 2.05233e-06
delta: 6.03098e-06
delta: 1.11089e-06
delta: 4.18886e-06
delta: 8.51797e-06
delta: 5.32172e-06
delta: 9.04663e-06
delta: 4.91502e-06
delta: 2.66808e-06
delta: 1.78714e-06
delta: 3.99521e-06
delta: 2.55801e-06
delta: 4.5766e-06
delta: 4.52254e-06
delta: 1.90197e-06
delta: 2.67597e-06
delta: 6.0287e-06
delta: 3.67309e-06
delta: 7.90176e-07
delta: 8.5002e-07
delta: 3.94611e-06
delta: 4.24319e-06
delta: 6.13501e-06
delta: 1.62402e-06
delta: 1.22603e-06
delta: 2.73742e-06
delta: 2.50335e-06
delta: 2.14848e-06
delta: 7.0376e-06
delta: 1.4628e-06
delta: 1.31127e-06
delta: 2.88761e-06
delta: 9.37486e-07
delta: 7.20312e-07
delta: 1.75564e-06
delta: 1.60754e-05
delta: 2.4193e-06
delta: 8.86516e-07
delta: 1.13547e-06
delta: 1.50054e-06
delta: 5.80839e-06
delta: 4.53962e-06
delta: 7.78891e-07
delta: 5.67026e-07
delta: 1.51893e-06
delta: 1.02132e-06
delta: 1.54519e-06
delta: 2.52334e-06
delta: 1.9712e-06
delta: 8.79175e-07
delta: 1.99099e-06
delta: 9.01303e-07
delta: 1.49604e-06
delta: 1.14786e-06
delta: 7.25672e-07
delta: 1.84164e-06
delta: 2.97932e-06
delta: 5.20219e-07
delta: 2.30236e-06
delta: 9.4865e-07
delta: 1.77496e-06
delta: 5.55645e-07
delta: 5.51087e-07
delta: 1.00103e-06
delta: 1.95782e-06
delta: 2.3576e-06
delta: 2.68642e-07
delta: 3.03205e-07
delta: 9.45928e-07
delta: 3.47726e-07
delta: 1.48091e-06
delta: 4.91236e-07
delta: 6.4129e-07
delta: 8.88051e-07
delta: 4.41072e-07
delta: 1.8121e-07
delta: 1.30391e-06
delta: 3.94418e-06
delta: 2.27677e-07
delta: 3.95291e-07
delta: 4.94281e-07
delta: 2.43408e-06
delta: 2.33988e-07
delta: 7.92857e-07
delta: 6.09278e-07
delta: 4.50019e-06
delta: 6.87334e-07
delta: 9.84884e-08
delta: 1.335e-06
delta: 1.40778e-06
delta: 3.83739e-07
delta: 3.85423e-07
delta: 2.65772e-07
delta: 1.72228e-06
delta: 1.85566e-07
delta: 7.19219e-07
delta: 3.15441e-07
delta: 3.44203e-07
delta: 1.76166e-06
delta: 6.90162e-08
delta: 1.50411e-07
delta: 5.09377e-07
delta: 5.23259e-07
delta: 3.37308e-07
delta: 5.44338e-08
delta: 9.82228e-07
delta: 3.13965e-07
delta: 4.81354e-07
delta: 3.98882e-07
delta: 7.27984e-08
delta: 6.99832e-07
delta: 1.27945e-07
delta: 4.08251e-07
delta: 2.07245e-07
delta: 1.13317e-06
delta: 1.19806e-06
delta: 7.08729e-08
delta: 3.97437e-07
delta: 9.73574e-07
delta: 1.76251e-07
delta: 4.2061e-07
delta: 7.57933e-08
delta: 1.34239e-06
delta: 1.54847e-07
delta: 1.26329e-07
delta: 2.43871e-07
delta: 2.31774e-07
delta: 3.81616e-07
delta: 8.72127e-08
delta: 1.24992e-07
delta: 7.11192e-07
delta: 1.0988e-07
delta: 1.23202e-07
delta: 3.03366e-07
delta: 1.94545e-06
delta: 1.57982e-07
delta: 6.25977e-08
delta: 1.38853e-07
delta: 2.37886e-07
delta: 2.19385e-07
delta: 6.99893e-08
delta: 1.47378e-07
delta: 4.02754e-07
delta: 9.59629e-08
delta: 5.55248e-08
delta: 1.88747e-07
delta: 3.2669e-07
delta: 1.13683e-07
delta: 8.83489e-08
delta: 3.34353e-08
Number of iterations: 435 (max. 1000)
Final norm of residuum: 3.34353e-08
counted_mult_calls: 444
solver duration: 237.495s
Number of grid points:     397825
alpha min: -11700.3 max: 39341.5
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.03343s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38623
find clusters duration: 0.276285s
detected clusters: 99
size cluster i: 0 -> 10003
size cluster i: 1 -> 9999
size cluster i: 2 -> 20003
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10002
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 20005
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9933
size cluster i: 31 -> 20007
size cluster i: 32 -> 10006
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10004
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9932
size cluster i: 42 -> 10003
size cluster i: 43 -> 10003
size cluster i: 44 -> 10000
size cluster i: 45 -> 9964
size cluster i: 46 -> 10001
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10002
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 10002
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10005
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10003
size cluster i: 73 -> 10003
size cluster i: 74 -> 10000
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10003
size cluster i: 84 -> 10001
size cluster i: 85 -> 10003
size cluster i: 86 -> 8511
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10002
size cluster i: 92 -> 10000
size cluster i: 93 -> 9986
size cluster i: 94 -> 10001
size cluster i: 95 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 998473
score: 96.9106


Runtimes: 
--------- 
finished computation at Thu Jan  3 12:53:58 2019
elapsed time: 260.324s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:233787] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:233787] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 998473
score: 96.9106
elapsed_time: 260.324

--------------- END RUN -----------------
attempt lambda: 1e-08 threshold: 1462.96296296 percent_correct: 0.9491892156862745
-> new overall_best_percent_correct:0.9499990196078432, overall_best_lambda_value: 1e-08, new best_threshold:1314.81481481
thresholds:[0.0, 166.66666666666666, 333.33333333333331, 500.0, 666.66666666666663, 833.33333333333326, 1000.0, 1166.6666666666665, 1333.3333333333333, 1500.0], threshold_step:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 0.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[47380,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 0


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2117s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.5706s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.06942s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37747
find clusters duration: 64.3822s
detected clusters: 1
size cluster i: 0 -> 1020000
datapoints in clusters: 1020000
score: 1


Runtimes: 
--------- 
finished computation at Thu Jan  3 12:57:03 2019
elapsed time: 174.558s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 7: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:234157] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:234157] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 1
datapoints_clusters: 1020000
score: 1.0
elapsed_time: 174.558

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 0.0 percent_correct: 0.00980392156862745
-> new best_percent_correct:0.00980392156862745new best_threshold:0.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 166.66666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[49056,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 166.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1119s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.0911s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.98496s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39384
find clusters duration: 5.5959s
detected clusters: 282
size cluster i: 0 -> 510863
size cluster i: 1 -> 10016
size cluster i: 2 -> 30047
size cluster i: 3 -> 40059
size cluster i: 4 -> 10002
size cluster i: 5 -> 10018
size cluster i: 6 -> 30058
size cluster i: 7 -> 10010
size cluster i: 8 -> 10013
size cluster i: 9 -> 10016
size cluster i: 10 -> 20034
size cluster i: 11 -> 10004
size cluster i: 12 -> 10009
size cluster i: 13 -> 10024
size cluster i: 14 -> 10008
size cluster i: 15 -> 10018
size cluster i: 16 -> 10015
size cluster i: 17 -> 10010
size cluster i: 18 -> 30064
size cluster i: 19 -> 10006
size cluster i: 20 -> 10015
size cluster i: 21 -> 10006
size cluster i: 22 -> 10016
size cluster i: 23 -> 20021
size cluster i: 24 -> 10012
size cluster i: 25 -> 10010
size cluster i: 26 -> 10008
size cluster i: 27 -> 10014
size cluster i: 28 -> 10013
size cluster i: 29 -> 10015
size cluster i: 30 -> 10012
size cluster i: 31 -> 10022
size cluster i: 32 -> 10019
size cluster i: 33 -> 10001
size cluster i: 34 -> 10013
size cluster i: 35 -> 10005
size cluster i: 36 -> 10010
size cluster i: 37 -> 10010
size cluster i: 38 -> 10016
size cluster i: 39 -> 2
size cluster i: 40 -> 2
size cluster i: 41 -> 2
size cluster i: 42 -> 3
size cluster i: 43 -> 2
size cluster i: 44 -> 2
size cluster i: 45 -> 2
size cluster i: 46 -> 3
size cluster i: 47 -> 4
size cluster i: 48 -> 4
size cluster i: 49 -> 5
size cluster i: 50 -> 3
size cluster i: 51 -> 2
size cluster i: 52 -> 2
size cluster i: 53 -> 2
size cluster i: 54 -> 2
size cluster i: 55 -> 3
size cluster i: 56 -> 2
size cluster i: 57 -> 4
size cluster i: 58 -> 5
size cluster i: 59 -> 3
size cluster i: 60 -> 2
size cluster i: 61 -> 2
size cluster i: 62 -> 3
size cluster i: 63 -> 3
size cluster i: 64 -> 3
size cluster i: 65 -> 2
size cluster i: 67 -> 5
size cluster i: 69 -> 5
size cluster i: 70 -> 2
size cluster i: 71 -> 3
size cluster i: 72 -> 2
size cluster i: 73 -> 2
size cluster i: 74 -> 2
size cluster i: 75 -> 2
size cluster i: 78 -> 2
size cluster i: 79 -> 2
size cluster i: 80 -> 3
size cluster i: 81 -> 2
size cluster i: 82 -> 5
size cluster i: 83 -> 4
size cluster i: 84 -> 2
size cluster i: 85 -> 5
size cluster i: 86 -> 3
size cluster i: 87 -> 4
size cluster i: 88 -> 4
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 3
size cluster i: 92 -> 2
size cluster i: 93 -> 3
size cluster i: 94 -> 4
size cluster i: 95 -> 4
size cluster i: 96 -> 4
size cluster i: 97 -> 3
size cluster i: 99 -> 4
size cluster i: 100 -> 5
size cluster i: 101 -> 3
size cluster i: 102 -> 4
size cluster i: 103 -> 2
size cluster i: 104 -> 3
size cluster i: 105 -> 3
size cluster i: 106 -> 2
size cluster i: 107 -> 2
size cluster i: 108 -> 2
size cluster i: 109 -> 2
size cluster i: 110 -> 3
size cluster i: 112 -> 3
size cluster i: 113 -> 4
size cluster i: 114 -> 4
size cluster i: 116 -> 2
size cluster i: 117 -> 2
size cluster i: 119 -> 4
size cluster i: 121 -> 2
size cluster i: 122 -> 3
size cluster i: 123 -> 4
size cluster i: 124 -> 3
size cluster i: 125 -> 3
size cluster i: 126 -> 2
size cluster i: 127 -> 3
size cluster i: 128 -> 2
size cluster i: 129 -> 2
size cluster i: 130 -> 2
size cluster i: 131 -> 3
size cluster i: 132 -> 2
size cluster i: 134 -> 2
size cluster i: 135 -> 2
size cluster i: 136 -> 2
size cluster i: 137 -> 3
size cluster i: 138 -> 5
size cluster i: 140 -> 4
size cluster i: 142 -> 2
size cluster i: 144 -> 3
size cluster i: 145 -> 2
size cluster i: 146 -> 2
size cluster i: 148 -> 2
size cluster i: 149 -> 3
size cluster i: 150 -> 2
size cluster i: 151 -> 6
size cluster i: 152 -> 2
size cluster i: 153 -> 2
size cluster i: 154 -> 2
size cluster i: 155 -> 2
size cluster i: 156 -> 2
size cluster i: 157 -> 2
size cluster i: 158 -> 2
size cluster i: 159 -> 2
size cluster i: 160 -> 2
size cluster i: 161 -> 2
size cluster i: 162 -> 4
size cluster i: 163 -> 4
size cluster i: 164 -> 2
size cluster i: 166 -> 4
size cluster i: 167 -> 2
size cluster i: 168 -> 4
size cluster i: 169 -> 4
size cluster i: 170 -> 2
size cluster i: 173 -> 2
size cluster i: 174 -> 2
size cluster i: 175 -> 2
size cluster i: 176 -> 3
size cluster i: 178 -> 2
size cluster i: 179 -> 2
size cluster i: 180 -> 3
size cluster i: 181 -> 2
size cluster i: 182 -> 2
size cluster i: 183 -> 2
size cluster i: 184 -> 3
size cluster i: 185 -> 2
size cluster i: 188 -> 4
size cluster i: 190 -> 2
size cluster i: 191 -> 2
size cluster i: 193 -> 3
size cluster i: 195 -> 2
size cluster i: 196 -> 3
size cluster i: 197 -> 2
size cluster i: 199 -> 2
size cluster i: 200 -> 4
size cluster i: 201 -> 2
size cluster i: 202 -> 2
size cluster i: 203 -> 3
size cluster i: 205 -> 2
size cluster i: 206 -> 3
size cluster i: 208 -> 2
size cluster i: 210 -> 2
size cluster i: 212 -> 3
size cluster i: 216 -> 2
size cluster i: 217 -> 2
size cluster i: 218 -> 2
size cluster i: 219 -> 2
size cluster i: 220 -> 2
size cluster i: 224 -> 2
size cluster i: 225 -> 2
size cluster i: 227 -> 2
size cluster i: 230 -> 3
size cluster i: 233 -> 2
size cluster i: 235 -> 2
size cluster i: 237 -> 2
size cluster i: 240 -> 2
size cluster i: 243 -> 2
size cluster i: 252 -> 2
size cluster i: 258 -> 2
size cluster i: 265 -> 2
size cluster i: 266 -> 2
datapoints in clusters: 1002049
score: 0


Runtimes: 
--------- 
finished computation at Thu Jan  3 12:59:08 2019
elapsed time: 115.135s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Node 1: Exiting... 
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:234521] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:234521] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with theWarning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Tivalue given in the MPI config!
 and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 282
datapoints_clusters: 1002049
score: 0.0
elapsed_time: 115.135

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 166.666666667 percent_correct: 0.3999519607843137
-> new best_percent_correct:0.3999519607843137new best_threshold:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[48839,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 333.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2305s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.3195s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.03131s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39020
find clusters duration: 0.3401s
detected clusters: 137
size cluster i: 0 -> 100102
size cluster i: 1 -> 10004
size cluster i: 2 -> 20018
size cluster i: 3 -> 30026
size cluster i: 4 -> 10002
size cluster i: 5 -> 10007
size cluster i: 6 -> 30035
size cluster i: 7 -> 10007
size cluster i: 8 -> 10005
size cluster i: 9 -> 10009
size cluster i: 10 -> 10006
size cluster i: 11 -> 10007
size cluster i: 12 -> 30019
size cluster i: 13 -> 10007
size cluster i: 14 -> 10004
size cluster i: 15 -> 10001
size cluster i: 16 -> 10010
size cluster i: 17 -> 60058
size cluster i: 18 -> 10012
size cluster i: 19 -> 10002
size cluster i: 20 -> 10004
size cluster i: 21 -> 20020
size cluster i: 22 -> 10004
size cluster i: 23 -> 10006
size cluster i: 24 -> 10012
size cluster i: 25 -> 10007
size cluster i: 26 -> 10007
size cluster i: 27 -> 10004
size cluster i: 28 -> 20018
size cluster i: 29 -> 10009
size cluster i: 30 -> 20016
size cluster i: 31 -> 10004
size cluster i: 32 -> 30032
size cluster i: 33 -> 20011
size cluster i: 34 -> 10008
size cluster i: 35 -> 20019
size cluster i: 36 -> 10003
size cluster i: 37 -> 10006
size cluster i: 38 -> 10006
size cluster i: 39 -> 10003
size cluster i: 40 -> 10005
size cluster i: 41 -> 20014
size cluster i: 42 -> 10003
size cluster i: 43 -> 10008
size cluster i: 44 -> 10005
size cluster i: 45 -> 10004
size cluster i: 46 -> 10004
size cluster i: 47 -> 10007
size cluster i: 48 -> 10008
size cluster i: 49 -> 10009
size cluster i: 50 -> 10007
size cluster i: 51 -> 10002
size cluster i: 52 -> 10008
size cluster i: 53 -> 10006
size cluster i: 54 -> 10009
size cluster i: 55 -> 10006
size cluster i: 56 -> 10005
size cluster i: 57 -> 10003
size cluster i: 58 -> 10019
size cluster i: 59 -> 10005
size cluster i: 60 -> 10006
size cluster i: 61 -> 10005
size cluster i: 62 -> 10007
size cluster i: 63 -> 10002
size cluster i: 64 -> 10000
size cluster i: 65 -> 10007
size cluster i: 66 -> 10008
size cluster i: 67 -> 10001
size cluster i: 68 -> 10003
size cluster i: 69 -> 10001
size cluster i: 70 -> 10008
size cluster i: 71 -> 2
size cluster i: 72 -> 4
size cluster i: 73 -> 2
size cluster i: 74 -> 3
size cluster i: 75 -> 2
size cluster i: 76 -> 3
size cluster i: 77 -> 2
size cluster i: 78 -> 2
size cluster i: 79 -> 5
size cluster i: 80 -> 2
size cluster i: 81 -> 3
size cluster i: 82 -> 2
size cluster i: 83 -> 2
size cluster i: 84 -> 4
size cluster i: 85 -> 4
size cluster i: 86 -> 3
size cluster i: 87 -> 5
size cluster i: 88 -> 2
size cluster i: 89 -> 2
size cluster i: 90 -> 3
size cluster i: 92 -> 3
size cluster i: 93 -> 2
size cluster i: 95 -> 3
size cluster i: 98 -> 2
size cluster i: 100 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 2
size cluster i: 103 -> 2
size cluster i: 105 -> 2
size cluster i: 107 -> 2
size cluster i: 108 -> 3
size cluster i: 109 -> 3
size cluster i: 110 -> 2
size cluster i: 112 -> 4
size cluster i: 115 -> 2
size cluster i: 116 -> 2
size cluster i: 117 -> 3
size cluster i: 119 -> 2
size cluster i: 120 -> 2
size cluster i: 121 -> 2
size cluster i: 123 -> 2
size cluster i: 124 -> 2
size cluster i: 131 -> 2
size cluster i: 134 -> 2
datapoints in clusters: 1000859
score: 61.8178


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:01:09 2019
elapsed time: 110.265s


Finishing: 
---------- 
Beginning cleanup...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 6: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 5: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:234878] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:234878] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 137
datapoints_clusters: 1000859
score: 61.8178
elapsed_time: 110.265

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 333.333333333 percent_correct: 0.7148441176470588
-> new best_percent_correct:0.7148441176470588new best_threshold:333.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[48474,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1768s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.3818s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.91398s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38868
find clusters duration: 0.291969s
detected clusters: 105
size cluster i: 0 -> 20017
size cluster i: 1 -> 10004
size cluster i: 2 -> 20011
size cluster i: 3 -> 20015
size cluster i: 4 -> 10000
size cluster i: 5 -> 10004
size cluster i: 6 -> 20012
size cluster i: 7 -> 10004
size cluster i: 8 -> 10004
size cluster i: 9 -> 10005
size cluster i: 10 -> 10004
size cluster i: 11 -> 10005
size cluster i: 12 -> 30016
size cluster i: 13 -> 10006
size cluster i: 14 -> 10003
size cluster i: 15 -> 10001
size cluster i: 16 -> 10007
size cluster i: 17 -> 40025
size cluster i: 18 -> 10009
size cluster i: 19 -> 10001
size cluster i: 20 -> 10001
size cluster i: 21 -> 10007
size cluster i: 22 -> 10003
size cluster i: 23 -> 10005
size cluster i: 24 -> 30020
size cluster i: 25 -> 10007
size cluster i: 26 -> 10004
size cluster i: 27 -> 10003
size cluster i: 28 -> 10001
size cluster i: 29 -> 20009
size cluster i: 30 -> 10007
size cluster i: 31 -> 10004
size cluster i: 32 -> 10003
size cluster i: 33 -> 10010
size cluster i: 34 -> 10005
size cluster i: 35 -> 10007
size cluster i: 36 -> 20013
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10003
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10004
size cluster i: 43 -> 20007
size cluster i: 44 -> 10002
size cluster i: 45 -> 10003
size cluster i: 46 -> 10007
size cluster i: 47 -> 10005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10006
size cluster i: 52 -> 10002
size cluster i: 53 -> 10003
size cluster i: 54 -> 20020
size cluster i: 55 -> 10003
size cluster i: 56 -> 10002
size cluster i: 57 -> 10004
size cluster i: 58 -> 10003
size cluster i: 59 -> 10007
size cluster i: 60 -> 10005
size cluster i: 61 -> 20011
size cluster i: 62 -> 10007
size cluster i: 63 -> 10004
size cluster i: 64 -> 10004
size cluster i: 65 -> 10007
size cluster i: 66 -> 10008
size cluster i: 67 -> 10000
size cluster i: 68 -> 10011
size cluster i: 69 -> 10004
size cluster i: 70 -> 10003
size cluster i: 71 -> 10003
size cluster i: 72 -> 10003
size cluster i: 73 -> 10006
size cluster i: 74 -> 10001
size cluster i: 75 -> 9993
size cluster i: 76 -> 10004
size cluster i: 77 -> 10002
size cluster i: 78 -> 10005
size cluster i: 79 -> 10000
size cluster i: 80 -> 10005
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10002
size cluster i: 84 -> 2
size cluster i: 85 -> 2
size cluster i: 86 -> 2
size cluster i: 87 -> 4
size cluster i: 88 -> 3
size cluster i: 89 -> 4
size cluster i: 90 -> 2
size cluster i: 92 -> 3
size cluster i: 93 -> 2
size cluster i: 94 -> 4
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 98 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 2
datapoints in clusters: 1000490
score: 93.1829


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:03:09 2019
elapsed time: 110.142s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node Warning: Node 5 is going to delete COUNT entry for Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!


3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!


[argon-gtx:235235] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:235235] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 105
datapoints_clusters: 1000490
score: 93.1829
elapsed_time: 110.142

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 500.0 percent_correct: 0.8426431372549019
-> new best_percent_correct:0.8426431372549019new best_threshold:500.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 666.6666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[45962,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 666.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2209s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4018s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.98502s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38774
find clusters duration: 0.286703s
detected clusters: 99
size cluster i: 0 -> 10005
size cluster i: 1 -> 10002
size cluster i: 2 -> 20009
size cluster i: 3 -> 20009
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10001
size cluster i: 7 -> 10002
size cluster i: 8 -> 10003
size cluster i: 9 -> 10004
size cluster i: 10 -> 10002
size cluster i: 11 -> 10003
size cluster i: 12 -> 20010
size cluster i: 13 -> 10002
size cluster i: 14 -> 10002
size cluster i: 15 -> 10001
size cluster i: 16 -> 10006
size cluster i: 17 -> 30006
size cluster i: 18 -> 10007
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10004
size cluster i: 22 -> 10001
size cluster i: 23 -> 10004
size cluster i: 24 -> 30015
size cluster i: 25 -> 10004
size cluster i: 26 -> 10003
size cluster i: 27 -> 10003
size cluster i: 28 -> 9995
size cluster i: 29 -> 20008
size cluster i: 30 -> 10006
size cluster i: 31 -> 10001
size cluster i: 32 -> 10002
size cluster i: 33 -> 10005
size cluster i: 34 -> 10007
size cluster i: 35 -> 10003
size cluster i: 36 -> 10004
size cluster i: 37 -> 10003
size cluster i: 38 -> 9999
size cluster i: 39 -> 10003
size cluster i: 40 -> 10003
size cluster i: 41 -> 10000
size cluster i: 42 -> 10001
size cluster i: 43 -> 10003
size cluster i: 44 -> 20007
size cluster i: 45 -> 10001
size cluster i: 46 -> 10003
size cluster i: 47 -> 10002
size cluster i: 48 -> 10002
size cluster i: 49 -> 10001
size cluster i: 50 -> 10002
size cluster i: 51 -> 10002
size cluster i: 52 -> 10005
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 20013
size cluster i: 56 -> 10002
size cluster i: 57 -> 10001
size cluster i: 58 -> 10006
size cluster i: 59 -> 10004
size cluster i: 60 -> 10003
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10006
size cluster i: 64 -> 20009
size cluster i: 65 -> 10003
size cluster i: 66 -> 10004
size cluster i: 67 -> 10004
size cluster i: 68 -> 10006
size cluster i: 69 -> 10005
size cluster i: 70 -> 10000
size cluster i: 71 -> 10007
size cluster i: 72 -> 10004
size cluster i: 73 -> 10003
size cluster i: 74 -> 10003
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10003
size cluster i: 78 -> 10001
size cluster i: 79 -> 10005
size cluster i: 80 -> 9737
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 10003
size cluster i: 84 -> 10000
size cluster i: 85 -> 10003
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 4
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 1000052
score: 97.0639


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:05:10 2019
elapsed time: 110.237s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 1: Exiting... 
Node 8: Exiting... 
Node 3: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node Warning: Node Warning: Node 52 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!


Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:235571] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:235571] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 1000052
score: 97.0639
elapsed_time: 110.237

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 666.666666667 percent_correct: 0.8915745098039216
-> new best_percent_correct:0.8915745098039216new best_threshold:666.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 833.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[45620,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 833.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1953s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.3395s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.06693s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38677
find clusters duration: 0.276657s
detected clusters: 97
size cluster i: 0 -> 10004
size cluster i: 1 -> 9999
size cluster i: 2 -> 20005
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10001
size cluster i: 8 -> 10002
size cluster i: 9 -> 10004
size cluster i: 10 -> 10001
size cluster i: 11 -> 10002
size cluster i: 12 -> 20008
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10001
size cluster i: 16 -> 10003
size cluster i: 17 -> 20002
size cluster i: 18 -> 10003
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10002
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20008
size cluster i: 25 -> 10005
size cluster i: 26 -> 10003
size cluster i: 27 -> 10002
size cluster i: 28 -> 10002
size cluster i: 29 -> 10001
size cluster i: 30 -> 9897
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10002
size cluster i: 36 -> 10005
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10004
size cluster i: 40 -> 10003
size cluster i: 41 -> 9931
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9938
size cluster i: 46 -> 10002
size cluster i: 47 -> 20006
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 10004
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 20009
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 20009
size cluster i: 68 -> 10003
size cluster i: 69 -> 10003
size cluster i: 70 -> 10001
size cluster i: 71 -> 10006
size cluster i: 72 -> 10004
size cluster i: 73 -> 10000
size cluster i: 74 -> 10006
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10002
size cluster i: 78 -> 10000
size cluster i: 79 -> 10003
size cluster i: 80 -> 10002
size cluster i: 81 -> 10001
size cluster i: 82 -> 10004
size cluster i: 83 -> 8575
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10002
size cluster i: 87 -> 10000
size cluster i: 88 -> 10002
size cluster i: 89 -> 10000
size cluster i: 90 -> 9983
size cluster i: 91 -> 10001
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 2
datapoints in clusters: 998549
score: 94.9601


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:07:10 2019
elapsed time: 110.231s


Finishing: 
---------- 
Beginning cleanup...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:235917] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:235917] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 998549
score: 94.9601
elapsed_time: 110.231

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 833.333333333 percent_correct: 0.9196990196078432
-> new best_percent_correct:0.9196990196078432new best_threshold:833.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1000.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[45414,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1000


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.3159s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4345s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.99671s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38607
find clusters duration: 0.273319s
detected clusters: 96
size cluster i: 0 -> 10004
size cluster i: 1 -> 9988
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20005
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9556
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9674
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9529
size cluster i: 46 -> 10001
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10004
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 20008
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10005
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10002
size cluster i: 73 -> 10000
size cluster i: 74 -> 10004
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10001
size cluster i: 82 -> 10001
size cluster i: 83 -> 10004
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 6580
size cluster i: 90 -> 10002
size cluster i: 91 -> 9997
size cluster i: 92 -> 9852
size cluster i: 93 -> 10000
size cluster i: 94 -> 2
size cluster i: 95 -> 2
datapoints in clusters: 995326
score: 93.6777


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:09:10 2019
elapsed time: 110.35s


Finishing: 
---------- 
Beginning cleanup...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:236255] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:236255] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 995326
score: 93.6777
elapsed_time: 110.35

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 1000.0 percent_correct: 0.9362980392156863
-> new best_percent_correct:0.9362980392156863new best_threshold:1000.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1166.6666666666665 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[46988,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1166.67


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1059s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.5315s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.97215s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38440
find clusters duration: 0.270608s
detected clusters: 96
size cluster i: 0 -> 10004
size cluster i: 1 -> 9915
size cluster i: 2 -> 20002
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9994
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10004
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20003
size cluster i: 25 -> 10003
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 8863
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9036
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 8481
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10002
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 10002
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 4278
size cluster i: 91 -> 10002
size cluster i: 92 -> 9950
size cluster i: 93 -> 9451
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
datapoints in clusters: 990086
score: 93.1846


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:11:10 2019
elapsed time: 110.223s


Finishing: 
---------- 
Beginning cleanup...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Node 5: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:236597] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:236597] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 990086
score: 93.1846
elapsed_time: 110.223

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 1166.66666667 percent_correct: 0.9410294117647059
-> new best_percent_correct:0.9410294117647059new best_threshold:1166.66666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[46790,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1333.33


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1635s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.463s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.07655s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38225
find clusters duration: 0.276618s
detected clusters: 98
size cluster i: 0 -> 10003
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9956
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10004
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10001
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 20003
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10002
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 6835
size cluster i: 43 -> 10001
size cluster i: 44 -> 20003
size cluster i: 45 -> 10000
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10002
size cluster i: 59 -> 10002
size cluster i: 60 -> 10000
size cluster i: 61 -> 10002
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10000
size cluster i: 71 -> 10002
size cluster i: 72 -> 7856
size cluster i: 73 -> 10001
size cluster i: 74 -> 10003
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 9661
size cluster i: 79 -> 10002
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 8048
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 9795
size cluster i: 92 -> 8726
size cluster i: 93 -> 10000
size cluster i: 94 -> 2401
size cluster i: 95 -> 4
size cluster i: 97 -> 2
datapoints in clusters: 983377
score: 94.4813


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:13:10 2019
elapsed time: 110.296s


Finishing: 
---------- 
Node 5 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:236927] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:236927] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 983377
score: 94.4813
elapsed_time: 110.296

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 1333.33333333 percent_correct: 0.9344921568627451

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[46454,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.23s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.5079s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.03531s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37972
find clusters duration: 0.263204s
detected clusters: 102
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9822
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9997
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 5033
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9979
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10002
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 6696
size cluster i: 76 -> 10001
size cluster i: 77 -> 10002
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 9128
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 6745
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 10000
size cluster i: 92 -> 10000
size cluster i: 93 -> 10001
size cluster i: 94 -> 9406
size cluster i: 95 -> 7697
size cluster i: 96 -> 10000
size cluster i: 97 -> 1099
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 2
datapoints in clusters: 975686
score: 93.7424


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:15:10 2019
elapsed time: 110.396s


Finishing: 
---------- 
Beginning cleanup...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:237263] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:237263] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 102
datapoints_clusters: 975686
score: 93.7424
elapsed_time: 110.396

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 1500.0 percent_correct: 0.9563970588235294
-> new best_percent_correct:0.9563970588235294new best_threshold:1500.0
thresholds:[ 1370.37037037  1407.40740741  1444.44444444  1481.48148148  1518.51851852
  1555.55555556  1592.59259259  1629.62962963], threshold_step:37.037037037

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1370.3703703703702 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[35750,1],1]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1370.37


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1111s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4325s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.86637s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38176
find clusters duration: 0.278208s
detected clusters: 99
size cluster i: 0 -> 10003
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9932
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10004
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 10000
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10002
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 6439
size cluster i: 43 -> 10001
size cluster i: 44 -> 20003
size cluster i: 45 -> 10000
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10002
size cluster i: 59 -> 10002
size cluster i: 60 -> 10000
size cluster i: 61 -> 10002
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10002
size cluster i: 73 -> 7611
size cluster i: 74 -> 10001
size cluster i: 75 -> 10003
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 9575
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 7794
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 10000
size cluster i: 91 -> 10001
size cluster i: 92 -> 9736
size cluster i: 93 -> 8525
size cluster i: 94 -> 10000
size cluster i: 95 -> 2067
size cluster i: 96 -> 3
size cluster i: 97 -> 2
datapoints in clusters: 981775
score: 95.2899


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:17:10 2019
elapsed time: 110.02s


Finishing: 
---------- 
Beginning cleanup...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:237599] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:237599] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 981775
score: 95.2899
elapsed_time: 110.02

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 1370.37037037 percent_correct: 0.9427303921568627

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1407.4074074074074 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[35525,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1407.41


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0737s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4622s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.9978s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38135
find clusters duration: 0.266336s
detected clusters: 100
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9911
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10004
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 10000
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10001
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 6035
size cluster i: 43 -> 10001
size cluster i: 44 -> 20003
size cluster i: 45 -> 9997
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10002
size cluster i: 59 -> 10002
size cluster i: 60 -> 10000
size cluster i: 61 -> 10002
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10002
size cluster i: 73 -> 7362
size cluster i: 74 -> 10001
size cluster i: 75 -> 10002
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 9459
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 7500
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 10000
size cluster i: 91 -> 10001
size cluster i: 92 -> 9661
size cluster i: 93 -> 8323
size cluster i: 94 -> 10000
size cluster i: 95 -> 1752
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 2
datapoints in clusters: 980091
score: 96.0874


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:19:10 2019
elapsed time: 110.125s


Finishing: 
---------- 
Beginning cleanup...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:237948] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:237948] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 100
datapoints_clusters: 980091
score: 96.0874
elapsed_time: 110.125

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 1407.40740741 percent_correct: 0.941092156862745

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1444.4444444444443 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[35189,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1444.44


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0638s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4727s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.88244s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38073
find clusters duration: 0.277217s
detected clusters: 100
size cluster i: 0 -> 10001
size cluster i: 1 -> 10001
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9881
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 20005
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 5633
size cluster i: 44 -> 10001
size cluster i: 45 -> 20003
size cluster i: 46 -> 9991
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10001
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10001
size cluster i: 72 -> 10000
size cluster i: 73 -> 10002
size cluster i: 74 -> 7087
size cluster i: 75 -> 10001
size cluster i: 76 -> 10002
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 9328
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 7210
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 9560
size cluster i: 94 -> 8075
size cluster i: 95 -> 10000
size cluster i: 96 -> 1455
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 99 -> 2
datapoints in clusters: 978308
score: 95.9125


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:21:10 2019
elapsed time: 110.051s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Node 3: Exiting... 
Node 2: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:238284] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:238284] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 100
datapoints_clusters: 978308
score: 95.9125
elapsed_time: 110.051

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 1444.44444444 percent_correct: 0.9491529411764706

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1481.4814814814815 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[36780,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1481.48


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1754s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4719s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.97656s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38003
find clusters duration: 0.266856s
detected clusters: 101
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9839
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9997
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 20004
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 5217
size cluster i: 44 -> 10001
size cluster i: 45 -> 20003
size cluster i: 46 -> 9982
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10004
size cluster i: 66 -> 10001
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10001
size cluster i: 72 -> 10000
size cluster i: 73 -> 10001
size cluster i: 74 -> 6824
size cluster i: 75 -> 10001
size cluster i: 76 -> 10002
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 9191
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 6905
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 9471
size cluster i: 94 -> 7823
size cluster i: 95 -> 10000
size cluster i: 96 -> 1219
size cluster i: 98 -> 4
size cluster i: 99 -> 2
datapoints in clusters: 976553
score: 94.7831


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:23:10 2019
elapsed time: 110.209s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Beginning cleanup...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:238613] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:238613] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 101
datapoints_clusters: 976553
score: 94.7831
elapsed_time: 110.209

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 1481.48148148 percent_correct: 0.9474401960784313

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1518.5185185185185 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[36568,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1518.52


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1709s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4558s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.20016s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37958
find clusters duration: 0.261124s
detected clusters: 103
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9803
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9996
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10002
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 4827
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9978
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10002
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 6541
size cluster i: 76 -> 10001
size cluster i: 77 -> 10002
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 9055
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 6589
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 10000
size cluster i: 92 -> 10000
size cluster i: 93 -> 10001
size cluster i: 94 -> 9356
size cluster i: 95 -> 7576
size cluster i: 96 -> 10000
size cluster i: 97 -> 984
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 100 -> 4
size cluster i: 101 -> 2
datapoints in clusters: 974790
score: 92.7006


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:25:10 2019
elapsed time: 110.461s


Finishing: 
---------- 
Beginning cleanup...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 5: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:238945] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:238945] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 103
datapoints_clusters: 974790
score: 92.7006
elapsed_time: 110.461

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 1518.51851852 percent_correct: 0.9555186274509804

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1555.5555555555557 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[36099,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1555.56


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1992s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4501s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.99779s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37908
find clusters duration: 0.264489s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9737
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9993
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 4410
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9970
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10002
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 6292
size cluster i: 76 -> 10001
size cluster i: 77 -> 10001
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 8912
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 6268
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 10000
size cluster i: 92 -> 10000
size cluster i: 93 -> 10001
size cluster i: 94 -> 9247
size cluster i: 95 -> 7296
size cluster i: 96 -> 10000
size cluster i: 97 -> 811
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 2
datapoints in clusters: 973018
score: 91.5782


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:27:10 2019
elapsed time: 110.247s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:239290] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:239290] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 973018
score: 91.5782
elapsed_time: 110.247

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 1555.55555556 percent_correct: 0.9537892156862745

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1592.5925925925926 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[33714,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1592.59


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1757s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4234s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.01886s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37845
find clusters duration: 0.267161s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9669
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10003
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9992
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 3947
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9954
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10000
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 5996
size cluster i: 76 -> 10001
size cluster i: 77 -> 10001
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 8718
size cluster i: 82 -> 10001
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 9109
size cluster i: 94 -> 5930
size cluster i: 95 -> 7036
size cluster i: 96 -> 10000
size cluster i: 97 -> 637
size cluster i: 98 -> 3
size cluster i: 99 -> 2
datapoints in clusters: 971060
score: 91.3939


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:29:11 2019
elapsed time: 110.214s


Finishing: 
---------- 
Beginning cleanup...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 8: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Node 3: Exiting... 
Node 4: Exiting... 
Cleanup done!
Node 2: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:239627] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:239627] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 971060
score: 91.3939
elapsed_time: 110.214

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 1592.59259259 percent_correct: 0.9518872549019608

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-07 --k 6 --epsilon 0.001 --threshold 1629.6296296296296 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[33508,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1629.63


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1274s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4322s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.03308s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37834
find clusters duration: 0.260015s
detected clusters: 105
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9602
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10003
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9991
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10002
size cluster i: 44 -> 3582
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9929
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 8524
size cluster i: 81 -> 10001
size cluster i: 82 -> 5695
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 8947
size cluster i: 94 -> 5646
size cluster i: 95 -> 6759
size cluster i: 96 -> 10000
size cluster i: 97 -> 500
size cluster i: 99 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 2
size cluster i: 102 -> 3
datapoints in clusters: 969248
score: 90.2731


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:31:11 2019
elapsed time: 110.203s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7: Exiting... 
Node 4: Exiting... 
Node 3: Exiting... 
Node 2: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 5: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:239965] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:239965] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 105
datapoints_clusters: 969248
score: 90.2731
elapsed_time: 110.203

--------------- END RUN -----------------
attempt lambda: 1e-07 threshold: 1629.62962963 percent_correct: 0.9501127450980392
-> new overall_best_percent_correct:0.9563970588235294, overall_best_lambda_value: 1e-07, new best_threshold:1500.0
thresholds:[0.0, 166.66666666666666, 333.33333333333331, 500.0, 666.66666666666663, 833.33333333333326, 1000.0, 1166.6666666666665, 1333.3333333333333, 1500.0], threshold_step:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 0.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[33036,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 0


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.204s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.5881s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.89634s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37747
find clusters duration: 66.0622s
detected clusters: 1
size cluster i: 0 -> 1020000
datapoints in clusters: 1020000
score: 1


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:33:22 2019
elapsed time: 121.085s


Finishing: 
---------- 
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 4: Exiting... 
Node 7: Exiting... 
Node 6: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 1: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:240309] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:240309] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 1
datapoints_clusters: 1020000
score: 1.0
elapsed_time: 121.085

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 0.0 percent_correct: 0.00980392156862745
-> new best_percent_correct:0.00980392156862745new best_threshold:0.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 166.66666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[34729,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 166.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1405s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.3092s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.90559s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39164
find clusters duration: 2.95557s
detected clusters: 92
size cluster i: 0 -> 490592
size cluster i: 1 -> 10008
size cluster i: 2 -> 20020
size cluster i: 3 -> 10009
size cluster i: 4 -> 10007
size cluster i: 5 -> 10005
size cluster i: 6 -> 10012
size cluster i: 7 -> 10012
size cluster i: 8 -> 10012
size cluster i: 9 -> 80075
size cluster i: 10 -> 10005
size cluster i: 11 -> 10002
size cluster i: 12 -> 20030
size cluster i: 13 -> 10004
size cluster i: 14 -> 10006
size cluster i: 15 -> 10016
size cluster i: 16 -> 10008
size cluster i: 17 -> 10002
size cluster i: 18 -> 10011
size cluster i: 19 -> 10004
size cluster i: 20 -> 10001
size cluster i: 21 -> 10011
size cluster i: 22 -> 10004
size cluster i: 23 -> 10006
size cluster i: 24 -> 20020
size cluster i: 25 -> 10004
size cluster i: 26 -> 10010
size cluster i: 27 -> 10007
size cluster i: 28 -> 10012
size cluster i: 29 -> 10004
size cluster i: 30 -> 10014
size cluster i: 31 -> 10010
size cluster i: 32 -> 10006
size cluster i: 33 -> 10007
size cluster i: 34 -> 10003
size cluster i: 35 -> 10000
size cluster i: 36 -> 10010
size cluster i: 37 -> 10012
size cluster i: 38 -> 10002
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10009
size cluster i: 42 -> 3
size cluster i: 43 -> 2
size cluster i: 44 -> 4
size cluster i: 45 -> 4
size cluster i: 46 -> 2
size cluster i: 47 -> 2
size cluster i: 48 -> 3
size cluster i: 49 -> 2
size cluster i: 50 -> 3
size cluster i: 51 -> 3
size cluster i: 52 -> 2
size cluster i: 53 -> 4
size cluster i: 54 -> 3
size cluster i: 55 -> 3
size cluster i: 56 -> 2
size cluster i: 57 -> 5
size cluster i: 59 -> 2
size cluster i: 60 -> 3
size cluster i: 61 -> 2
size cluster i: 62 -> 2
size cluster i: 63 -> 4
size cluster i: 64 -> 2
size cluster i: 65 -> 3
size cluster i: 68 -> 2
size cluster i: 69 -> 3
size cluster i: 70 -> 2
size cluster i: 71 -> 2
size cluster i: 72 -> 4
size cluster i: 73 -> 2
size cluster i: 75 -> 2
size cluster i: 76 -> 2
size cluster i: 77 -> 3
size cluster i: 78 -> 2
size cluster i: 79 -> 2
size cluster i: 80 -> 2
size cluster i: 81 -> 2
size cluster i: 82 -> 2
size cluster i: 83 -> 2
size cluster i: 88 -> 2
size cluster i: 89 -> 2
datapoints in clusters: 1001108
score: 90.296


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:34:30 2019
elapsed time: 57.6521s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node Warning: Node Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:240656] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:240656] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 92
datapoints_clusters: 1001108
score: 90.296
elapsed_time: 57.6521

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 166.666666667 percent_correct: 0.43028627450980395
-> new best_percent_correct:0.43028627450980395new best_threshold:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[34515,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 333.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0494s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4454s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.83925s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38748
find clusters duration: 0.327063s
detected clusters: 84
size cluster i: 0 -> 60039
size cluster i: 1 -> 9993
size cluster i: 2 -> 20008
size cluster i: 3 -> 30016
size cluster i: 4 -> 10000
size cluster i: 5 -> 10002
size cluster i: 6 -> 50028
size cluster i: 7 -> 10003
size cluster i: 8 -> 10002
size cluster i: 9 -> 10004
size cluster i: 10 -> 10001
size cluster i: 11 -> 10003
size cluster i: 12 -> 20011
size cluster i: 13 -> 50029
size cluster i: 14 -> 10002
size cluster i: 15 -> 10001
size cluster i: 16 -> 10006
size cluster i: 17 -> 10009
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10006
size cluster i: 21 -> 10002
size cluster i: 22 -> 10005
size cluster i: 23 -> 30021
size cluster i: 24 -> 10006
size cluster i: 25 -> 10004
size cluster i: 26 -> 10002
size cluster i: 27 -> 9620
size cluster i: 28 -> 20009
size cluster i: 29 -> 10005
size cluster i: 30 -> 10000
size cluster i: 31 -> 10003
size cluster i: 32 -> 30025
size cluster i: 33 -> 10003
size cluster i: 34 -> 10005
size cluster i: 35 -> 20011
size cluster i: 36 -> 9725
size cluster i: 37 -> 10003
size cluster i: 38 -> 10003
size cluster i: 39 -> 9833
size cluster i: 40 -> 10002
size cluster i: 41 -> 20011
size cluster i: 42 -> 10002
size cluster i: 43 -> 10003
size cluster i: 44 -> 10003
size cluster i: 45 -> 10004
size cluster i: 46 -> 10001
size cluster i: 47 -> 10002
size cluster i: 48 -> 10005
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10005
size cluster i: 54 -> 10002
size cluster i: 55 -> 10005
size cluster i: 56 -> 10004
size cluster i: 57 -> 20010
size cluster i: 58 -> 10004
size cluster i: 59 -> 10005
size cluster i: 60 -> 10000
size cluster i: 61 -> 10005
size cluster i: 62 -> 10003
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10003
size cluster i: 66 -> 10005
size cluster i: 67 -> 10001
size cluster i: 68 -> 8233
size cluster i: 69 -> 10000
size cluster i: 70 -> 10002
size cluster i: 71 -> 10000
size cluster i: 72 -> 9998
size cluster i: 73 -> 9950
size cluster i: 74 -> 10001
size cluster i: 75 -> 3
size cluster i: 76 -> 2
size cluster i: 77 -> 2
size cluster i: 78 -> 2
size cluster i: 79 -> 3
size cluster i: 81 -> 2
datapoints in clusters: 997743
score: 82.1671


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:35:35 2019
elapsed time: 55.0341s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 5: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:241002] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:241002] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 84
datapoints_clusters: 997743
score: 82.1671
elapsed_time: 55.0341

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 333.333333333 percent_correct: 0.7519186274509804
-> new best_percent_correct:0.7519186274509804new best_threshold:333.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[34052,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2806s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4808s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.97039s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38221
find clusters duration: 0.279263s
detected clusters: 91
size cluster i: 0 -> 10004
size cluster i: 1 -> 20002
size cluster i: 2 -> 20008
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 20005
size cluster i: 6 -> 10000
size cluster i: 7 -> 9939
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10001
size cluster i: 11 -> 10007
size cluster i: 12 -> 10003
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10002
size cluster i: 16 -> 30006
size cluster i: 17 -> 10006
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20007
size cluster i: 24 -> 10003
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 20005
size cluster i: 29 -> 10005
size cluster i: 30 -> 20004
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10002
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10006
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10002
size cluster i: 40 -> 10002
size cluster i: 41 -> 6401
size cluster i: 42 -> 10001
size cluster i: 43 -> 20005
size cluster i: 44 -> 10000
size cluster i: 45 -> 10001
size cluster i: 46 -> 10002
size cluster i: 47 -> 10001
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10005
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 20010
size cluster i: 55 -> 10000
size cluster i: 56 -> 10001
size cluster i: 57 -> 10004
size cluster i: 58 -> 10003
size cluster i: 59 -> 10000
size cluster i: 60 -> 10003
size cluster i: 61 -> 10001
size cluster i: 62 -> 20009
size cluster i: 63 -> 10003
size cluster i: 64 -> 10003
size cluster i: 65 -> 10003
size cluster i: 66 -> 10003
size cluster i: 67 -> 10004
size cluster i: 68 -> 10000
size cluster i: 69 -> 6572
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 9194
size cluster i: 75 -> 10002
size cluster i: 76 -> 10001
size cluster i: 77 -> 10000
size cluster i: 78 -> 6731
size cluster i: 79 -> 10004
size cluster i: 80 -> 10000
size cluster i: 81 -> 10001
size cluster i: 82 -> 10001
size cluster i: 83 -> 10000
size cluster i: 84 -> 10002
size cluster i: 85 -> 9555
size cluster i: 86 -> 8379
size cluster i: 87 -> 10000
size cluster i: 88 -> 2405
size cluster i: 89 -> 2
size cluster i: 90 -> 2
datapoints in clusters: 979363
score: 87.3745


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:36:40 2019
elapsed time: 55.3538s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:241341] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:241341] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 91
datapoints_clusters: 979363
score: 87.3745
elapsed_time: 55.3538

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 500.0 percent_correct: 0.8715578431372549
-> new best_percent_correct:0.8715578431372549new best_threshold:500.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 666.6666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[39850,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 666.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2305s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4517s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.99646s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37343
find clusters duration: 0.263677s
detected clusters: 103
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 8762
size cluster i: 8 -> 10001
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9938
size cluster i: 15 -> 10000
size cluster i: 16 -> 20003
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10002
size cluster i: 22 -> 10002
size cluster i: 23 -> 20005
size cluster i: 24 -> 10003
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10003
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 1743
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9902
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20003
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 9999
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 6077
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 9996
size cluster i: 88 -> 10001
size cluster i: 89 -> 6993
size cluster i: 90 -> 4713
size cluster i: 91 -> 10000
size cluster i: 92 -> 2574
size cluster i: 93 -> 2549
size cluster i: 94 -> 155
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 6
size cluster i: 98 -> 2
size cluster i: 99 -> 3
size cluster i: 101 -> 2
size cluster i: 102 -> 3
datapoints in clusters: 953514
score: 90.6773


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:37:45 2019
elapsed time: 55.2977s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 4: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 3: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 TiWarning: Node  and will insert its own SELECT entry with thevalue given in the MPI config!
8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:241683] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:241683] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 103
datapoints_clusters: 953514
score: 90.6773
elapsed_time: 55.2977

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 666.666666667 percent_correct: 0.9052049019607843
-> new best_percent_correct:0.9052049019607843new best_threshold:666.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 833.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[39653,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 833.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0706s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.5169s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.99957s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 36536
find clusters duration: 0.255728s
detected clusters: 124
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 9967
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9256
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 9999
size cluster i: 20 -> 10000
size cluster i: 21 -> 10000
size cluster i: 22 -> 10002
size cluster i: 23 -> 20003
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10000
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 9978
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10003
size cluster i: 38 -> 10001
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10001
size cluster i: 43 -> 10000
size cluster i: 44 -> 20002
size cluster i: 45 -> 8881
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 20001
size cluster i: 56 -> 10000
size cluster i: 57 -> 9998
size cluster i: 58 -> 10001
size cluster i: 59 -> 10001
size cluster i: 60 -> 10000
size cluster i: 61 -> 10000
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10004
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10000
size cluster i: 68 -> 10000
size cluster i: 69 -> 10001
size cluster i: 70 -> 9969
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 2433
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 9741
size cluster i: 87 -> 10000
size cluster i: 88 -> 5919
size cluster i: 89 -> 1598
size cluster i: 90 -> 10000
size cluster i: 91 -> 458
size cluster i: 92 -> 3447
size cluster i: 93 -> 537
size cluster i: 94 -> 110
size cluster i: 95 -> 2
size cluster i: 96 -> 3
size cluster i: 97 -> 2
size cluster i: 98 -> 4
size cluster i: 99 -> 3
size cluster i: 101 -> 2
size cluster i: 102 -> 3
size cluster i: 103 -> 3
size cluster i: 105 -> 2
size cluster i: 106 -> 3
size cluster i: 108 -> 3
size cluster i: 109 -> 2
size cluster i: 112 -> 2
size cluster i: 115 -> 2
size cluster i: 116 -> 3
datapoints in clusters: 932393
score: 69.4724


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:38:50 2019
elapsed time: 55.1923s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 4: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:242012] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:242012] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 124
datapoints_clusters: 932393
score: 69.4724
elapsed_time: 55.1923

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 833.333333333 percent_correct: 0.8943529411764706

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 1000.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[39202,1],5]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 1000


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1224s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4544s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.78535s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 35792
find clusters duration: 0.249987s
detected clusters: 124
size cluster i: 0 -> 10001
size cluster i: 1 -> 9996
size cluster i: 2 -> 10000
size cluster i: 3 -> 9606
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10000
size cluster i: 10 -> 9999
size cluster i: 11 -> 10001
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 10000
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10002
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 10000
size cluster i: 25 -> 10000
size cluster i: 26 -> 9991
size cluster i: 27 -> 10000
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 9994
size cluster i: 31 -> 9608
size cluster i: 32 -> 10000
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10002
size cluster i: 36 -> 10000
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10000
size cluster i: 40 -> 10001
size cluster i: 41 -> 9992
size cluster i: 42 -> 20002
size cluster i: 43 -> 10000
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 10000
size cluster i: 47 -> 10000
size cluster i: 48 -> 10000
size cluster i: 49 -> 9992
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 9999
size cluster i: 53 -> 20001
size cluster i: 54 -> 10000
size cluster i: 55 -> 9915
size cluster i: 56 -> 7509
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10000
size cluster i: 61 -> 10000
size cluster i: 62 -> 10001
size cluster i: 63 -> 10004
size cluster i: 64 -> 10001
size cluster i: 65 -> 6477
size cluster i: 66 -> 10001
size cluster i: 67 -> 10000
size cluster i: 68 -> 10001
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 9583
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 9983
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 2603
size cluster i: 89 -> 8593
size cluster i: 90 -> 342
size cluster i: 91 -> 9981
size cluster i: 92 -> 1027
size cluster i: 93 -> 562
size cluster i: 94 -> 10
size cluster i: 95 -> 2
size cluster i: 96 -> 26
size cluster i: 97 -> 18
size cluster i: 98 -> 2
size cluster i: 100 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 2
size cluster i: 103 -> 2
size cluster i: 105 -> 2
size cluster i: 106 -> 2
size cluster i: 107 -> 10
size cluster i: 110 -> 3
size cluster i: 112 -> 2
size cluster i: 113 -> 2
size cluster i: 114 -> 2
size cluster i: 115 -> 2
size cluster i: 116 -> 2
size cluster i: 117 -> 2
datapoints in clusters: 915888
score: 68.2426


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:39:55 2019
elapsed time: 54.9516s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 5: Exiting... 
Node 2: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:242331] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:242331] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 124
datapoints_clusters: 915888
score: 68.2426
elapsed_time: 54.9516

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 1000.0 percent_correct: 0.8978078431372549

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 1166.6666666666665 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[39012,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 1166.67


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1542s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4567s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.92358s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 35144
find clusters duration: 0.244287s
detected clusters: 115
size cluster i: 0 -> 10000
size cluster i: 1 -> 9928
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9992
size cluster i: 8 -> 9989
size cluster i: 9 -> 9980
size cluster i: 10 -> 10001
size cluster i: 11 -> 10000
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10001
size cluster i: 16 -> 10000
size cluster i: 17 -> 10000
size cluster i: 18 -> 10000
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10000
size cluster i: 24 -> 10000
size cluster i: 25 -> 8557
size cluster i: 26 -> 9888
size cluster i: 27 -> 10000
size cluster i: 28 -> 10000
size cluster i: 29 -> 10000
size cluster i: 30 -> 9919
size cluster i: 31 -> 8420
size cluster i: 32 -> 10000
size cluster i: 33 -> 10001
size cluster i: 34 -> 9975
size cluster i: 35 -> 10001
size cluster i: 36 -> 9996
size cluster i: 37 -> 10000
size cluster i: 38 -> 10000
size cluster i: 39 -> 10000
size cluster i: 40 -> 10001
size cluster i: 41 -> 9914
size cluster i: 42 -> 20001
size cluster i: 43 -> 9992
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 10000
size cluster i: 47 -> 10000
size cluster i: 48 -> 10000
size cluster i: 49 -> 9865
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 9978
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 9519
size cluster i: 56 -> 5004
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10000
size cluster i: 61 -> 10000
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10000
size cluster i: 67 -> 10001
size cluster i: 68 -> 10000
size cluster i: 69 -> 10000
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 9999
size cluster i: 73 -> 10000
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 8507
size cluster i: 78 -> 3629
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 9986
size cluster i: 84 -> 10000
size cluster i: 85 -> 9859
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 694
size cluster i: 90 -> 9703
size cluster i: 91 -> 6504
size cluster i: 92 -> 146
size cluster i: 93 -> 42
size cluster i: 94 -> 27
size cluster i: 95 -> 2
size cluster i: 96 -> 3
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 99 -> 6
size cluster i: 100 -> 4
size cluster i: 101 -> 4
size cluster i: 102 -> 4
size cluster i: 103 -> 2
size cluster i: 104 -> 2
size cluster i: 105 -> 2
size cluster i: 106 -> 2
size cluster i: 107 -> 2
size cluster i: 110 -> 2
size cluster i: 111 -> 2
datapoints in clusters: 900074
score: 75.0062


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:41:00 2019
elapsed time: 55.1132s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 2: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:242653] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:242653] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with theWarning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
value given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 115
datapoints_clusters: 900074
score: 75.0062
elapsed_time: 55.1132

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 1166.66666667 percent_correct: 0.8921519607843137

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 1333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[40604,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 1333.33


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1942s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4735s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.94219s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 34643
find clusters duration: 0.236121s
detected clusters: 109
size cluster i: 0 -> 10000
size cluster i: 1 -> 9596
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9893
size cluster i: 8 -> 9886
size cluster i: 9 -> 9809
size cluster i: 10 -> 10001
size cluster i: 11 -> 10000
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10001
size cluster i: 16 -> 10000
size cluster i: 17 -> 10000
size cluster i: 18 -> 10000
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10000
size cluster i: 24 -> 10000
size cluster i: 25 -> 6705
size cluster i: 26 -> 10000
size cluster i: 27 -> 9996
size cluster i: 28 -> 10000
size cluster i: 29 -> 9630
size cluster i: 30 -> 6357
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 9831
size cluster i: 34 -> 10000
size cluster i: 35 -> 9943
size cluster i: 36 -> 10000
size cluster i: 37 -> 10000
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9509
size cluster i: 41 -> 10001
size cluster i: 42 -> 9924
size cluster i: 43 -> 10000
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 10000
size cluster i: 47 -> 10000
size cluster i: 48 -> 9411
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 9800
size cluster i: 52 -> 10000
size cluster i: 53 -> 9997
size cluster i: 54 -> 8473
size cluster i: 55 -> 2604
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10000
size cluster i: 61 -> 10000
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 9440
size cluster i: 65 -> 10001
size cluster i: 66 -> 10000
size cluster i: 67 -> 10000
size cluster i: 68 -> 10000
size cluster i: 69 -> 10000
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 9991
size cluster i: 73 -> 10000
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 6766
size cluster i: 78 -> 10000
size cluster i: 79 -> 9987
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 9884
size cluster i: 84 -> 10000
size cluster i: 85 -> 9480
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 8826
size cluster i: 90 -> 4067
size cluster i: 91 -> 1535
size cluster i: 92 -> 67
size cluster i: 94 -> 3
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 4
size cluster i: 98 -> 2
size cluster i: 100 -> 3
size cluster i: 102 -> 3
size cluster i: 103 -> 2
size cluster i: 107 -> 3
datapoints in clusters: 881450
score: 78.6392


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:42:05 2019
elapsed time: 55.1807s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 6: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:242981] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:242981] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 109
datapoints_clusters: 881450
score: 78.6392
elapsed_time: 55.1807

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 1333.33333333 percent_correct: 0.8837254901960784

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 1500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[40430,1],1]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 1500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2221s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4807s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.90767s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 33974
find clusters duration: 0.230509s
detected clusters: 109
size cluster i: 0 -> 10000
size cluster i: 1 -> 8829
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9516
size cluster i: 8 -> 9483
size cluster i: 9 -> 9270
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10001
size cluster i: 16 -> 9986
size cluster i: 17 -> 10000
size cluster i: 18 -> 10000
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 9989
size cluster i: 24 -> 10000
size cluster i: 25 -> 10000
size cluster i: 26 -> 9965
size cluster i: 27 -> 10000
size cluster i: 28 -> 8963
size cluster i: 29 -> 10000
size cluster i: 30 -> 10001
size cluster i: 31 -> 9374
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10000
size cluster i: 36 -> 10000
size cluster i: 37 -> 10000
size cluster i: 38 -> 9680
size cluster i: 39 -> 10000
size cluster i: 40 -> 10000
size cluster i: 41 -> 10000
size cluster i: 42 -> 10000
size cluster i: 43 -> 10000
size cluster i: 44 -> 8413
size cluster i: 45 -> 10000
size cluster i: 46 -> 10000
size cluster i: 47 -> 8471
size cluster i: 48 -> 9268
size cluster i: 49 -> 10000
size cluster i: 50 -> 9931
size cluster i: 51 -> 6846
size cluster i: 52 -> 10000
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 9998
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10001
size cluster i: 60 -> 9766
size cluster i: 61 -> 8420
size cluster i: 62 -> 10000
size cluster i: 63 -> 10000
size cluster i: 64 -> 10000
size cluster i: 65 -> 10000
size cluster i: 66 -> 10000
size cluster i: 67 -> 10000
size cluster i: 68 -> 10000
size cluster i: 69 -> 9932
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 9998
size cluster i: 73 -> 10000
size cluster i: 74 -> 10000
size cluster i: 75 -> 9907
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10001
size cluster i: 79 -> 9471
size cluster i: 80 -> 9986
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 8711
size cluster i: 84 -> 10000
size cluster i: 85 -> 4597
size cluster i: 86 -> 4444
size cluster i: 87 -> 7283
size cluster i: 88 -> 3944
size cluster i: 89 -> 2030
size cluster i: 90 -> 1020
size cluster i: 91 -> 408
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 4
size cluster i: 95 -> 3
size cluster i: 96 -> 2
size cluster i: 97 -> 3
size cluster i: 98 -> 4
size cluster i: 99 -> 3
size cluster i: 100 -> 4
size cluster i: 102 -> 2
size cluster i: 103 -> 2
datapoints in clusters: 857943
score: 76.542


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:43:10 2019
elapsed time: 55.1816s


Finishing: 
---------- 
Node 7 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 5: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:243287] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:243287] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 109
datapoints_clusters: 857943
score: 76.542
elapsed_time: 55.1816

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 1500.0 percent_correct: 0.8606794117647059
thresholds:[ 537.03703704  574.07407407  611.11111111  648.14814815  685.18518519
  722.22222222  759.25925926  796.2962963 ], threshold_step:37.037037037

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 537.0370370370371 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[39958,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 537.037


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0953s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4823s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.83912s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38049
find clusters duration: 0.271622s
detected clusters: 94
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 20005
size cluster i: 6 -> 10000
size cluster i: 7 -> 9853
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9999
size cluster i: 15 -> 10001
size cluster i: 16 -> 30006
size cluster i: 17 -> 10003
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10006
size cluster i: 25 -> 10002
size cluster i: 26 -> 10002
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10005
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10001
size cluster i: 34 -> 10002
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10005
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10001
size cluster i: 42 -> 5205
size cluster i: 43 -> 10001
size cluster i: 44 -> 20004
size cluster i: 45 -> 10000
size cluster i: 46 -> 10000
size cluster i: 47 -> 10002
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10002
size cluster i: 51 -> 10001
size cluster i: 52 -> 10004
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 20007
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10003
size cluster i: 59 -> 10003
size cluster i: 60 -> 10000
size cluster i: 61 -> 10003
size cluster i: 62 -> 10001
size cluster i: 63 -> 20009
size cluster i: 64 -> 10003
size cluster i: 65 -> 10002
size cluster i: 66 -> 10002
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 8689
size cluster i: 75 -> 10002
size cluster i: 76 -> 5607
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10003
size cluster i: 80 -> 10000
size cluster i: 81 -> 10001
size cluster i: 82 -> 10001
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10002
size cluster i: 86 -> 9206
size cluster i: 87 -> 5723
size cluster i: 88 -> 7631
size cluster i: 89 -> 10000
size cluster i: 90 -> 1542
size cluster i: 91 -> 2
datapoints in clusters: 973605
score: 89.7244


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:44:15 2019
elapsed time: 55.0231s


Finishing: 
---------- 
Beginning cleanup...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:243631] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:243631] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 94
datapoints_clusters: 973605
score: 89.7244
elapsed_time: 55.0231

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 537.037037037 percent_correct: 0.885593137254902

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 574.074074074074 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[37696,1],1]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 574.074


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1482s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.5187s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.91677s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37879
find clusters duration: 0.267474s
detected clusters: 96
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9687
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9994
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10003
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10006
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10001
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10001
size cluster i: 34 -> 10002
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 4045
size cluster i: 43 -> 10001
size cluster i: 44 -> 20004
size cluster i: 45 -> 9997
size cluster i: 46 -> 10000
size cluster i: 47 -> 10002
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10002
size cluster i: 51 -> 10001
size cluster i: 52 -> 10004
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 20005
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10003
size cluster i: 59 -> 10003
size cluster i: 60 -> 10000
size cluster i: 61 -> 10002
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 20008
size cluster i: 65 -> 10003
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10000
size cluster i: 71 -> 10004
size cluster i: 72 -> 10001
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 8035
size cluster i: 77 -> 10002
size cluster i: 78 -> 4637
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10003
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10002
size cluster i: 88 -> 8674
size cluster i: 89 -> 4733
size cluster i: 90 -> 6840
size cluster i: 91 -> 10000
size cluster i: 92 -> 887
size cluster i: 93 -> 2
size cluster i: 94 -> 4
datapoints in clusters: 967662
score: 91.0741


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:45:20 2019
elapsed time: 55.1951s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:243961] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:243961] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 967662
score: 91.0741
elapsed_time: 55.1951

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 574.074074074 percent_correct: 0.8994107843137255

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 611.1111111111111 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[37252,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 611.111


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1052s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.5034s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.97018s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37648
find clusters duration: 0.266195s
detected clusters: 103
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9408
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 9983
size cluster i: 15 -> 10001
size cluster i: 16 -> 20004
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10005
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 3014
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9982
size cluster i: 47 -> 10000
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20004
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 20007
size cluster i: 66 -> 10003
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10004
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 7324
size cluster i: 78 -> 10002
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10003
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 8073
size cluster i: 89 -> 3741
size cluster i: 90 -> 3777
size cluster i: 91 -> 6011
size cluster i: 92 -> 10000
size cluster i: 93 -> 451
size cluster i: 94 -> 4
size cluster i: 95 -> 4
size cluster i: 98 -> 2
size cluster i: 99 -> 3
size cluster i: 100 -> 3
datapoints in clusters: 961894
score: 91.4742


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:46:25 2019
elapsed time: 55.2011s


Finishing: 
---------- 
Beginning cleanup...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 3: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:244285] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:244285] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 103
datapoints_clusters: 961894
score: 91.4742
elapsed_time: 55.2011

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 611.111111111 percent_correct: 0.9035823529411765

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 648.1481481481482 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[37060,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 648.148


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0871s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4479s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.85697s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37458
find clusters duration: 0.261995s
detected clusters: 101
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9000
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9961
size cluster i: 15 -> 10000
size cluster i: 16 -> 20003
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10002
size cluster i: 22 -> 10003
size cluster i: 23 -> 20005
size cluster i: 24 -> 10003
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 2095
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9937
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20003
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 9999
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 6515
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 9998
size cluster i: 88 -> 10001
size cluster i: 89 -> 7378
size cluster i: 90 -> 2967
size cluster i: 91 -> 5143
size cluster i: 92 -> 10000
size cluster i: 93 -> 2898
size cluster i: 94 -> 224
size cluster i: 96 -> 2
size cluster i: 98 -> 2
size cluster i: 99 -> 4
datapoints in clusters: 956222
score: 92.8098


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:47:30 2019
elapsed time: 54.9831s


Finishing: 
---------- 
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:244605] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:244605] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 101
datapoints_clusters: 956222
score: 92.8098
elapsed_time: 54.9831

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 648.148148148 percent_correct: 0.9078617647058823
-> new best_percent_correct:0.9078617647058823new best_threshold:648.148148148

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 685.1851851851852 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[38665,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 685.185


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.125s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.5472s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.13012s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37230
find clusters duration: 0.26391s
detected clusters: 111
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 8541
size cluster i: 8 -> 10001
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9904
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10002
size cluster i: 22 -> 10002
size cluster i: 23 -> 20004
size cluster i: 24 -> 10003
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10003
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 1383
size cluster i: 44 -> 10001
size cluster i: 45 -> 20004
size cluster i: 46 -> 9863
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20003
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 9999
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 5657
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 9991
size cluster i: 88 -> 10001
size cluster i: 89 -> 6605
size cluster i: 90 -> 4304
size cluster i: 91 -> 10000
size cluster i: 92 -> 2215
size cluster i: 93 -> 2210
size cluster i: 94 -> 83
size cluster i: 95 -> 2
size cluster i: 96 -> 4
size cluster i: 97 -> 4
size cluster i: 99 -> 3
size cluster i: 100 -> 2
size cluster i: 102 -> 2
size cluster i: 103 -> 2
size cluster i: 104 -> 3
size cluster i: 105 -> 3
size cluster i: 107 -> 2
size cluster i: 108 -> 2
datapoints in clusters: 950877
score: 82.9687


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:48:35 2019
elapsed time: 55.4033s


Finishing: 
---------- 
Node 7 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 5: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:244912] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:244912] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with theWarning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
value given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 111
datapoints_clusters: 950877
score: 82.9687
elapsed_time: 55.4033

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 685.185185185 percent_correct: 0.9026147058823529

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 722.2222222222222 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[38503,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 722.222


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1635s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.4995s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.07136s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37049
find clusters duration: 0.261212s
detected clusters: 116
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 9999
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 10001
size cluster i: 8 -> 10000
size cluster i: 9 -> 10000
size cluster i: 10 -> 10003
size cluster i: 11 -> 10001
size cluster i: 12 -> 10000
size cluster i: 13 -> 9810
size cluster i: 14 -> 10000
size cluster i: 15 -> 20001
size cluster i: 16 -> 10001
size cluster i: 17 -> 10000
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10002
size cluster i: 21 -> 10002
size cluster i: 22 -> 20003
size cluster i: 23 -> 10003
size cluster i: 24 -> 10000
size cluster i: 25 -> 10001
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10000
size cluster i: 30 -> 10002
size cluster i: 31 -> 10000
size cluster i: 32 -> 9999
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10003
size cluster i: 37 -> 10001
size cluster i: 38 -> 10002
size cluster i: 39 -> 10001
size cluster i: 40 -> 10000
size cluster i: 41 -> 10003
size cluster i: 42 -> 10000
size cluster i: 43 -> 20003
size cluster i: 44 -> 9726
size cluster i: 45 -> 10000
size cluster i: 46 -> 10001
size cluster i: 47 -> 10000
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10003
size cluster i: 52 -> 10000
size cluster i: 53 -> 10000
size cluster i: 54 -> 20003
size cluster i: 55 -> 10000
size cluster i: 56 -> 10001
size cluster i: 57 -> 10002
size cluster i: 58 -> 10002
size cluster i: 59 -> 10000
size cluster i: 60 -> 10002
size cluster i: 61 -> 10001
size cluster i: 62 -> 10001
size cluster i: 63 -> 10005
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10000
size cluster i: 68 -> 10001
size cluster i: 69 -> 9998
size cluster i: 70 -> 10002
size cluster i: 71 -> 10001
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 4790
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10003
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 9975
size cluster i: 86 -> 8013
size cluster i: 87 -> 10001
size cluster i: 88 -> 5799
size cluster i: 89 -> 3478
size cluster i: 90 -> 10000
size cluster i: 91 -> 856
size cluster i: 92 -> 1583
size cluster i: 93 -> 1656
size cluster i: 94 -> 2
size cluster i: 95 -> 20
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 99 -> 6
size cluster i: 100 -> 3
size cluster i: 101 -> 3
size cluster i: 102 -> 3
size cluster i: 103 -> 4
size cluster i: 104 -> 2
size cluster i: 107 -> 3
size cluster i: 111 -> 2
datapoints in clusters: 945824
score: 77.8914


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:49:41 2019
elapsed time: 55.3386s


Finishing: 
---------- 
Beginning cleanup...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:245214] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:245214] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 116
datapoints_clusters: 945824
score: 77.8914
elapsed_time: 55.3386

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 722.222222222 percent_correct: 0.8976696078431372

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 759.2592592592592 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[38041,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 759.259


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1647s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.5095s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.05549s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 36861
find clusters duration: 0.259096s
detected clusters: 111
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 9997
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 10001
size cluster i: 8 -> 10000
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10003
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9671
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 20003
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10000
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 9998
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10003
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10002
size cluster i: 43 -> 10000
size cluster i: 44 -> 20003
size cluster i: 45 -> 9527
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 20003
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10000
size cluster i: 61 -> 10001
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10003
size cluster i: 66 -> 10001
size cluster i: 67 -> 10001
size cluster i: 68 -> 10000
size cluster i: 69 -> 10001
size cluster i: 70 -> 9990
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 3913
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 9932
size cluster i: 87 -> 7341
size cluster i: 88 -> 10000
size cluster i: 89 -> 5014
size cluster i: 90 -> 2799
size cluster i: 91 -> 10000
size cluster i: 92 -> 488
size cluster i: 93 -> 1103
size cluster i: 94 -> 1180
size cluster i: 95 -> 3
size cluster i: 96 -> 3
size cluster i: 98 -> 2
size cluster i: 100 -> 2
size cluster i: 101 -> 4
size cluster i: 102 -> 3
size cluster i: 103 -> 2
size cluster i: 105 -> 3
datapoints in clusters: 941049
score: 82.1111


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:50:46 2019
elapsed time: 55.334s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 7: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:245536] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:245536] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 111
datapoints_clusters: 941049
score: 82.1111
elapsed_time: 55.334

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 759.259259259 percent_correct: 0.9028343137254902

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-06 --k 6 --epsilon 0.001 --threshold 796.2962962962963 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[60376,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-06
k: 6
threshold: 796.296


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1558s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0227662
delta: 0.0522043
delta: 0.040487
delta: 0.0263514
delta: 0.0268883
delta: 0.0354235
delta: 0.0223369
delta: 0.0128135
delta: 0.0160162
delta: 0.00527968
delta: 0.0208103
delta: 0.0150187
delta: 0.00507932
delta: 0.00148946
delta: 0.00154097
delta: 0.00386133
delta: 0.00115102
delta: 0.000766263
delta: 0.00158197
delta: 0.00097899
delta: 0.000241499
delta: 0.00347325
delta: 0.000273173
delta: 0.000440172
delta: 8.18287e-05
delta: 0.000143168
delta: 0.000696858
delta: 4.63046e-05
delta: 0.000125762
delta: 2.13767e-05
delta: 1.08221e-05
delta: 0.00014349
delta: 6.45954e-06
delta: 1.25165e-05
delta: 9.37913e-06
delta: 3.53325e-06
delta: 3.57816e-05
delta: 2.52538e-06
delta: 1.75821e-06
delta: 9.99273e-07
delta: 9.35277e-07
delta: 1.79215e-06
delta: 6.80946e-07
delta: 8.67858e-07
delta: 1.79097e-07
delta: 2.96056e-07
delta: 1.71901e-06
delta: 9.16908e-08
delta: 2.48167e-07
delta: 5.95749e-08
delta: 8.64598e-08
delta: 5.53691e-08
delta: 1.91557e-07
delta: 5.98259e-08
delta: 2.99406e-08
Number of iterations: 55 (max. 1000)
Final norm of residuum: 2.99406e-08
counted_mult_calls: 57
solver duration: 32.489s
Number of grid points:     397825
alpha min: -444.015 max: 1573.82
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.91554s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 36736
find clusters duration: 0.271846s
detected clusters: 119
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 9985
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9508
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 20003
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10000
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 9991
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10003
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10001
size cluster i: 43 -> 10000
size cluster i: 44 -> 20002
size cluster i: 45 -> 9257
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 20002
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10001
size cluster i: 59 -> 10001
size cluster i: 60 -> 10000
size cluster i: 61 -> 10001
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10004
size cluster i: 65 -> 10003
size cluster i: 66 -> 10001
size cluster i: 67 -> 10000
size cluster i: 68 -> 10000
size cluster i: 69 -> 10001
size cluster i: 70 -> 9982
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 3136
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 9855
size cluster i: 87 -> 10000
size cluster i: 88 -> 6664
size cluster i: 89 -> 4199
size cluster i: 90 -> 2163
size cluster i: 91 -> 10000
size cluster i: 92 -> 738
size cluster i: 93 -> 815
size cluster i: 94 -> 256
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 3
size cluster i: 102 -> 2
size cluster i: 103 -> 3
size cluster i: 104 -> 2
size cluster i: 105 -> 2
size cluster i: 106 -> 2
size cluster i: 107 -> 2
size cluster i: 109 -> 2
size cluster i: 111 -> 2
size cluster i: 112 -> 3
datapoints in clusters: 936649
score: 74.3809


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:51:51 2019
elapsed time: 55.1782s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:245857] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:245857] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 119
datapoints_clusters: 936649
score: 74.3809
elapsed_time: 55.1782

--------------- END RUN -----------------
attempt lambda: 1e-06 threshold: 796.296296296 percent_correct: 0.8985225490196078
thresholds:[0.0, 166.66666666666666, 333.33333333333331, 500.0, 666.66666666666663, 833.33333333333326, 1000.0, 1166.6666666666665, 1333.3333333333333, 1500.0], threshold_step:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 0.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[59925,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 0


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1394s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5602s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.01346s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37747
find clusters duration: 64.5613s
detected clusters: 1
size cluster i: 0 -> 1020000
datapoints in clusters: 1020000
score: 1


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:53:41 2019
elapsed time: 99.6223s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 7: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:246188] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:246188] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 1
datapoints_clusters: 1020000
score: 1.0
elapsed_time: 99.6223

--------------- END RUN -----------------
attempt lambda: 1e-05 threshold: 0.0 percent_correct: 0.00980392156862745
-> new best_percent_correct:0.00980392156862745new best_threshold:0.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 166.66666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[59726,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 166.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.179s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5232s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.89313s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37288
find clusters duration: 0.346593s
detected clusters: 97
size cluster i: 0 -> 70038
size cluster i: 1 -> 20002
size cluster i: 2 -> 20010
size cluster i: 3 -> 9997
size cluster i: 4 -> 10006
size cluster i: 5 -> 130087
size cluster i: 6 -> 10001
size cluster i: 7 -> 10002
size cluster i: 8 -> 10000
size cluster i: 9 -> 10001
size cluster i: 10 -> 10009
size cluster i: 11 -> 10000
size cluster i: 12 -> 9937
size cluster i: 13 -> 10005
size cluster i: 14 -> 10010
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10004
size cluster i: 18 -> 20004
size cluster i: 19 -> 10004
size cluster i: 20 -> 10006
size cluster i: 21 -> 10003
size cluster i: 22 -> 10001
size cluster i: 23 -> 10004
size cluster i: 24 -> 20006
size cluster i: 25 -> 10000
size cluster i: 26 -> 10002
size cluster i: 27 -> 30019
size cluster i: 28 -> 10002
size cluster i: 29 -> 10002
size cluster i: 30 -> 20008
size cluster i: 31 -> 10004
size cluster i: 32 -> 10003
size cluster i: 33 -> 10003
size cluster i: 34 -> 2353
size cluster i: 35 -> 10001
size cluster i: 36 -> 20009
size cluster i: 37 -> 9989
size cluster i: 38 -> 10000
size cluster i: 39 -> 10004
size cluster i: 40 -> 10006
size cluster i: 41 -> 10000
size cluster i: 42 -> 10001
size cluster i: 43 -> 10009
size cluster i: 44 -> 10003
size cluster i: 45 -> 10001
size cluster i: 46 -> 10003
size cluster i: 47 -> 20003
size cluster i: 48 -> 10002
size cluster i: 49 -> 10002
size cluster i: 50 -> 20015
size cluster i: 51 -> 10004
size cluster i: 52 -> 10003
size cluster i: 53 -> 9999
size cluster i: 54 -> 10002
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 3531
size cluster i: 58 -> 10003
size cluster i: 59 -> 10005
size cluster i: 60 -> 10000
size cluster i: 61 -> 10000
size cluster i: 62 -> 10001
size cluster i: 63 -> 9733
size cluster i: 64 -> 10002
size cluster i: 65 -> 7459
size cluster i: 66 -> 5956
size cluster i: 67 -> 4567
size cluster i: 68 -> 10000
size cluster i: 69 -> 1991
size cluster i: 70 -> 401
size cluster i: 71 -> 823
size cluster i: 72 -> 2
size cluster i: 73 -> 2
size cluster i: 74 -> 4
size cluster i: 75 -> 3
size cluster i: 76 -> 2
size cluster i: 77 -> 3
size cluster i: 78 -> 2
size cluster i: 79 -> 6
size cluster i: 80 -> 2
size cluster i: 82 -> 2
size cluster i: 84 -> 3
size cluster i: 85 -> 4
size cluster i: 89 -> 2
datapoints in clusters: 947112
score: 90.0685


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:54:27 2019
elapsed time: 35.2904s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:246519] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:246519] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 947112
score: 90.0685
elapsed_time: 35.2904

--------------- END RUN -----------------
attempt lambda: 1e-05 threshold: 166.666666667 percent_correct: 0.6729352941176471
-> new best_percent_correct:0.6729352941176471new best_threshold:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[61433,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 333.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2299s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5325s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.87804s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 34254
find clusters duration: 0.256038s
detected clusters: 100
size cluster i: 0 -> 10001
size cluster i: 1 -> 9220
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9543
size cluster i: 8 -> 9909
size cluster i: 9 -> 9962
size cluster i: 10 -> 10001
size cluster i: 11 -> 30005
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10001
size cluster i: 18 -> 10001
size cluster i: 19 -> 20004
size cluster i: 20 -> 10001
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 10000
size cluster i: 25 -> 9989
size cluster i: 26 -> 10001
size cluster i: 27 -> 8662
size cluster i: 28 -> 6017
size cluster i: 29 -> 10000
size cluster i: 30 -> 10001
size cluster i: 31 -> 9405
size cluster i: 32 -> 10002
size cluster i: 33 -> 9980
size cluster i: 34 -> 10001
size cluster i: 35 -> 10000
size cluster i: 36 -> 10000
size cluster i: 37 -> 10001
size cluster i: 38 -> 19971
size cluster i: 39 -> 9520
size cluster i: 40 -> 10000
size cluster i: 41 -> 10000
size cluster i: 42 -> 10000
size cluster i: 43 -> 10001
size cluster i: 44 -> 9216
size cluster i: 45 -> 10001
size cluster i: 46 -> 10000
size cluster i: 47 -> 9457
size cluster i: 48 -> 20002
size cluster i: 49 -> 9962
size cluster i: 50 -> 7694
size cluster i: 51 -> 2491
size cluster i: 52 -> 10000
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 20005
size cluster i: 59 -> 10003
size cluster i: 60 -> 9151
size cluster i: 61 -> 10001
size cluster i: 62 -> 10001
size cluster i: 63 -> 10000
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10000
size cluster i: 67 -> 9704
size cluster i: 68 -> 10000
size cluster i: 69 -> 8160
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 8542
size cluster i: 76 -> 10000
size cluster i: 77 -> 9000
size cluster i: 78 -> 10001
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 5604
size cluster i: 82 -> 3283
size cluster i: 83 -> 4698
size cluster i: 84 -> 5879
size cluster i: 85 -> 290
size cluster i: 86 -> 4
size cluster i: 87 -> 4
size cluster i: 88 -> 2
size cluster i: 89 -> 9
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 3
datapoints in clusters: 865385
score: 84.8417


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:55:12 2019
elapsed time: 35.2308s


Finishing: 
---------- 
Node 4 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 8: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 TiWarning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
 and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:246848] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:246848] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 100
datapoints_clusters: 865385
score: 84.8417
elapsed_time: 35.2308

--------------- END RUN -----------------
attempt lambda: 1e-05 threshold: 333.333333333 percent_correct: 0.8091107843137255
-> new best_percent_correct:0.8091107843137255new best_threshold:333.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[61126,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1075s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.583s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.88406s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 28928
find clusters duration: 0.20398s
detected clusters: 153
size cluster i: 0 -> 10000
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 9877
size cluster i: 5 -> 9720
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9481
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 6477
size cluster i: 13 -> 10000
size cluster i: 14 -> 9930
size cluster i: 15 -> 9991
size cluster i: 16 -> 10000
size cluster i: 17 -> 10000
size cluster i: 18 -> 10000
size cluster i: 19 -> 9844
size cluster i: 20 -> 10000
size cluster i: 21 -> 4631
size cluster i: 22 -> 10000
size cluster i: 23 -> 9707
size cluster i: 24 -> 10000
size cluster i: 25 -> 10001
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 10000
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 9987
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10000
size cluster i: 36 -> 10000
size cluster i: 37 -> 10000
size cluster i: 38 -> 10000
size cluster i: 39 -> 20001
size cluster i: 40 -> 9927
size cluster i: 41 -> 10000
size cluster i: 42 -> 9935
size cluster i: 43 -> 10000
size cluster i: 44 -> 10000
size cluster i: 45 -> 10001
size cluster i: 46 -> 10001
size cluster i: 47 -> 7856
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 7815
size cluster i: 52 -> 10000
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 9972
size cluster i: 56 -> 8558
size cluster i: 57 -> 10000
size cluster i: 58 -> 6478
size cluster i: 59 -> 9590
size cluster i: 60 -> 10000
size cluster i: 61 -> 8716
size cluster i: 62 -> 3399
size cluster i: 63 -> 10000
size cluster i: 64 -> 6956
size cluster i: 65 -> 10000
size cluster i: 66 -> 10001
size cluster i: 67 -> 8536
size cluster i: 68 -> 6044
size cluster i: 69 -> 9795
size cluster i: 70 -> 10000
size cluster i: 71 -> 2134
size cluster i: 72 -> 9998
size cluster i: 73 -> 2535
size cluster i: 74 -> 2662
size cluster i: 75 -> 1709
size cluster i: 76 -> 3138
size cluster i: 77 -> 2220
size cluster i: 78 -> 3705
size cluster i: 79 -> 2390
size cluster i: 80 -> 720
size cluster i: 81 -> 223
size cluster i: 82 -> 635
size cluster i: 83 -> 29
size cluster i: 84 -> 709
size cluster i: 85 -> 32
size cluster i: 86 -> 3
size cluster i: 87 -> 3
size cluster i: 88 -> 6
size cluster i: 89 -> 2
size cluster i: 90 -> 19
size cluster i: 91 -> 2
size cluster i: 93 -> 3
size cluster i: 94 -> 3
size cluster i: 95 -> 3
size cluster i: 96 -> 3
size cluster i: 97 -> 2
size cluster i: 98 -> 4
size cluster i: 99 -> 2
size cluster i: 100 -> 4
size cluster i: 101 -> 2
size cluster i: 102 -> 3
size cluster i: 103 -> 2
size cluster i: 104 -> 2
size cluster i: 105 -> 2
size cluster i: 106 -> 2
size cluster i: 107 -> 3
size cluster i: 108 -> 9
size cluster i: 109 -> 2
size cluster i: 111 -> 3
size cluster i: 112 -> 3
size cluster i: 113 -> 2
size cluster i: 114 -> 2
size cluster i: 115 -> 3
size cluster i: 116 -> 3
size cluster i: 117 -> 2
size cluster i: 118 -> 2
size cluster i: 121 -> 4
size cluster i: 123 -> 3
size cluster i: 124 -> 2
size cluster i: 125 -> 3
size cluster i: 126 -> 4
size cluster i: 127 -> 3
size cluster i: 131 -> 2
size cluster i: 132 -> 2
size cluster i: 136 -> 3
size cluster i: 137 -> 3
size cluster i: 138 -> 3
size cluster i: 139 -> 2
size cluster i: 140 -> 4
size cluster i: 142 -> 2
size cluster i: 145 -> 2
datapoints in clusters: 716236
score: 33.003


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:55:57 2019
elapsed time: 35.1325s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 2 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 7: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:247167] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:247167] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 153
datapoints_clusters: 716236
score: 33.003
elapsed_time: 35.1325

--------------- END RUN -----------------
datapoints_clusters too low
thresholds:[ 203.7037037   240.74074074  277.77777778  314.81481481  351.85185185
  388.88888889  425.92592593  462.96296296], threshold_step:37.037037037

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 203.7037037037037 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[60682,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 203.704


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1579s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5422s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.91389s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 36392
find clusters duration: 0.291646s
detected clusters: 119
size cluster i: 0 -> 10003
size cluster i: 1 -> 20002
size cluster i: 2 -> 20007
size cluster i: 3 -> 9881
size cluster i: 4 -> 10003
size cluster i: 5 -> 40023
size cluster i: 6 -> 10000
size cluster i: 7 -> 10002
size cluster i: 8 -> 10000
size cluster i: 9 -> 10000
size cluster i: 10 -> 10007
size cluster i: 11 -> 50019
size cluster i: 12 -> 10000
size cluster i: 13 -> 9407
size cluster i: 14 -> 10001
size cluster i: 15 -> 10008
size cluster i: 16 -> 10000
size cluster i: 17 -> 9998
size cluster i: 18 -> 10001
size cluster i: 19 -> 10002
size cluster i: 20 -> 10002
size cluster i: 21 -> 30017
size cluster i: 22 -> 10004
size cluster i: 23 -> 10001
size cluster i: 24 -> 10001
size cluster i: 25 -> 10003
size cluster i: 26 -> 10002
size cluster i: 27 -> 20004
size cluster i: 28 -> 10000
size cluster i: 29 -> 9981
size cluster i: 30 -> 10001
size cluster i: 31 -> 10002
size cluster i: 32 -> 10001
size cluster i: 33 -> 10001
size cluster i: 34 -> 10003
size cluster i: 35 -> 10002
size cluster i: 36 -> 10003
size cluster i: 37 -> 10000
size cluster i: 38 -> 20004
size cluster i: 39 -> 9682
size cluster i: 40 -> 10000
size cluster i: 41 -> 10002
size cluster i: 42 -> 10001
size cluster i: 43 -> 10000
size cluster i: 44 -> 30013
size cluster i: 45 -> 10001
size cluster i: 46 -> 10005
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 9996
size cluster i: 51 -> 10002
size cluster i: 52 -> 10003
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10001
size cluster i: 56 -> 20013
size cluster i: 57 -> 20009
size cluster i: 58 -> 10002
size cluster i: 59 -> 10003
size cluster i: 60 -> 9955
size cluster i: 61 -> 10001
size cluster i: 62 -> 10001
size cluster i: 63 -> 10000
size cluster i: 64 -> 10000
size cluster i: 65 -> 10002
size cluster i: 66 -> 10002
size cluster i: 67 -> 10000
size cluster i: 68 -> 10004
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 10001
size cluster i: 72 -> 10000
size cluster i: 73 -> 3939
size cluster i: 74 -> 8268
size cluster i: 75 -> 1670
size cluster i: 76 -> 9995
size cluster i: 77 -> 2595
size cluster i: 78 -> 394
size cluster i: 79 -> 778
size cluster i: 80 -> 12
size cluster i: 81 -> 315
size cluster i: 82 -> 2
size cluster i: 83 -> 28
size cluster i: 84 -> 2
size cluster i: 85 -> 3
size cluster i: 86 -> 2
size cluster i: 87 -> 3
size cluster i: 88 -> 3
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 92 -> 2
size cluster i: 93 -> 3
size cluster i: 94 -> 12
size cluster i: 95 -> 3
size cluster i: 96 -> 4
size cluster i: 97 -> 3
size cluster i: 98 -> 2
size cluster i: 100 -> 2
size cluster i: 101 -> 3
size cluster i: 102 -> 2
size cluster i: 103 -> 2
size cluster i: 105 -> 3
size cluster i: 106 -> 2
size cluster i: 108 -> 2
size cluster i: 109 -> 2
size cluster i: 112 -> 2
datapoints in clusters: 927172
score: 73.6284


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:56:35 2019
elapsed time: 35.2493s


Finishing: 
---------- 
Beginning cleanup...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 7: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:247475] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:247475] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 119
datapoints_clusters: 927172
score: 73.6284
elapsed_time: 35.2493

--------------- END RUN -----------------
attempt lambda: 1e-05 threshold: 203.703703704 percent_correct: 0.76145

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 240.74074074074073 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[60486,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 240.741


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1767s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5047s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.89646s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 35585
find clusters duration: 0.283517s
detected clusters: 119
size cluster i: 0 -> 10002
size cluster i: 1 -> 19998
size cluster i: 2 -> 20005
size cluster i: 3 -> 10003
size cluster i: 4 -> 30013
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10004
size cluster i: 10 -> 40010
size cluster i: 11 -> 10000
size cluster i: 12 -> 7957
size cluster i: 13 -> 10001
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10002
size cluster i: 19 -> 20008
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 9245
size cluster i: 24 -> 9994
size cluster i: 25 -> 10000
size cluster i: 26 -> 10000
size cluster i: 27 -> 20003
size cluster i: 28 -> 9977
size cluster i: 29 -> 9704
size cluster i: 30 -> 10001
size cluster i: 31 -> 10002
size cluster i: 32 -> 9998
size cluster i: 33 -> 10005
size cluster i: 34 -> 10001
size cluster i: 35 -> 10003
size cluster i: 36 -> 10002
size cluster i: 37 -> 10002
size cluster i: 38 -> 10004
size cluster i: 39 -> 9968
size cluster i: 40 -> 20003
size cluster i: 41 -> 9998
size cluster i: 42 -> 10001
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 30009
size cluster i: 46 -> 9995
size cluster i: 47 -> 10004
size cluster i: 48 -> 10000
size cluster i: 49 -> 9998
size cluster i: 50 -> 10000
size cluster i: 51 -> 9907
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10001
size cluster i: 57 -> 20011
size cluster i: 58 -> 10003
size cluster i: 59 -> 8562
size cluster i: 60 -> 10001
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 9571
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10000
size cluster i: 67 -> 10000
size cluster i: 68 -> 10000
size cluster i: 69 -> 10000
size cluster i: 70 -> 10000
size cluster i: 71 -> 10003
size cluster i: 72 -> 9979
size cluster i: 73 -> 10000
size cluster i: 74 -> 9978
size cluster i: 75 -> 10001
size cluster i: 76 -> 10000
size cluster i: 77 -> 1165
size cluster i: 78 -> 366
size cluster i: 79 -> 9780
size cluster i: 80 -> 5521
size cluster i: 81 -> 579
size cluster i: 82 -> 6
size cluster i: 83 -> 9
size cluster i: 84 -> 30
size cluster i: 85 -> 17
size cluster i: 86 -> 3
size cluster i: 87 -> 4
size cluster i: 88 -> 3
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 4
size cluster i: 92 -> 2
size cluster i: 93 -> 5
size cluster i: 94 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 3
size cluster i: 100 -> 2
size cluster i: 101 -> 2
size cluster i: 103 -> 4
size cluster i: 104 -> 5
size cluster i: 108 -> 2
size cluster i: 109 -> 5
size cluster i: 111 -> 3
size cluster i: 117 -> 3
datapoints in clusters: 912497
score: 72.463


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:57:20 2019
elapsed time: 35.2035s


Finishing: 
---------- 
Beginning cleanup...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:247807] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:247807] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 119
datapoints_clusters: 912497
score: 72.463
elapsed_time: 35.2035

--------------- END RUN -----------------
attempt lambda: 1e-05 threshold: 240.740740741 percent_correct: 0.786435294117647

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 277.77777777777777 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[57984,1],5]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 277.778


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1463s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5857s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.90837s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 35136
find clusters duration: 0.271116s
detected clusters: 95
size cluster i: 0 -> 10001
size cluster i: 1 -> 9916
size cluster i: 2 -> 10000
size cluster i: 3 -> 10002
size cluster i: 4 -> 30007
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9971
size cluster i: 8 -> 9997
size cluster i: 9 -> 9999
size cluster i: 10 -> 10002
size cluster i: 11 -> 30007
size cluster i: 12 -> 10000
size cluster i: 13 -> 10001
size cluster i: 14 -> 10002
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10002
size cluster i: 19 -> 20006
size cluster i: 20 -> 10004
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 7916
size cluster i: 25 -> 9914
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 20003
size cluster i: 29 -> 9778
size cluster i: 30 -> 8773
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 9949
size cluster i: 34 -> 10003
size cluster i: 35 -> 10000
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10000
size cluster i: 39 -> 10001
size cluster i: 40 -> 9736
size cluster i: 41 -> 20001
size cluster i: 42 -> 9955
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 30005
size cluster i: 47 -> 9900
size cluster i: 48 -> 10002
size cluster i: 49 -> 10000
size cluster i: 50 -> 9966
size cluster i: 51 -> 10000
size cluster i: 52 -> 9495
size cluster i: 53 -> 5855
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 20008
size cluster i: 61 -> 10003
size cluster i: 62 -> 6592
size cluster i: 63 -> 10001
size cluster i: 64 -> 10002
size cluster i: 65 -> 10000
size cluster i: 66 -> 8494
size cluster i: 67 -> 10001
size cluster i: 68 -> 9987
size cluster i: 69 -> 10000
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 10003
size cluster i: 75 -> 9826
size cluster i: 76 -> 10000
size cluster i: 77 -> 9817
size cluster i: 78 -> 10001
size cluster i: 79 -> 10000
size cluster i: 80 -> 8815
size cluster i: 81 -> 2688
size cluster i: 82 -> 129
size cluster i: 83 -> 58
size cluster i: 84 -> 75
size cluster i: 85 -> 5
size cluster i: 86 -> 2
size cluster i: 87 -> 6
size cluster i: 88 -> 2
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 92 -> 2
datapoints in clusters: 897706
score: 83.6099


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:58:05 2019
elapsed time: 35.2475s


Finishing: 
---------- 
Beginning cleanup...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:248121] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:248121] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 95
datapoints_clusters: 897706
score: 83.6099
elapsed_time: 35.2475

--------------- END RUN -----------------
attempt lambda: 1e-05 threshold: 277.777777778 percent_correct: 0.8014823529411764

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 314.8148148148148 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[57811,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 314.815


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1542s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5419s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.83817s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 34563
find clusters duration: 0.260175s
detected clusters: 96
size cluster i: 0 -> 10001
size cluster i: 1 -> 9557
size cluster i: 2 -> 10000
size cluster i: 3 -> 10001
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9779
size cluster i: 8 -> 9966
size cluster i: 9 -> 9982
size cluster i: 10 -> 10001
size cluster i: 11 -> 30006
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 20005
size cluster i: 20 -> 10003
size cluster i: 21 -> 10000
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 5810
size cluster i: 25 -> 10000
size cluster i: 26 -> 9998
size cluster i: 27 -> 10001
size cluster i: 28 -> 9174
size cluster i: 29 -> 7044
size cluster i: 30 -> 10001
size cluster i: 31 -> 10001
size cluster i: 32 -> 9704
size cluster i: 33 -> 10002
size cluster i: 34 -> 9992
size cluster i: 35 -> 10002
size cluster i: 36 -> 10000
size cluster i: 37 -> 10000
size cluster i: 38 -> 10001
size cluster i: 39 -> 8899
size cluster i: 40 -> 19989
size cluster i: 41 -> 9747
size cluster i: 42 -> 10001
size cluster i: 43 -> 10000
size cluster i: 44 -> 10000
size cluster i: 45 -> 10002
size cluster i: 46 -> 9537
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 9738
size cluster i: 50 -> 20002
size cluster i: 51 -> 9992
size cluster i: 52 -> 8443
size cluster i: 53 -> 3553
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 20006
size cluster i: 61 -> 10003
size cluster i: 62 -> 9538
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10000
size cluster i: 66 -> 10001
size cluster i: 67 -> 10001
size cluster i: 68 -> 10000
size cluster i: 69 -> 9868
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 6686
size cluster i: 74 -> 4396
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10001
size cluster i: 78 -> 9151
size cluster i: 79 -> 10000
size cluster i: 80 -> 9357
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 7002
size cluster i: 85 -> 796
size cluster i: 86 -> 3
size cluster i: 87 -> 2
size cluster i: 88 -> 3
size cluster i: 91 -> 3
size cluster i: 92 -> 2
datapoints in clusters: 877768
score: 82.6135


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:58:50 2019
elapsed time: 35.1296s


Finishing: 
---------- 
Node 4 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 2: Exiting... 
Node 4: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 3: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:248426] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:248426] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 877768
score: 82.6135
elapsed_time: 35.1296

--------------- END RUN -----------------
attempt lambda: 1e-05 threshold: 314.814814815 percent_correct: 0.8212303921568628
-> new best_percent_correct:0.8212303921568628new best_threshold:314.814814815

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 351.85185185185185 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[57375,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 351.852


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0748s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5835s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.90511s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 33838
find clusters duration: 0.240169s
detected clusters: 105
size cluster i: 0 -> 10001
size cluster i: 1 -> 8691
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9212
size cluster i: 8 -> 9783
size cluster i: 9 -> 9886
size cluster i: 10 -> 10001
size cluster i: 11 -> 10001
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 20001
size cluster i: 15 -> 10001
size cluster i: 16 -> 9999
size cluster i: 17 -> 10000
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10000
size cluster i: 24 -> 10000
size cluster i: 25 -> 10000
size cluster i: 26 -> 9968
size cluster i: 27 -> 10001
size cluster i: 28 -> 4827
size cluster i: 29 -> 10000
size cluster i: 30 -> 10001
size cluster i: 31 -> 8976
size cluster i: 32 -> 10001
size cluster i: 33 -> 9938
size cluster i: 34 -> 10000
size cluster i: 35 -> 10000
size cluster i: 36 -> 10000
size cluster i: 37 -> 10001
size cluster i: 38 -> 7982
size cluster i: 39 -> 10001
size cluster i: 40 -> 9200
size cluster i: 41 -> 10000
size cluster i: 42 -> 10000
size cluster i: 43 -> 10000
size cluster i: 44 -> 10001
size cluster i: 45 -> 8776
size cluster i: 46 -> 10001
size cluster i: 47 -> 10000
size cluster i: 48 -> 20002
size cluster i: 49 -> 9916
size cluster i: 50 -> 6850
size cluster i: 51 -> 10000
size cluster i: 52 -> 10000
size cluster i: 53 -> 10000
size cluster i: 54 -> 9998
size cluster i: 55 -> 10000
size cluster i: 56 -> 10001
size cluster i: 57 -> 10003
size cluster i: 58 -> 10002
size cluster i: 59 -> 8663
size cluster i: 60 -> 10001
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10000
size cluster i: 67 -> 9487
size cluster i: 68 -> 9999
size cluster i: 69 -> 7248
size cluster i: 70 -> 10002
size cluster i: 71 -> 10000
size cluster i: 72 -> 10000
size cluster i: 73 -> 10000
size cluster i: 74 -> 9919
size cluster i: 75 -> 10000
size cluster i: 76 -> 10001
size cluster i: 77 -> 7796
size cluster i: 78 -> 9996
size cluster i: 79 -> 10001
size cluster i: 80 -> 10000
size cluster i: 81 -> 9034
size cluster i: 82 -> 8485
size cluster i: 83 -> 10000
size cluster i: 84 -> 4445
size cluster i: 85 -> 4664
size cluster i: 86 -> 3672
size cluster i: 87 -> 1690
size cluster i: 88 -> 2369
size cluster i: 89 -> 69
size cluster i: 91 -> 2
size cluster i: 92 -> 2
size cluster i: 96 -> 2
size cluster i: 98 -> 2
size cluster i: 101 -> 2
datapoints in clusters: 851589
score: 79.3147


Runtimes: 
--------- 
finished computation at Thu Jan  3 13:59:35 2019
elapsed time: 35.1321s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:248742] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:248742] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 105
datapoints_clusters: 851589
score: 79.3147
elapsed_time: 35.1321

--------------- END RUN -----------------
attempt lambda: 1e-05 threshold: 351.851851852 percent_correct: 0.8348088235294118
-> new best_percent_correct:0.8348088235294118new best_threshold:351.851851852

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 388.8888888888889 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[59236,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 388.889


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.166s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.6465s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.81426s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 32827
find clusters duration: 0.237907s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 7308
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 8115
size cluster i: 8 -> 9216
size cluster i: 9 -> 9536
size cluster i: 10 -> 10001
size cluster i: 11 -> 10001
size cluster i: 12 -> 9996
size cluster i: 13 -> 10000
size cluster i: 14 -> 20001
size cluster i: 15 -> 10001
size cluster i: 16 -> 9971
size cluster i: 17 -> 10000
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10000
size cluster i: 22 -> 10000
size cluster i: 23 -> 10000
size cluster i: 24 -> 10000
size cluster i: 25 -> 10000
size cluster i: 26 -> 9837
size cluster i: 27 -> 10000
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 6075
size cluster i: 36 -> 10000
size cluster i: 37 -> 10000
size cluster i: 38 -> 10000
size cluster i: 39 -> 10000
size cluster i: 40 -> 10000
size cluster i: 41 -> 7429
size cluster i: 42 -> 10001
size cluster i: 43 -> 10000
size cluster i: 44 -> 8206
size cluster i: 45 -> 20001
size cluster i: 46 -> 9574
size cluster i: 47 -> 4839
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 9963
size cluster i: 52 -> 10000
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10002
size cluster i: 56 -> 9771
size cluster i: 57 -> 7085
size cluster i: 58 -> 10001
size cluster i: 59 -> 10000
size cluster i: 60 -> 10000
size cluster i: 61 -> 10000
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10000
size cluster i: 65 -> 8637
size cluster i: 66 -> 9970
size cluster i: 67 -> 5007
size cluster i: 68 -> 10002
size cluster i: 69 -> 9999
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 9656
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 9962
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 7763
size cluster i: 79 -> 5874
size cluster i: 80 -> 7218
size cluster i: 81 -> 10000
size cluster i: 82 -> 7719
size cluster i: 83 -> 2416
size cluster i: 84 -> 2570
size cluster i: 85 -> 2622
size cluster i: 86 -> 988
size cluster i: 87 -> 1963
size cluster i: 88 -> 600
size cluster i: 89 -> 3
size cluster i: 90 -> 2
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 3
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 2
datapoints in clusters: 819929
score: 77.1698


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:00:20 2019
elapsed time: 35.2043s


Finishing: 
---------- 
Node 5 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 8: Exiting... 
Node 1: Exiting... 
Node 6: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:249053] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:249053] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 819929
score: 77.1698
elapsed_time: 35.2043

--------------- END RUN -----------------
attempt lambda: 1e-05 threshold: 388.888888889 percent_correct: 0.8037901960784314

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 425.92592592592587 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[58783,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 425.926


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2641s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5399s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.07041s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 31613
find clusters duration: 0.227896s
detected clusters: 127
size cluster i: 0 -> 10000
size cluster i: 1 -> 5578
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 9998
size cluster i: 6 -> 9981
size cluster i: 7 -> 6653
size cluster i: 8 -> 8116
size cluster i: 9 -> 8854
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 9954
size cluster i: 13 -> 10000
size cluster i: 14 -> 20001
size cluster i: 15 -> 10001
size cluster i: 16 -> 9713
size cluster i: 17 -> 10000
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10000
size cluster i: 22 -> 10000
size cluster i: 23 -> 9997
size cluster i: 24 -> 10000
size cluster i: 25 -> 10000
size cluster i: 26 -> 9982
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10000
size cluster i: 36 -> 10000
size cluster i: 37 -> 10000
size cluster i: 38 -> 10000
size cluster i: 39 -> 5800
size cluster i: 40 -> 10000
size cluster i: 41 -> 10000
size cluster i: 42 -> 6743
size cluster i: 43 -> 20001
size cluster i: 44 -> 10000
size cluster i: 45 -> 10000
size cluster i: 46 -> 9997
size cluster i: 47 -> 9815
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10002
size cluster i: 52 -> 9422
size cluster i: 53 -> 5210
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 9498
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10000
size cluster i: 61 -> 10000
size cluster i: 62 -> 7250
size cluster i: 63 -> 9814
size cluster i: 64 -> 10000
size cluster i: 65 -> 9971
size cluster i: 66 -> 10000
size cluster i: 67 -> 10000
size cluster i: 68 -> 9055
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 9797
size cluster i: 72 -> 8894
size cluster i: 73 -> 9990
size cluster i: 74 -> 10000
size cluster i: 75 -> 6003
size cluster i: 76 -> 5672
size cluster i: 77 -> 10000
size cluster i: 78 -> 6051
size cluster i: 79 -> 3941
size cluster i: 80 -> 3671
size cluster i: 81 -> 1034
size cluster i: 82 -> 2876
size cluster i: 83 -> 3066
size cluster i: 84 -> 942
size cluster i: 85 -> 1117
size cluster i: 86 -> 3
size cluster i: 87 -> 769
size cluster i: 88 -> 143
size cluster i: 89 -> 309
size cluster i: 90 -> 2
size cluster i: 91 -> 3
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 3
size cluster i: 98 -> 4
size cluster i: 99 -> 5
size cluster i: 100 -> 2
size cluster i: 101 -> 3
size cluster i: 102 -> 2
size cluster i: 104 -> 2
size cluster i: 105 -> 2
size cluster i: 106 -> 3
size cluster i: 107 -> 3
size cluster i: 108 -> 2
size cluster i: 110 -> 2
size cluster i: 111 -> 3
size cluster i: 112 -> 2
size cluster i: 113 -> 2
size cluster i: 114 -> 2
size cluster i: 115 -> 2
size cluster i: 119 -> 2
size cluster i: 121 -> 3
datapoints in clusters: 785762
score: 56.2359


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:01:06 2019
elapsed time: 35.4337s


Finishing: 
---------- 
Node 2 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Node 4: Exiting... 
Cleanup done!
Node 2: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:249382] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:249382] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 127
datapoints_clusters: 785762
score: 56.2359
elapsed_time: 35.4337

--------------- END RUN -----------------
attempt lambda: 1e-05 threshold: 425.925925926 percent_correct: 0.7702598039215687

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1e-05 --k 6 --epsilon 0.001 --threshold 462.96296296296293 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[58606,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-05
k: 6
threshold: 462.963


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1463s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0222391
delta: 0.0355118
delta: 0.0186099
delta: 0.00656055
delta: 0.00312669
delta: 0.00138396
delta: 0.00192416
delta: 0.000310522
delta: 0.000222112
delta: 3.60603e-05
delta: 5.48379e-05
delta: 1.93389e-05
delta: 2.19638e-05
delta: 1.18486e-06
delta: 4.20265e-07
delta: 1.1687e-07
delta: 7.42938e-07
delta: 3.7979e-08
Number of iterations: 18 (max. 1000)
Final norm of residuum: 3.7979e-08
counted_mult_calls: 19
solver duration: 12.5321s
Number of grid points:     397825
alpha min: -60.3844 max: 217.579
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.18697s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 30299
find clusters duration: 0.213296s
detected clusters: 141
size cluster i: 0 -> 10000
size cluster i: 1 -> 3729
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 9978
size cluster i: 6 -> 9909
size cluster i: 7 -> 4825
size cluster i: 8 -> 6493
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 9819
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10001
size cluster i: 15 -> 8732
size cluster i: 16 -> 10000
size cluster i: 17 -> 9989
size cluster i: 18 -> 9997
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10000
size cluster i: 22 -> 9962
size cluster i: 23 -> 10000
size cluster i: 24 -> 10000
size cluster i: 25 -> 9916
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10000
size cluster i: 33 -> 9997
size cluster i: 34 -> 10000
size cluster i: 35 -> 10000
size cluster i: 36 -> 10000
size cluster i: 37 -> 10000
size cluster i: 38 -> 10000
size cluster i: 39 -> 3981
size cluster i: 40 -> 10000
size cluster i: 41 -> 10000
size cluster i: 42 -> 20001
size cluster i: 43 -> 9990
size cluster i: 44 -> 10000
size cluster i: 45 -> 9982
size cluster i: 46 -> 10000
size cluster i: 47 -> 9422
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 8762
size cluster i: 52 -> 10000
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 8849
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 9997
size cluster i: 60 -> 5538
size cluster i: 61 -> 9380
size cluster i: 62 -> 10000
size cluster i: 63 -> 7829
size cluster i: 64 -> 9872
size cluster i: 65 -> 10000
size cluster i: 66 -> 5052
size cluster i: 67 -> 10000
size cluster i: 68 -> 8149
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 9377
size cluster i: 72 -> 7719
size cluster i: 73 -> 9944
size cluster i: 74 -> 10000
size cluster i: 75 -> 4130
size cluster i: 76 -> 3965
size cluster i: 77 -> 10000
size cluster i: 78 -> 4220
size cluster i: 79 -> 2043
size cluster i: 80 -> 3280
size cluster i: 81 -> 1193
size cluster i: 82 -> 1872
size cluster i: 83 -> 198
size cluster i: 84 -> 1609
size cluster i: 85 -> 261
size cluster i: 86 -> 302
size cluster i: 87 -> 3
size cluster i: 88 -> 198
size cluster i: 89 -> 36
size cluster i: 90 -> 4
size cluster i: 91 -> 2
size cluster i: 92 -> 4
size cluster i: 93 -> 4
size cluster i: 95 -> 2
size cluster i: 96 -> 15
size cluster i: 97 -> 3
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 100 -> 2
size cluster i: 101 -> 5
size cluster i: 102 -> 3
size cluster i: 103 -> 2
size cluster i: 104 -> 3
size cluster i: 105 -> 2
size cluster i: 106 -> 2
size cluster i: 107 -> 8
size cluster i: 108 -> 7
size cluster i: 109 -> 6
size cluster i: 110 -> 4
size cluster i: 111 -> 3
size cluster i: 112 -> 2
size cluster i: 113 -> 2
size cluster i: 114 -> 2
size cluster i: 116 -> 2
size cluster i: 117 -> 9
size cluster i: 120 -> 3
size cluster i: 121 -> 2
size cluster i: 122 -> 2
size cluster i: 123 -> 2
size cluster i: 125 -> 2
size cluster i: 128 -> 3
size cluster i: 130 -> 3
size cluster i: 131 -> 2
datapoints in clusters: 750643
score: 43.4195


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:01:51 2019
elapsed time: 35.429s


Finishing: 
---------- 
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7: Exiting... 
Node 8: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:249687] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:249687] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 141
datapoints_clusters: 750643
score: 43.4195
elapsed_time: 35.429

--------------- END RUN -----------------
attempt lambda: 1e-05 threshold: 462.962962963 percent_correct: 0.745586274509804
thresholds:[0.0, 166.66666666666666, 333.33333333333331, 500.0, 666.66666666666663, 833.33333333333326, 1000.0, 1166.6666666666665, 1333.3333333333333, 1500.0], threshold_step:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=0.0001 --k 6 --epsilon 0.001 --threshold 0.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[64300,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 0.0001
k: 6
threshold: 0


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2858s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0178424
delta: 0.00424417
delta: 0.000638236
delta: 4.81329e-05
delta: 3.83537e-06
delta: 2.42143e-06
delta: 1.79012e-07
delta: 7.3494e-09
Number of iterations: 8 (max. 1000)
Final norm of residuum: 7.3494e-09
counted_mult_calls: 9
solver duration: 7.34608s
Number of grid points:     397825
alpha min: -10.3024 max: 33.7619
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.88591s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37747
find clusters duration: 63.8956s
detected clusters: 1
size cluster i: 0 -> 1020000
datapoints in clusters: 1020000
score: 1


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:03:34 2019
elapsed time: 93.7562s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 7: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 3: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:250005] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:250005] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 1
datapoints_clusters: 1020000
score: 1.0
elapsed_time: 93.7562

--------------- END RUN -----------------
attempt lambda: 0.0001 threshold: 0.0 percent_correct: 0.00980392156862745
-> new best_percent_correct:0.00980392156862745new best_threshold:0.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=0.0001 --k 6 --epsilon 0.001 --threshold 166.66666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[64111,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 0.0001
k: 6
threshold: 166.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1263s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0178424
delta: 0.00424417
delta: 0.000638236
delta: 4.81329e-05
delta: 3.83537e-06
delta: 2.42143e-06
delta: 1.79012e-07
delta: 7.3494e-09
Number of iterations: 8 (max. 1000)
Final norm of residuum: 7.3494e-09
counted_mult_calls: 9
solver duration: 7.35372s
Number of grid points:     397825
alpha min: -10.3024 max: 33.7619
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.89208s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 17715
find clusters duration: 0.12196s
detected clusters: 87
size cluster i: 0 -> 7257
size cluster i: 1 -> 9983
size cluster i: 2 -> 10001
size cluster i: 3 -> 9999
size cluster i: 4 -> 9622
size cluster i: 5 -> 8409
size cluster i: 6 -> 10000
size cluster i: 7 -> 9375
size cluster i: 8 -> 8980
size cluster i: 9 -> 9820
size cluster i: 10 -> 9984
size cluster i: 11 -> 8674
size cluster i: 12 -> 10000
size cluster i: 13 -> 9853
size cluster i: 14 -> 9977
size cluster i: 15 -> 9196
size cluster i: 16 -> 9999
size cluster i: 17 -> 9871
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 9986
size cluster i: 21 -> 9992
size cluster i: 22 -> 9999
size cluster i: 23 -> 19812
size cluster i: 24 -> 10000
size cluster i: 25 -> 10001
size cluster i: 26 -> 9404
size cluster i: 27 -> 10000
size cluster i: 28 -> 3233
size cluster i: 29 -> 3952
size cluster i: 30 -> 3864
size cluster i: 31 -> 10000
size cluster i: 32 -> 8131
size cluster i: 33 -> 10000
size cluster i: 34 -> 9998
size cluster i: 35 -> 10000
size cluster i: 36 -> 10000
size cluster i: 37 -> 3721
size cluster i: 38 -> 9997
size cluster i: 39 -> 8200
size cluster i: 40 -> 9740
size cluster i: 41 -> 10001
size cluster i: 42 -> 3913
size cluster i: 43 -> 3809
size cluster i: 44 -> 10000
size cluster i: 45 -> 4200
size cluster i: 46 -> 2705
size cluster i: 47 -> 6756
size cluster i: 48 -> 6565
size cluster i: 49 -> 267
size cluster i: 50 -> 550
size cluster i: 51 -> 71
size cluster i: 52 -> 376
size cluster i: 53 -> 7
size cluster i: 54 -> 2
size cluster i: 55 -> 5
size cluster i: 56 -> 4
size cluster i: 57 -> 3
size cluster i: 58 -> 2
size cluster i: 60 -> 3
size cluster i: 62 -> 2
size cluster i: 63 -> 2
size cluster i: 64 -> 2
size cluster i: 65 -> 2
size cluster i: 66 -> 2
size cluster i: 67 -> 3
size cluster i: 69 -> 2
size cluster i: 70 -> 4
size cluster i: 71 -> 2
size cluster i: 72 -> 3
size cluster i: 74 -> 2
size cluster i: 76 -> 2
size cluster i: 79 -> 3
size cluster i: 80 -> 2
size cluster i: 81 -> 2
datapoints in clusters: 430316
score: 36.7034


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:04:15 2019
elapsed time: 29.8594s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Node 2: Exiting... 
Cleanup done!
Node 1: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node Warning: Node Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!


[argon-gtx:250326] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:250326] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 87
datapoints_clusters: 430316
score: 36.7034
elapsed_time: 29.8594

--------------- END RUN -----------------
datapoints_clusters too low
thresholds:[  18.51851852   37.03703704   55.55555556   74.07407407   92.59259259
  111.11111111  129.62962963  148.14814815], threshold_step:18.5185185185

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=0.0001 --k 6 --epsilon 0.001 --threshold 18.51851851851852 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[63670,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 0.0001
k: 6
threshold: 18.5185


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2908s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0178424
delta: 0.00424417
delta: 0.000638236
delta: 4.81329e-05
delta: 3.83537e-06
delta: 2.42143e-06
delta: 1.79012e-07
delta: 7.3494e-09
Number of iterations: 8 (max. 1000)
Final norm of residuum: 7.3494e-09
counted_mult_calls: 9
solver duration: 7.30119s
Number of grid points:     397825
alpha min: -10.3024 max: 33.7619
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.90031s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39162
find clusters duration: 60.3849s
detected clusters: 31
size cluster i: 0 -> 985890
size cluster i: 1 -> 9533
size cluster i: 2 -> 3
size cluster i: 4 -> 2
size cluster i: 5 -> 3
size cluster i: 6 -> 3
size cluster i: 7 -> 2
size cluster i: 9 -> 4
size cluster i: 10 -> 2
size cluster i: 11 -> 2
size cluster i: 12 -> 2
size cluster i: 13 -> 3
size cluster i: 14 -> 3
size cluster i: 16 -> 2
size cluster i: 17 -> 2
size cluster i: 21 -> 2
size cluster i: 25 -> 2
datapoints in clusters: 995474
score: 30.2546


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:05:48 2019
elapsed time: 90.2017s


Finishing: 
---------- 
Beginning cleanup...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 2: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:250639] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:250639] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with theWarning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
value given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 31
datapoints_clusters: 995474
score: 30.2546
elapsed_time: 90.2017

--------------- END RUN -----------------
attempt lambda: 0.0001 threshold: 18.5185185185 percent_correct: 0.03420980392156863
-> new best_percent_correct:0.03420980392156863new best_threshold:18.5185185185

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=0.0001 --k 6 --epsilon 0.001 --threshold 37.03703703703704 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[65498,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 0.0001
k: 6
threshold: 37.037


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1166s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0178424
delta: 0.00424417
delta: 0.000638236
delta: 4.81329e-05
delta: 3.83537e-06
delta: 2.42143e-06
delta: 1.79012e-07
delta: 7.3494e-09
Number of iterations: 8 (max. 1000)
Final norm of residuum: 7.3494e-09
counted_mult_calls: 9
solver duration: 7.33193s
Number of grid points:     397825
alpha min: -10.3024 max: 33.7619
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.80828s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 36937
find clusters duration: 30.9282s
detected clusters: 70
size cluster i: 0 -> 801537
size cluster i: 1 -> 10009
size cluster i: 2 -> 10004
size cluster i: 3 -> 10006
size cluster i: 4 -> 10000
size cluster i: 5 -> 10013
size cluster i: 6 -> 10004
size cluster i: 7 -> 2409
size cluster i: 8 -> 9993
size cluster i: 9 -> 10001
size cluster i: 10 -> 10002
size cluster i: 11 -> 10003
size cluster i: 12 -> 9972
size cluster i: 13 -> 3834
size cluster i: 14 -> 4798
size cluster i: 15 -> 4779
size cluster i: 16 -> 749
size cluster i: 17 -> 902
size cluster i: 18 -> 607
size cluster i: 19 -> 3
size cluster i: 20 -> 2
size cluster i: 22 -> 2
size cluster i: 23 -> 3
size cluster i: 24 -> 2
size cluster i: 25 -> 4
size cluster i: 28 -> 2
size cluster i: 30 -> 2
size cluster i: 32 -> 3
size cluster i: 33 -> 4
size cluster i: 34 -> 4
size cluster i: 36 -> 3
size cluster i: 39 -> 4
size cluster i: 40 -> 2
size cluster i: 42 -> 2
size cluster i: 44 -> 2
size cluster i: 45 -> 2
size cluster i: 47 -> 5
size cluster i: 49 -> 2
size cluster i: 50 -> 2
size cluster i: 51 -> 2
size cluster i: 53 -> 2
size cluster i: 55 -> 4
size cluster i: 56 -> 2
size cluster i: 61 -> 2
size cluster i: 63 -> 2
datapoints in clusters: 929716
score: 63.804


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:07:00 2019
elapsed time: 60.5114s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 1 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 4: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 5: Exiting... 
Node 8: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:250979] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:250979] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 70
datapoints_clusters: 929716
score: 63.804
elapsed_time: 60.5114

--------------- END RUN -----------------
attempt lambda: 0.0001 threshold: 37.037037037 percent_correct: 0.15339411764705882
-> new best_percent_correct:0.15339411764705882new best_threshold:37.037037037

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=0.0001 --k 6 --epsilon 0.001 --threshold 55.55555555555556 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[65060,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 0.0001
k: 6
threshold: 55.5556


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1896s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0178424
delta: 0.00424417
delta: 0.000638236
delta: 4.81329e-05
delta: 3.83537e-06
delta: 2.42143e-06
delta: 1.79012e-07
delta: 7.3494e-09
Number of iterations: 8 (max. 1000)
Final norm of residuum: 7.3494e-09
counted_mult_calls: 9
solver duration: 7.36572s
Number of grid points:     397825
alpha min: -10.3024 max: 33.7619
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.85946s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 34957
find clusters duration: 6.17904s
detected clusters: 69
size cluster i: 0 -> 507195
size cluster i: 1 -> 19680
size cluster i: 2 -> 10000
size cluster i: 3 -> 10001
size cluster i: 4 -> 9795
size cluster i: 5 -> 9999
size cluster i: 6 -> 10001
size cluster i: 7 -> 10008
size cluster i: 8 -> 10013
size cluster i: 9 -> 10006
size cluster i: 10 -> 10003
size cluster i: 11 -> 9997
size cluster i: 12 -> 9008
size cluster i: 13 -> 9086
size cluster i: 14 -> 10005
size cluster i: 15 -> 19973
size cluster i: 16 -> 10002
size cluster i: 17 -> 10001
size cluster i: 18 -> 9896
size cluster i: 19 -> 10012
size cluster i: 20 -> 10002
size cluster i: 21 -> 20005
size cluster i: 22 -> 10000
size cluster i: 23 -> 10002
size cluster i: 24 -> 9439
size cluster i: 25 -> 8931
size cluster i: 26 -> 10004
size cluster i: 27 -> 10001
size cluster i: 28 -> 9499
size cluster i: 29 -> 9999
size cluster i: 30 -> 8113
size cluster i: 31 -> 7251
size cluster i: 32 -> 10005
size cluster i: 33 -> 9251
size cluster i: 34 -> 8897
size cluster i: 35 -> 7835
size cluster i: 36 -> 10000
size cluster i: 37 -> 239
size cluster i: 38 -> 92
size cluster i: 39 -> 5
size cluster i: 40 -> 3
size cluster i: 41 -> 2
size cluster i: 42 -> 3
size cluster i: 43 -> 2
size cluster i: 44 -> 3
size cluster i: 45 -> 6
size cluster i: 46 -> 4
size cluster i: 47 -> 7
size cluster i: 49 -> 2
size cluster i: 51 -> 2
size cluster i: 52 -> 5
size cluster i: 53 -> 3
size cluster i: 54 -> 2
size cluster i: 55 -> 2
size cluster i: 56 -> 2
size cluster i: 57 -> 3
size cluster i: 59 -> 2
size cluster i: 60 -> 2
size cluster i: 61 -> 2
size cluster i: 62 -> 2
size cluster i: 63 -> 2
size cluster i: 67 -> 2
datapoints in clusters: 884321
score: 59.8217


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:07:46 2019
elapsed time: 35.9402s


Finishing: 
---------- 
Beginning cleanup...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node Warning: Node Warning: Node Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:251293] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:251293] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 69
datapoints_clusters: 884321
score: 59.8217
elapsed_time: 35.9402

--------------- END RUN -----------------
attempt lambda: 0.0001 threshold: 55.5555555556 percent_correct: 0.36931764705882353
-> new best_percent_correct:0.36931764705882353new best_threshold:55.5555555556

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=0.0001 --k 6 --epsilon 0.001 --threshold 74.07407407407408 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[64886,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 0.0001
k: 6
threshold: 74.0741


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2242s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0178424
delta: 0.00424417
delta: 0.000638236
delta: 4.81329e-05
delta: 3.83537e-06
delta: 2.42143e-06
delta: 1.79012e-07
delta: 7.3494e-09
Number of iterations: 8 (max. 1000)
Final norm of residuum: 7.3494e-09
counted_mult_calls: 9
solver duration: 7.3591s
Number of grid points:     397825
alpha min: -10.3024 max: 33.7619
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.78178s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 32520
find clusters duration: 0.696846s
detected clusters: 76
size cluster i: 0 -> 20011
size cluster i: 1 -> 6553
size cluster i: 2 -> 21438
size cluster i: 3 -> 331460
size cluster i: 4 -> 9977
size cluster i: 5 -> 9946
size cluster i: 6 -> 7469
size cluster i: 7 -> 9285
size cluster i: 8 -> 9842
size cluster i: 9 -> 9949
size cluster i: 10 -> 10002
size cluster i: 11 -> 10006
size cluster i: 12 -> 9999
size cluster i: 13 -> 10004
size cluster i: 14 -> 10001
size cluster i: 15 -> 9483
size cluster i: 16 -> 4638
size cluster i: 17 -> 10002
size cluster i: 18 -> 10001
size cluster i: 19 -> 9840
size cluster i: 20 -> 10004
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10001
size cluster i: 24 -> 10000
size cluster i: 25 -> 8246
size cluster i: 26 -> 10003
size cluster i: 27 -> 10000
size cluster i: 28 -> 9474
size cluster i: 29 -> 4734
size cluster i: 30 -> 10000
size cluster i: 31 -> 10002
size cluster i: 32 -> 19963
size cluster i: 33 -> 9880
size cluster i: 34 -> 10000
size cluster i: 35 -> 5496
size cluster i: 36 -> 10003
size cluster i: 37 -> 9968
size cluster i: 38 -> 5717
size cluster i: 39 -> 9227
size cluster i: 40 -> 10002
size cluster i: 41 -> 4984
size cluster i: 42 -> 10002
size cluster i: 43 -> 8623
size cluster i: 44 -> 9934
size cluster i: 45 -> 5626
size cluster i: 46 -> 3398
size cluster i: 47 -> 10000
size cluster i: 48 -> 4760
size cluster i: 49 -> 1714
size cluster i: 50 -> 4182
size cluster i: 51 -> 2197
size cluster i: 52 -> 2369
size cluster i: 53 -> 129
size cluster i: 54 -> 4
size cluster i: 55 -> 2
size cluster i: 56 -> 2
size cluster i: 57 -> 2
size cluster i: 58 -> 3
size cluster i: 59 -> 5
size cluster i: 60 -> 3
size cluster i: 61 -> 5
size cluster i: 62 -> 2
size cluster i: 63 -> 2
size cluster i: 69 -> 2
size cluster i: 70 -> 2
size cluster i: 71 -> 2
size cluster i: 72 -> 2
datapoints in clusters: 800592
score: 59.652


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:08:26 2019
elapsed time: 30.411s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Node 3: Exiting... 
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:251599] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:251599] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 76
datapoints_clusters: 800592
score: 59.652
elapsed_time: 30.411

--------------- END RUN -----------------
attempt lambda: 0.0001 threshold: 74.0740740741 percent_correct: 0.45818333333333333
-> new best_percent_correct:0.45818333333333333new best_threshold:74.0740740741

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=0.0001 --k 6 --epsilon 0.001 --threshold 92.5925925925926 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[62372,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 0.0001
k: 6
threshold: 92.5926


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.137s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0178424
delta: 0.00424417
delta: 0.000638236
delta: 4.81329e-05
delta: 3.83537e-06
delta: 2.42143e-06
delta: 1.79012e-07
delta: 7.3494e-09
Number of iterations: 8 (max. 1000)
Final norm of residuum: 7.3494e-09
counted_mult_calls: 9
solver duration: 7.33075s
Number of grid points:     397825
alpha min: -10.3024 max: 33.7619
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.91678s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 28810
find clusters duration: 0.21935s
detected clusters: 125
size cluster i: 0 -> 10004
size cluster i: 1 -> 10000
size cluster i: 2 -> 10007
size cluster i: 3 -> 10000
size cluster i: 4 -> 9085
size cluster i: 5 -> 9022
size cluster i: 6 -> 3255
size cluster i: 7 -> 5659
size cluster i: 8 -> 10003
size cluster i: 9 -> 40020
size cluster i: 10 -> 8840
size cluster i: 11 -> 10002
size cluster i: 12 -> 8235
size cluster i: 13 -> 10003
size cluster i: 14 -> 28599
size cluster i: 15 -> 10000
size cluster i: 16 -> 69255
size cluster i: 17 -> 9890
size cluster i: 18 -> 30015
size cluster i: 19 -> 10003
size cluster i: 20 -> 10001
size cluster i: 21 -> 9992
size cluster i: 22 -> 10001
size cluster i: 23 -> 10000
size cluster i: 24 -> 9433
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10002
size cluster i: 28 -> 10000
size cluster i: 29 -> 12414
size cluster i: 30 -> 10001
size cluster i: 31 -> 9998
size cluster i: 32 -> 9997
size cluster i: 33 -> 10000
size cluster i: 34 -> 4279
size cluster i: 35 -> 10003
size cluster i: 36 -> 10000
size cluster i: 37 -> 9720
size cluster i: 38 -> 10001
size cluster i: 39 -> 18942
size cluster i: 40 -> 10000
size cluster i: 41 -> 10002
size cluster i: 42 -> 20010
size cluster i: 43 -> 20006
size cluster i: 44 -> 8676
size cluster i: 45 -> 10001
size cluster i: 46 -> 6560
size cluster i: 47 -> 9337
size cluster i: 48 -> 5157
size cluster i: 49 -> 10000
size cluster i: 50 -> 8409
size cluster i: 51 -> 10001
size cluster i: 52 -> 4629
size cluster i: 53 -> 10005
size cluster i: 54 -> 8414
size cluster i: 55 -> 6863
size cluster i: 56 -> 10001
size cluster i: 57 -> 1828
size cluster i: 58 -> 9970
size cluster i: 59 -> 1057
size cluster i: 60 -> 1299
size cluster i: 61 -> 1176
size cluster i: 62 -> 1060
size cluster i: 63 -> 1628
size cluster i: 64 -> 703
size cluster i: 65 -> 16
size cluster i: 66 -> 890
size cluster i: 67 -> 780
size cluster i: 68 -> 299
size cluster i: 69 -> 80
size cluster i: 70 -> 3
size cluster i: 71 -> 5
size cluster i: 72 -> 4
size cluster i: 73 -> 9
size cluster i: 74 -> 2
size cluster i: 75 -> 4
size cluster i: 76 -> 18
size cluster i: 77 -> 2
size cluster i: 78 -> 4
size cluster i: 79 -> 3
size cluster i: 80 -> 8
size cluster i: 81 -> 65
size cluster i: 82 -> 2
size cluster i: 83 -> 2
size cluster i: 85 -> 6
size cluster i: 86 -> 4
size cluster i: 87 -> 4
size cluster i: 88 -> 2
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 3
size cluster i: 93 -> 3
size cluster i: 94 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 3
size cluster i: 97 -> 2
size cluster i: 98 -> 3
size cluster i: 99 -> 11
size cluster i: 100 -> 4
size cluster i: 105 -> 2
size cluster i: 109 -> 2
size cluster i: 110 -> 2
size cluster i: 111 -> 2
size cluster i: 114 -> 2
size cluster i: 115 -> 2
size cluster i: 117 -> 2
size cluster i: 119 -> 2
datapoints in clusters: 705757
score: 51.8939


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:09:05 2019
elapsed time: 29.9539s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 1: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:251933] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:251933] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 125
datapoints_clusters: 705757
score: 51.8939
elapsed_time: 29.9539

--------------- END RUN -----------------
datapoints_clusters too low
lambda_factor: 3.1622776601683795 lambda_start_lower:1e-08 overall_best_lambda_value: 1e-07
lambda_values:[3.16227766016838e-08, 1.0000000000000002e-07, 3.16227766016838e-07]
thresholds:[0.0, 166.66666666666666, 333.33333333333331, 500.0, 666.66666666666663, 833.33333333333326, 1000.0, 1166.6666666666665, 1333.3333333333333, 1500.0], threshold_step:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 0.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[62191,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 0


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.118s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 134.606s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.03587s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37747
find clusters duration: 64.5629s
detected clusters: 1
size cluster i: 0 -> 1020000
datapoints in clusters: 1020000
score: 1


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:12:50 2019
elapsed time: 221.653s


Finishing: 
---------- 
Beginning cleanup...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:252246] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:252246] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 1
datapoints_clusters: 1020000
score: 1.0
elapsed_time: 221.653

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 0.0 percent_correct: 0.00980392156862745
-> new best_percent_correct:0.00980392156862745new best_threshold:0.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 166.66666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[61701,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 166.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1509s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 134.74s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.98993s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39487
find clusters duration: 4.98243s
detected clusters: 490
size cluster i: 0 -> 581137
size cluster i: 1 -> 10015
size cluster i: 2 -> 30042
size cluster i: 3 -> 10027
size cluster i: 4 -> 10016
size cluster i: 5 -> 10013
size cluster i: 6 -> 10018
size cluster i: 7 -> 10026
size cluster i: 8 -> 10006
size cluster i: 9 -> 40073
size cluster i: 10 -> 10007
size cluster i: 11 -> 10009
size cluster i: 12 -> 10014
size cluster i: 13 -> 10023
size cluster i: 14 -> 10017
size cluster i: 15 -> 10031
size cluster i: 16 -> 10026
size cluster i: 17 -> 20034
size cluster i: 18 -> 30070
size cluster i: 19 -> 20033
size cluster i: 20 -> 10016
size cluster i: 21 -> 10018
size cluster i: 22 -> 10012
size cluster i: 23 -> 10008
size cluster i: 24 -> 10015
size cluster i: 25 -> 10011
size cluster i: 26 -> 10017
size cluster i: 27 -> 10030
size cluster i: 28 -> 10022
size cluster i: 29 -> 10027
size cluster i: 30 -> 10010
size cluster i: 31 -> 10003
size cluster i: 32 -> 10005
size cluster i: 33 -> 10011
size cluster i: 34 -> 3
size cluster i: 35 -> 2
size cluster i: 36 -> 2
size cluster i: 37 -> 5
size cluster i: 38 -> 2
size cluster i: 39 -> 3
size cluster i: 40 -> 2
size cluster i: 41 -> 2
size cluster i: 42 -> 3
size cluster i: 43 -> 4
size cluster i: 44 -> 3
size cluster i: 45 -> 2
size cluster i: 46 -> 3
size cluster i: 47 -> 4
size cluster i: 48 -> 4
size cluster i: 49 -> 3
size cluster i: 50 -> 3
size cluster i: 51 -> 2
size cluster i: 52 -> 2
size cluster i: 53 -> 4
size cluster i: 54 -> 3
size cluster i: 55 -> 3
size cluster i: 56 -> 4
size cluster i: 57 -> 2
size cluster i: 58 -> 2
size cluster i: 59 -> 2
size cluster i: 60 -> 18
size cluster i: 61 -> 2
size cluster i: 62 -> 3
size cluster i: 63 -> 2
size cluster i: 64 -> 6
size cluster i: 65 -> 6
size cluster i: 66 -> 2
size cluster i: 67 -> 5
size cluster i: 68 -> 3
size cluster i: 69 -> 10
size cluster i: 70 -> 5
size cluster i: 71 -> 2
size cluster i: 72 -> 2
size cluster i: 73 -> 2
size cluster i: 74 -> 5
size cluster i: 75 -> 4
size cluster i: 76 -> 3
size cluster i: 77 -> 2
size cluster i: 78 -> 3
size cluster i: 79 -> 4
size cluster i: 80 -> 4
size cluster i: 81 -> 2
size cluster i: 82 -> 2
size cluster i: 83 -> 2
size cluster i: 84 -> 6
size cluster i: 85 -> 6
size cluster i: 86 -> 3
size cluster i: 87 -> 11
size cluster i: 88 -> 5
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 6
size cluster i: 92 -> 2
size cluster i: 93 -> 3
size cluster i: 94 -> 10
size cluster i: 95 -> 3
size cluster i: 96 -> 3
size cluster i: 97 -> 3
size cluster i: 98 -> 2
size cluster i: 99 -> 3
size cluster i: 100 -> 4
size cluster i: 101 -> 4
size cluster i: 102 -> 3
size cluster i: 103 -> 6
size cluster i: 105 -> 5
size cluster i: 106 -> 3
size cluster i: 107 -> 6
size cluster i: 108 -> 3
size cluster i: 109 -> 3
size cluster i: 110 -> 2
size cluster i: 111 -> 3
size cluster i: 112 -> 2
size cluster i: 113 -> 2
size cluster i: 114 -> 2
size cluster i: 115 -> 2
size cluster i: 116 -> 2
size cluster i: 117 -> 2
size cluster i: 118 -> 2
size cluster i: 119 -> 2
size cluster i: 120 -> 2
size cluster i: 122 -> 2
size cluster i: 123 -> 5
size cluster i: 124 -> 2
size cluster i: 125 -> 3
size cluster i: 126 -> 2
size cluster i: 127 -> 3
size cluster i: 128 -> 3
size cluster i: 129 -> 4
size cluster i: 130 -> 4
size cluster i: 131 -> 5
size cluster i: 132 -> 4
size cluster i: 133 -> 4
size cluster i: 134 -> 3
size cluster i: 135 -> 3
size cluster i: 136 -> 10
size cluster i: 137 -> 2
size cluster i: 138 -> 4
size cluster i: 139 -> 3
size cluster i: 140 -> 2
size cluster i: 141 -> 2
size cluster i: 142 -> 2
size cluster i: 143 -> 4
size cluster i: 144 -> 5
size cluster i: 145 -> 2
size cluster i: 146 -> 2
size cluster i: 147 -> 3
size cluster i: 148 -> 3
size cluster i: 149 -> 5
size cluster i: 150 -> 3
size cluster i: 151 -> 3
size cluster i: 152 -> 3
size cluster i: 153 -> 4
size cluster i: 154 -> 2
size cluster i: 155 -> 4
size cluster i: 156 -> 2
size cluster i: 158 -> 3
size cluster i: 159 -> 3
size cluster i: 161 -> 2
size cluster i: 162 -> 3
size cluster i: 163 -> 2
size cluster i: 164 -> 2
size cluster i: 165 -> 4
size cluster i: 166 -> 3
size cluster i: 167 -> 4
size cluster i: 168 -> 3
size cluster i: 169 -> 2
size cluster i: 170 -> 2
size cluster i: 171 -> 2
size cluster i: 172 -> 4
size cluster i: 173 -> 6
size cluster i: 174 -> 5
size cluster i: 175 -> 3
size cluster i: 176 -> 3
size cluster i: 177 -> 2
size cluster i: 179 -> 3
size cluster i: 180 -> 2
size cluster i: 181 -> 5
size cluster i: 182 -> 3
size cluster i: 183 -> 4
size cluster i: 185 -> 4
size cluster i: 186 -> 3
size cluster i: 187 -> 4
size cluster i: 188 -> 4
size cluster i: 189 -> 4
size cluster i: 190 -> 4
size cluster i: 191 -> 2
size cluster i: 192 -> 2
size cluster i: 194 -> 2
size cluster i: 195 -> 2
size cluster i: 196 -> 2
size cluster i: 197 -> 2
size cluster i: 198 -> 2
size cluster i: 199 -> 4
size cluster i: 200 -> 2
size cluster i: 201 -> 2
size cluster i: 202 -> 2
size cluster i: 203 -> 3
size cluster i: 204 -> 3
size cluster i: 205 -> 4
size cluster i: 206 -> 2
size cluster i: 208 -> 3
size cluster i: 209 -> 2
size cluster i: 210 -> 2
size cluster i: 211 -> 2
size cluster i: 212 -> 3
size cluster i: 213 -> 2
size cluster i: 214 -> 4
size cluster i: 215 -> 4
size cluster i: 216 -> 3
size cluster i: 217 -> 2
size cluster i: 218 -> 2
size cluster i: 219 -> 4
size cluster i: 220 -> 2
size cluster i: 221 -> 3
size cluster i: 222 -> 3
size cluster i: 223 -> 3
size cluster i: 224 -> 3
size cluster i: 225 -> 2
size cluster i: 226 -> 2
size cluster i: 227 -> 2
size cluster i: 229 -> 2
size cluster i: 230 -> 2
size cluster i: 231 -> 3
size cluster i: 232 -> 2
size cluster i: 233 -> 3
size cluster i: 234 -> 5
size cluster i: 235 -> 2
size cluster i: 236 -> 2
size cluster i: 237 -> 3
size cluster i: 238 -> 2
size cluster i: 239 -> 2
size cluster i: 240 -> 5
size cluster i: 241 -> 4
size cluster i: 242 -> 2
size cluster i: 244 -> 2
size cluster i: 246 -> 3
size cluster i: 247 -> 2
size cluster i: 249 -> 3
size cluster i: 250 -> 2
size cluster i: 253 -> 2
size cluster i: 254 -> 2
size cluster i: 255 -> 2
size cluster i: 256 -> 2
size cluster i: 258 -> 2
size cluster i: 260 -> 3
size cluster i: 264 -> 2
size cluster i: 266 -> 3
size cluster i: 267 -> 3
size cluster i: 268 -> 2
size cluster i: 269 -> 2
size cluster i: 270 -> 4
size cluster i: 271 -> 2
size cluster i: 272 -> 3
size cluster i: 273 -> 2
size cluster i: 274 -> 2
size cluster i: 275 -> 2
size cluster i: 276 -> 2
size cluster i: 278 -> 2
size cluster i: 279 -> 2
size cluster i: 280 -> 2
size cluster i: 281 -> 2
size cluster i: 282 -> 2
size cluster i: 283 -> 6
size cluster i: 284 -> 2
size cluster i: 285 -> 2
size cluster i: 286 -> 3
size cluster i: 287 -> 2
size cluster i: 288 -> 4
size cluster i: 289 -> 2
size cluster i: 290 -> 2
size cluster i: 291 -> 5
size cluster i: 292 -> 2
size cluster i: 293 -> 4
size cluster i: 294 -> 2
size cluster i: 295 -> 2
size cluster i: 296 -> 2
size cluster i: 299 -> 2
size cluster i: 300 -> 2
size cluster i: 301 -> 2
size cluster i: 302 -> 2
size cluster i: 303 -> 3
size cluster i: 304 -> 3
size cluster i: 306 -> 3
size cluster i: 307 -> 3
size cluster i: 311 -> 3
size cluster i: 312 -> 6
size cluster i: 313 -> 2
size cluster i: 315 -> 3
size cluster i: 316 -> 3
size cluster i: 317 -> 4
size cluster i: 318 -> 3
size cluster i: 319 -> 2
size cluster i: 321 -> 2
size cluster i: 322 -> 3
size cluster i: 323 -> 4
size cluster i: 326 -> 2
size cluster i: 327 -> 3
size cluster i: 328 -> 2
size cluster i: 329 -> 2
size cluster i: 331 -> 2
size cluster i: 332 -> 4
size cluster i: 334 -> 2
size cluster i: 335 -> 2
size cluster i: 336 -> 3
size cluster i: 338 -> 2
size cluster i: 339 -> 2
size cluster i: 340 -> 2
size cluster i: 342 -> 4
size cluster i: 343 -> 5
size cluster i: 344 -> 2
size cluster i: 345 -> 2
size cluster i: 346 -> 2
size cluster i: 347 -> 3
size cluster i: 348 -> 3
size cluster i: 349 -> 2
size cluster i: 351 -> 2
size cluster i: 352 -> 2
size cluster i: 353 -> 3
size cluster i: 354 -> 2
size cluster i: 356 -> 2
size cluster i: 358 -> 2
size cluster i: 359 -> 2
size cluster i: 362 -> 3
size cluster i: 363 -> 2
size cluster i: 365 -> 3
size cluster i: 366 -> 3
size cluster i: 367 -> 2
size cluster i: 368 -> 2
size cluster i: 370 -> 4
size cluster i: 373 -> 2
size cluster i: 376 -> 3
size cluster i: 377 -> 2
size cluster i: 378 -> 2
size cluster i: 379 -> 2
size cluster i: 380 -> 2
size cluster i: 381 -> 2
size cluster i: 382 -> 2
size cluster i: 383 -> 2
size cluster i: 385 -> 2
size cluster i: 388 -> 2
size cluster i: 389 -> 2
size cluster i: 390 -> 3
size cluster i: 391 -> 4
size cluster i: 392 -> 5
size cluster i: 394 -> 2
size cluster i: 395 -> 2
size cluster i: 397 -> 2
size cluster i: 398 -> 2
size cluster i: 400 -> 2
size cluster i: 402 -> 3
size cluster i: 405 -> 2
size cluster i: 407 -> 2
size cluster i: 410 -> 2
size cluster i: 413 -> 4
size cluster i: 415 -> 2
size cluster i: 417 -> 2
size cluster i: 420 -> 2
size cluster i: 421 -> 2
size cluster i: 423 -> 2
size cluster i: 424 -> 2
size cluster i: 428 -> 2
size cluster i: 430 -> 2
size cluster i: 433 -> 2
size cluster i: 444 -> 2
size cluster i: 448 -> 2
size cluster i: 455 -> 2
size cluster i: 456 -> 2
size cluster i: 465 -> 2
size cluster i: 468 -> 2
size cluster i: 480 -> 2
size cluster i: 481 -> 2
size cluster i: 486 -> 2
datapoints in clusters: 1002946
score: 0


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:15:43 2019
elapsed time: 162.205s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 5: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:252604] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:252604] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 490
datapoints_clusters: 1002946
score: 0.0
elapsed_time: 162.205

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 166.666666667 percent_correct: 0.35005294117647057
-> new best_percent_correct:0.35005294117647057new best_threshold:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[63423,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 333.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0976s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 135.018s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.0543s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39086
find clusters duration: 0.328139s
detected clusters: 226
size cluster i: 0 -> 60063
size cluster i: 1 -> 10009
size cluster i: 2 -> 20021
size cluster i: 3 -> 20025
size cluster i: 4 -> 10002
size cluster i: 5 -> 10008
size cluster i: 6 -> 20020
size cluster i: 7 -> 10013
size cluster i: 8 -> 10007
size cluster i: 9 -> 10010
size cluster i: 10 -> 10009
size cluster i: 11 -> 10008
size cluster i: 12 -> 40030
size cluster i: 13 -> 10009
size cluster i: 14 -> 10006
size cluster i: 15 -> 10001
size cluster i: 16 -> 10010
size cluster i: 17 -> 40035
size cluster i: 18 -> 10012
size cluster i: 19 -> 10003
size cluster i: 20 -> 10004
size cluster i: 21 -> 20026
size cluster i: 22 -> 10005
size cluster i: 23 -> 10007
size cluster i: 24 -> 10017
size cluster i: 25 -> 10006
size cluster i: 26 -> 10009
size cluster i: 27 -> 10006
size cluster i: 28 -> 20023
size cluster i: 29 -> 10009
size cluster i: 30 -> 10009
size cluster i: 31 -> 10004
size cluster i: 32 -> 30032
size cluster i: 33 -> 20014
size cluster i: 34 -> 10009
size cluster i: 35 -> 20019
size cluster i: 36 -> 10003
size cluster i: 37 -> 10007
size cluster i: 38 -> 10008
size cluster i: 39 -> 10005
size cluster i: 40 -> 10003
size cluster i: 41 -> 10007
size cluster i: 42 -> 20016
size cluster i: 43 -> 10005
size cluster i: 44 -> 10010
size cluster i: 45 -> 40054
size cluster i: 46 -> 10007
size cluster i: 47 -> 10007
size cluster i: 48 -> 10006
size cluster i: 49 -> 10009
size cluster i: 50 -> 10008
size cluster i: 51 -> 10009
size cluster i: 52 -> 10008
size cluster i: 53 -> 10002
size cluster i: 54 -> 10008
size cluster i: 55 -> 10007
size cluster i: 56 -> 10010
size cluster i: 57 -> 10010
size cluster i: 58 -> 10006
size cluster i: 59 -> 10012
size cluster i: 60 -> 10014
size cluster i: 61 -> 10002
size cluster i: 62 -> 10021
size cluster i: 63 -> 10012
size cluster i: 64 -> 10006
size cluster i: 65 -> 10007
size cluster i: 66 -> 10003
size cluster i: 67 -> 10000
size cluster i: 68 -> 10008
size cluster i: 69 -> 10010
size cluster i: 70 -> 10014
size cluster i: 71 -> 10002
size cluster i: 72 -> 10006
size cluster i: 73 -> 10005
size cluster i: 74 -> 10004
size cluster i: 75 -> 10011
size cluster i: 76 -> 2
size cluster i: 77 -> 2
size cluster i: 78 -> 2
size cluster i: 79 -> 4
size cluster i: 80 -> 2
size cluster i: 81 -> 3
size cluster i: 82 -> 2
size cluster i: 83 -> 2
size cluster i: 84 -> 2
size cluster i: 85 -> 2
size cluster i: 86 -> 2
size cluster i: 87 -> 2
size cluster i: 88 -> 2
size cluster i: 89 -> 2
size cluster i: 90 -> 6
size cluster i: 91 -> 2
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 100 -> 7
size cluster i: 101 -> 2
size cluster i: 102 -> 4
size cluster i: 103 -> 4
size cluster i: 104 -> 2
size cluster i: 105 -> 3
size cluster i: 106 -> 5
size cluster i: 107 -> 2
size cluster i: 108 -> 2
size cluster i: 109 -> 2
size cluster i: 110 -> 3
size cluster i: 111 -> 4
size cluster i: 112 -> 3
size cluster i: 113 -> 2
size cluster i: 114 -> 4
size cluster i: 115 -> 3
size cluster i: 116 -> 3
size cluster i: 117 -> 2
size cluster i: 118 -> 2
size cluster i: 119 -> 2
size cluster i: 120 -> 4
size cluster i: 122 -> 2
size cluster i: 123 -> 3
size cluster i: 125 -> 3
size cluster i: 126 -> 2
size cluster i: 127 -> 2
size cluster i: 128 -> 2
size cluster i: 129 -> 3
size cluster i: 130 -> 2
size cluster i: 131 -> 3
size cluster i: 133 -> 4
size cluster i: 134 -> 3
size cluster i: 136 -> 3
size cluster i: 137 -> 4
size cluster i: 138 -> 2
size cluster i: 139 -> 2
size cluster i: 140 -> 5
size cluster i: 142 -> 2
size cluster i: 144 -> 2
size cluster i: 146 -> 2
size cluster i: 147 -> 2
size cluster i: 148 -> 4
size cluster i: 152 -> 2
size cluster i: 154 -> 3
size cluster i: 155 -> 2
size cluster i: 156 -> 2
size cluster i: 158 -> 4
size cluster i: 159 -> 2
size cluster i: 160 -> 2
size cluster i: 161 -> 2
size cluster i: 162 -> 3
size cluster i: 163 -> 3
size cluster i: 164 -> 3
size cluster i: 165 -> 2
size cluster i: 166 -> 2
size cluster i: 170 -> 4
size cluster i: 173 -> 2
size cluster i: 174 -> 3
size cluster i: 176 -> 3
size cluster i: 179 -> 2
size cluster i: 182 -> 2
size cluster i: 183 -> 2
size cluster i: 187 -> 2
size cluster i: 190 -> 2
size cluster i: 192 -> 2
size cluster i: 202 -> 2
size cluster i: 204 -> 2
size cluster i: 210 -> 2
size cluster i: 220 -> 2
datapoints in clusters: 1001148
score: 0


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:18:32 2019
elapsed time: 157.831s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:252934] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:252934] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 226
datapoints_clusters: 1001148
score: 0.0
elapsed_time: 157.831

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 333.333333333 percent_correct: 0.7635803921568628
-> new best_percent_correct:0.7635803921568628new best_threshold:333.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[63183,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0384s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 134.976s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.1304s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38920
find clusters duration: 0.307035s
detected clusters: 141
size cluster i: 0 -> 50045
size cluster i: 1 -> 10004
size cluster i: 2 -> 20015
size cluster i: 3 -> 20017
size cluster i: 4 -> 10001
size cluster i: 5 -> 10004
size cluster i: 6 -> 10004
size cluster i: 7 -> 10005
size cluster i: 8 -> 10004
size cluster i: 9 -> 10007
size cluster i: 10 -> 10005
size cluster i: 11 -> 10005
size cluster i: 12 -> 30018
size cluster i: 13 -> 10006
size cluster i: 14 -> 10003
size cluster i: 15 -> 10001
size cluster i: 16 -> 10006
size cluster i: 17 -> 40029
size cluster i: 18 -> 10008
size cluster i: 19 -> 10001
size cluster i: 20 -> 10002
size cluster i: 21 -> 10006
size cluster i: 22 -> 10003
size cluster i: 23 -> 10005
size cluster i: 24 -> 10007
size cluster i: 25 -> 10005
size cluster i: 26 -> 10003
size cluster i: 27 -> 10004
size cluster i: 28 -> 20010
size cluster i: 29 -> 10008
size cluster i: 30 -> 10003
size cluster i: 31 -> 10004
size cluster i: 32 -> 10013
size cluster i: 33 -> 20011
size cluster i: 34 -> 10009
size cluster i: 35 -> 10005
size cluster i: 36 -> 10001
size cluster i: 37 -> 10005
size cluster i: 38 -> 10005
size cluster i: 39 -> 10004
size cluster i: 40 -> 10003
size cluster i: 41 -> 10005
size cluster i: 42 -> 20007
size cluster i: 43 -> 10002
size cluster i: 44 -> 10006
size cluster i: 45 -> 40038
size cluster i: 46 -> 10004
size cluster i: 47 -> 10001
size cluster i: 48 -> 10004
size cluster i: 49 -> 10002
size cluster i: 50 -> 10006
size cluster i: 51 -> 10003
size cluster i: 52 -> 10005
size cluster i: 53 -> 10006
size cluster i: 54 -> 10002
size cluster i: 55 -> 10007
size cluster i: 56 -> 10005
size cluster i: 57 -> 10004
size cluster i: 58 -> 10008
size cluster i: 59 -> 10006
size cluster i: 60 -> 20011
size cluster i: 61 -> 10007
size cluster i: 62 -> 10004
size cluster i: 63 -> 10009
size cluster i: 64 -> 10008
size cluster i: 65 -> 10002
size cluster i: 66 -> 10008
size cluster i: 67 -> 10014
size cluster i: 68 -> 10004
size cluster i: 69 -> 10005
size cluster i: 70 -> 10004
size cluster i: 71 -> 10006
size cluster i: 72 -> 10002
size cluster i: 73 -> 10000
size cluster i: 74 -> 10006
size cluster i: 75 -> 10008
size cluster i: 76 -> 10006
size cluster i: 77 -> 10001
size cluster i: 78 -> 10005
size cluster i: 79 -> 10002
size cluster i: 80 -> 10000
size cluster i: 81 -> 10003
size cluster i: 82 -> 4
size cluster i: 83 -> 2
size cluster i: 84 -> 2
size cluster i: 85 -> 2
size cluster i: 86 -> 2
size cluster i: 87 -> 2
size cluster i: 88 -> 2
size cluster i: 89 -> 4
size cluster i: 90 -> 2
size cluster i: 91 -> 3
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 2
size cluster i: 95 -> 3
size cluster i: 96 -> 2
size cluster i: 97 -> 4
size cluster i: 98 -> 3
size cluster i: 99 -> 3
size cluster i: 100 -> 3
size cluster i: 102 -> 3
size cluster i: 104 -> 2
size cluster i: 105 -> 2
size cluster i: 107 -> 2
size cluster i: 110 -> 2
size cluster i: 111 -> 2
size cluster i: 112 -> 2
size cluster i: 113 -> 2
size cluster i: 115 -> 2
size cluster i: 116 -> 2
size cluster i: 117 -> 4
size cluster i: 118 -> 2
size cluster i: 119 -> 3
size cluster i: 120 -> 2
size cluster i: 121 -> 2
size cluster i: 122 -> 2
size cluster i: 125 -> 2
size cluster i: 126 -> 2
size cluster i: 128 -> 2
size cluster i: 130 -> 2
size cluster i: 132 -> 2
size cluster i: 135 -> 2
size cluster i: 138 -> 2
datapoints in clusters: 1000656
score: 57.8811


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:21:20 2019
elapsed time: 157.823s


Finishing: 
---------- 
Beginning cleanup...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 5: Exiting... 
Node 7: Exiting... 
Node 6: Exiting... 
Node 3: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:253302] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:253302] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 141
datapoints_clusters: 1000656
score: 57.8811
elapsed_time: 157.823

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 500.0 percent_correct: 0.8228862745098039
-> new best_percent_correct:0.8228862745098039new best_threshold:500.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 666.6666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[62835,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 666.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.21s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 135.216s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.02106s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38817
find clusters duration: 0.301637s
detected clusters: 108
size cluster i: 0 -> 10005
size cluster i: 1 -> 10004
size cluster i: 2 -> 20009
size cluster i: 3 -> 20013
size cluster i: 4 -> 10001
size cluster i: 5 -> 10003
size cluster i: 6 -> 10001
size cluster i: 7 -> 10003
size cluster i: 8 -> 10004
size cluster i: 9 -> 10004
size cluster i: 10 -> 10005
size cluster i: 11 -> 10004
size cluster i: 12 -> 30017
size cluster i: 13 -> 10003
size cluster i: 14 -> 10003
size cluster i: 15 -> 10001
size cluster i: 16 -> 10006
size cluster i: 17 -> 30010
size cluster i: 18 -> 10007
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10006
size cluster i: 22 -> 10002
size cluster i: 23 -> 10004
size cluster i: 24 -> 30019
size cluster i: 25 -> 10006
size cluster i: 26 -> 10003
size cluster i: 27 -> 10003
size cluster i: 28 -> 10001
size cluster i: 29 -> 20010
size cluster i: 30 -> 10007
size cluster i: 31 -> 10003
size cluster i: 32 -> 10002
size cluster i: 33 -> 10009
size cluster i: 34 -> 10005
size cluster i: 35 -> 10004
size cluster i: 36 -> 10004
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10003
size cluster i: 40 -> 10000
size cluster i: 41 -> 10002
size cluster i: 42 -> 10004
size cluster i: 43 -> 20007
size cluster i: 44 -> 10002
size cluster i: 45 -> 10003
size cluster i: 46 -> 10005
size cluster i: 47 -> 10003
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10005
size cluster i: 52 -> 10002
size cluster i: 53 -> 10003
size cluster i: 54 -> 20018
size cluster i: 55 -> 10003
size cluster i: 56 -> 10002
size cluster i: 57 -> 10007
size cluster i: 58 -> 10004
size cluster i: 59 -> 10002
size cluster i: 60 -> 10006
size cluster i: 61 -> 10004
size cluster i: 62 -> 10008
size cluster i: 63 -> 20009
size cluster i: 64 -> 10004
size cluster i: 65 -> 10004
size cluster i: 66 -> 10003
size cluster i: 67 -> 10006
size cluster i: 68 -> 10005
size cluster i: 69 -> 10000
size cluster i: 70 -> 10007
size cluster i: 71 -> 10010
size cluster i: 72 -> 10004
size cluster i: 73 -> 10003
size cluster i: 74 -> 10002
size cluster i: 75 -> 10003
size cluster i: 76 -> 10005
size cluster i: 77 -> 10001
size cluster i: 78 -> 10007
size cluster i: 79 -> 9996
size cluster i: 80 -> 10003
size cluster i: 81 -> 10002
size cluster i: 82 -> 10004
size cluster i: 83 -> 10000
size cluster i: 84 -> 10003
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10002
size cluster i: 88 -> 2
size cluster i: 89 -> 2
size cluster i: 90 -> 4
size cluster i: 91 -> 3
size cluster i: 92 -> 4
size cluster i: 93 -> 2
size cluster i: 96 -> 3
size cluster i: 97 -> 3
size cluster i: 98 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 2
size cluster i: 102 -> 2
size cluster i: 103 -> 2
size cluster i: 105 -> 2
datapoints in clusters: 1000421
score: 90.2341


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:24:08 2019
elapsed time: 158.079s


Finishing: 
---------- 
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Beginning cleanup...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 
Node 3: Exiting... 
Node 6: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:253642] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:253642] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 108
datapoints_clusters: 1000421
score: 90.2341
elapsed_time: 158.079

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 666.666666667 percent_correct: 0.8819323529411764
-> new best_percent_correct:0.8819323529411764new best_threshold:666.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 833.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[52135,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 833.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0707s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 135.115s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.95916s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38763
find clusters duration: 0.291645s
detected clusters: 99
size cluster i: 0 -> 10005
size cluster i: 1 -> 10002
size cluster i: 2 -> 20009
size cluster i: 3 -> 20009
size cluster i: 4 -> 10000
size cluster i: 5 -> 10002
size cluster i: 6 -> 10001
size cluster i: 7 -> 10002
size cluster i: 8 -> 10004
size cluster i: 9 -> 10004
size cluster i: 10 -> 10002
size cluster i: 11 -> 10004
size cluster i: 12 -> 20010
size cluster i: 13 -> 10002
size cluster i: 14 -> 10002
size cluster i: 15 -> 10001
size cluster i: 16 -> 10004
size cluster i: 17 -> 20003
size cluster i: 18 -> 10004
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10003
size cluster i: 22 -> 10002
size cluster i: 23 -> 10004
size cluster i: 24 -> 30013
size cluster i: 25 -> 10004
size cluster i: 26 -> 10003
size cluster i: 27 -> 10003
size cluster i: 28 -> 10002
size cluster i: 29 -> 10001
size cluster i: 30 -> 20007
size cluster i: 31 -> 10006
size cluster i: 32 -> 10002
size cluster i: 33 -> 10001
size cluster i: 34 -> 10005
size cluster i: 35 -> 10007
size cluster i: 36 -> 10003
size cluster i: 37 -> 10004
size cluster i: 38 -> 10004
size cluster i: 39 -> 10000
size cluster i: 40 -> 10003
size cluster i: 41 -> 10003
size cluster i: 42 -> 10000
size cluster i: 43 -> 10001
size cluster i: 44 -> 10003
size cluster i: 45 -> 20007
size cluster i: 46 -> 10002
size cluster i: 47 -> 10003
size cluster i: 48 -> 10002
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10002
size cluster i: 52 -> 10002
size cluster i: 53 -> 10004
size cluster i: 54 -> 10002
size cluster i: 55 -> 10002
size cluster i: 56 -> 10006
size cluster i: 57 -> 10002
size cluster i: 58 -> 10001
size cluster i: 59 -> 10006
size cluster i: 60 -> 10004
size cluster i: 61 -> 10002
size cluster i: 62 -> 10006
size cluster i: 63 -> 10003
size cluster i: 64 -> 10005
size cluster i: 65 -> 20009
size cluster i: 66 -> 10002
size cluster i: 67 -> 10004
size cluster i: 68 -> 10003
size cluster i: 69 -> 10006
size cluster i: 70 -> 10005
size cluster i: 71 -> 10000
size cluster i: 72 -> 10007
size cluster i: 73 -> 10007
size cluster i: 74 -> 10003
size cluster i: 75 -> 10003
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10004
size cluster i: 79 -> 10004
size cluster i: 80 -> 10001
size cluster i: 81 -> 10005
size cluster i: 82 -> 9912
size cluster i: 83 -> 10001
size cluster i: 84 -> 10002
size cluster i: 85 -> 10003
size cluster i: 86 -> 10000
size cluster i: 87 -> 10003
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 2
size cluster i: 92 -> 2
size cluster i: 94 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 2
datapoints in clusters: 1000222
score: 97.0804


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:26:55 2019
elapsed time: 157.79s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:253982] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:253982] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 1000222
score: 97.0804
elapsed_time: 157.79

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 833.333333333 percent_correct: 0.9113745098039215
-> new best_percent_correct:0.9113745098039215new best_threshold:833.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 1000.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[51765,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 1000


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0661s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 135.132s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.04686s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38699
find clusters duration: 0.274835s
detected clusters: 100
size cluster i: 0 -> 10004
size cluster i: 1 -> 10001
size cluster i: 2 -> 20005
size cluster i: 3 -> 10003
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10002
size cluster i: 9 -> 10004
size cluster i: 10 -> 10002
size cluster i: 11 -> 10002
size cluster i: 12 -> 20009
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10001
size cluster i: 16 -> 10002
size cluster i: 17 -> 20001
size cluster i: 18 -> 10003
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10002
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20006
size cluster i: 25 -> 10005
size cluster i: 26 -> 10003
size cluster i: 27 -> 10002
size cluster i: 28 -> 10002
size cluster i: 29 -> 10001
size cluster i: 30 -> 9980
size cluster i: 31 -> 20007
size cluster i: 32 -> 10006
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10003
size cluster i: 36 -> 10005
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10004
size cluster i: 40 -> 10002
size cluster i: 41 -> 9990
size cluster i: 42 -> 10003
size cluster i: 43 -> 10003
size cluster i: 44 -> 10000
size cluster i: 45 -> 9996
size cluster i: 46 -> 10003
size cluster i: 47 -> 20006
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 10004
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 10005
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10004
size cluster i: 63 -> 10001
size cluster i: 64 -> 10006
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 20008
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10006
size cluster i: 72 -> 10004
size cluster i: 73 -> 10000
size cluster i: 74 -> 10006
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10002
size cluster i: 78 -> 10000
size cluster i: 79 -> 10003
size cluster i: 80 -> 10002
size cluster i: 81 -> 10003
size cluster i: 82 -> 10001
size cluster i: 83 -> 10003
size cluster i: 84 -> 9459
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10002
size cluster i: 88 -> 10000
size cluster i: 89 -> 10003
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 2
datapoints in clusters: 999655
score: 98.0054


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:29:43 2019
elapsed time: 157.84s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Beginning cleanup...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!


Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:254348] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:254348] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 100
datapoints_clusters: 999655
score: 98.0054
elapsed_time: 157.84

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 1000.0 percent_correct: 0.9305813725490196
-> new best_percent_correct:0.9305813725490196new best_threshold:1000.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 1166.6666666666665 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[51523,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 1166.67


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1611s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 135.161s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.01161s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38641
find clusters duration: 0.282086s
detected clusters: 97
size cluster i: 0 -> 10004
size cluster i: 1 -> 9999
size cluster i: 2 -> 20004
size cluster i: 3 -> 10001
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10002
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 20006
size cluster i: 13 -> 10002
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20005
size cluster i: 25 -> 10005
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9916
size cluster i: 31 -> 20007
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10004
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9934
size cluster i: 42 -> 10003
size cluster i: 43 -> 10003
size cluster i: 44 -> 10000
size cluster i: 45 -> 9945
size cluster i: 46 -> 10002
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 10003
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 10004
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10004
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10003
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10003
size cluster i: 81 -> 10002
size cluster i: 82 -> 10003
size cluster i: 83 -> 10001
size cluster i: 84 -> 10003
size cluster i: 85 -> 8496
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 10002
size cluster i: 91 -> 10000
size cluster i: 92 -> 9981
size cluster i: 93 -> 10001
size cluster i: 94 -> 2
size cluster i: 95 -> 2
datapoints in clusters: 998440
score: 94.9497


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:32:31 2019
elapsed time: 157.951s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 4: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:254714] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:254714] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 998440
score: 94.9497
elapsed_time: 157.951

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 1166.66666667 percent_correct: 0.9393117647058824
-> new best_percent_correct:0.9393117647058824new best_threshold:1166.66666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 1333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[53235,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 1333.33


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1475s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 135.182s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.93116s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38609
find clusters duration: 0.287821s
detected clusters: 96
size cluster i: 0 -> 10004
size cluster i: 1 -> 9993
size cluster i: 2 -> 20003
size cluster i: 3 -> 10001
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 20005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9738
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10003
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9783
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9737
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10002
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10004
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10003
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10001
size cluster i: 82 -> 10003
size cluster i: 83 -> 10001
size cluster i: 84 -> 10003
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 7088
size cluster i: 90 -> 10002
size cluster i: 91 -> 9998
size cluster i: 92 -> 9916
size cluster i: 93 -> 10001
size cluster i: 94 -> 2
size cluster i: 95 -> 2
datapoints in clusters: 996387
score: 93.7776


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:35:19 2019
elapsed time: 157.871s


Finishing: 
---------- 
Beginning cleanup...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 1: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:255050] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:255050] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 996387
score: 93.7776
elapsed_time: 157.871

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 1333.33333333 percent_correct: 0.9373676470588236

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 1500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[52765,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 1500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1272s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 135.167s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.07349s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38514
find clusters duration: 0.269654s
detected clusters: 98
size cluster i: 0 -> 10003
size cluster i: 1 -> 9965
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9998
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9427
size cluster i: 31 -> 20006
size cluster i: 32 -> 10003
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9505
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9229
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10004
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10002
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 5424
size cluster i: 92 -> 10002
size cluster i: 93 -> 9992
size cluster i: 94 -> 9735
size cluster i: 95 -> 10000
size cluster i: 96 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 993389
score: 95.4433


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:38:06 2019
elapsed time: 157.989s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 2: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 7: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:255396] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:255396] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 993389
score: 95.4433
elapsed_time: 157.989

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 1500.0 percent_correct: 0.9540794117647059
-> new best_percent_correct:0.9540794117647059new best_threshold:1500.0
thresholds:[ 1370.37037037  1407.40740741  1444.44444444  1481.48148148  1518.51851852
  1555.55555556  1592.59259259  1629.62962963], threshold_step:37.037037037

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 1370.3703703703702 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[52395,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 1370.37


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1824s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 135.105s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.00869s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38581
find clusters duration: 0.273669s
detected clusters: 96
size cluster i: 0 -> 10003
size cluster i: 1 -> 9991
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 20005
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9682
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9742
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9656
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10004
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10002
size cluster i: 73 -> 10002
size cluster i: 74 -> 10000
size cluster i: 75 -> 10002
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10003
size cluster i: 84 -> 10001
size cluster i: 85 -> 10002
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 6735
size cluster i: 91 -> 10002
size cluster i: 92 -> 9998
size cluster i: 93 -> 9878
size cluster i: 94 -> 10001
size cluster i: 95 -> 2
datapoints in clusters: 995806
score: 93.7229


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:40:54 2019
elapsed time: 157.918s


Finishing: 
---------- 
Beginning cleanup...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:255762] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:255762] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 995806
score: 93.7229
elapsed_time: 157.918

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 1370.37037037 percent_correct: 0.946621568627451

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 1407.4074074074074 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[49982,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 1407.41


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.086s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 135.128s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.08447s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38561
find clusters duration: 0.271361s
detected clusters: 97
size cluster i: 0 -> 10003
size cluster i: 1 -> 9988
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 20005
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9617
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9690
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9537
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10004
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10002
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10002
size cluster i: 84 -> 10001
size cluster i: 85 -> 10002
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 6394
size cluster i: 91 -> 10002
size cluster i: 92 -> 9996
size cluster i: 93 -> 9851
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
datapoints in clusters: 995193
score: 94.6409


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:43:42 2019
elapsed time: 157.912s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 4: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 
Node 1: Exiting... 
Node 6: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:256135] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:256135] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 995193
score: 94.6409
elapsed_time: 157.912

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 1407.40740741 percent_correct: 0.9460294117647059

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 1444.4444444444443 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[49735,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 1444.44


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2848s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 135.204s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.92835s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38563
find clusters duration: 0.275312s
detected clusters: 98
size cluster i: 0 -> 10003
size cluster i: 1 -> 9982
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9555
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9623
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9422
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10004
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10002
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10002
size cluster i: 84 -> 10000
size cluster i: 85 -> 10002
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 5975
size cluster i: 92 -> 10002
size cluster i: 93 -> 9995
size cluster i: 94 -> 9807
size cluster i: 95 -> 10000
size cluster i: 96 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 994477
score: 95.5478


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:46:30 2019
elapsed time: 158.055s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 5: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:256510] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:256510] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 994477
score: 95.5478
elapsed_time: 158.055

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 1444.44444444 percent_correct: 0.9551382352941177
-> new best_percent_correct:0.9551382352941177new best_threshold:1444.44444444

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 1481.4814814814815 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[49370,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 1481.48


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2819s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 135.123s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.08542s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38535
find clusters duration: 0.275203s
detected clusters: 98
size cluster i: 0 -> 10003
size cluster i: 1 -> 9977
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9998
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9471
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9549
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9281
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10004
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10002
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 5609
size cluster i: 92 -> 10002
size cluster i: 93 -> 9994
size cluster i: 94 -> 9761
size cluster i: 95 -> 10000
size cluster i: 96 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 993756
score: 95.4785


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:49:18 2019
elapsed time: 158.101s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 5: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 8: Exiting... 
Node 7: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Node 1: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3Warning: Node  is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:256867] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:256867] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 993756
score: 95.4785
elapsed_time: 158.101

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 1481.48148148 percent_correct: 0.954435294117647

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 1518.5185185185185 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[50958,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 1518.52


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0925s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 135.106s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.01111s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38497
find clusters duration: 0.277742s
detected clusters: 97
size cluster i: 0 -> 10003
size cluster i: 1 -> 9962
size cluster i: 2 -> 20002
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9998
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9380
size cluster i: 31 -> 20006
size cluster i: 32 -> 10003
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9465
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9157
size cluster i: 46 -> 10001
size cluster i: 47 -> 20003
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10004
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10002
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 5236
size cluster i: 92 -> 10002
size cluster i: 93 -> 9988
size cluster i: 94 -> 9708
size cluster i: 95 -> 10000
size cluster i: 96 -> 2
datapoints in clusters: 993004
score: 94.4327


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:52:05 2019
elapsed time: 157.831s


Finishing: 
---------- 
Node 1 received cleanup signal...
Beginning cleanup...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 1: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:257207] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:257207] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 993004
score: 94.4327
elapsed_time: 157.831

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 1518.51851852 percent_correct: 0.9537078431372549

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 1555.5555555555557 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[50577,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 1555.56


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0856s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 135.061s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.98138s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38485
find clusters duration: 0.274021s
detected clusters: 98
size cluster i: 0 -> 10003
size cluster i: 1 -> 9951
size cluster i: 2 -> 20002
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9998
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9276
size cluster i: 31 -> 20006
size cluster i: 32 -> 10003
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9374
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 8991
size cluster i: 46 -> 10001
size cluster i: 47 -> 20003
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10004
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 10002
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10004
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10002
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 4903
size cluster i: 92 -> 10002
size cluster i: 93 -> 9981
size cluster i: 94 -> 9642
size cluster i: 95 -> 10000
size cluster i: 96 -> 4
size cluster i: 97 -> 2
datapoints in clusters: 992227
score: 95.3316


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:54:53 2019
elapsed time: 157.751s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:257576] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:257576] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 992227
score: 95.3316
elapsed_time: 157.751

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 1555.55555556 percent_correct: 0.9529480392156863

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 1592.5925925925926 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[50221,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 1592.59


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2497s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 135.193s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.06689s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38440
find clusters duration: 0.27106s
detected clusters: 99
size cluster i: 0 -> 10003
size cluster i: 1 -> 9939
size cluster i: 2 -> 20002
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9993
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9170
size cluster i: 31 -> 20006
size cluster i: 32 -> 10003
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9266
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 8826
size cluster i: 46 -> 10001
size cluster i: 47 -> 20003
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10004
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 10002
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10004
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10002
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 4566
size cluster i: 92 -> 10002
size cluster i: 93 -> 9973
size cluster i: 94 -> 9558
size cluster i: 95 -> 10000
size cluster i: 96 -> 3
size cluster i: 97 -> 2
size cluster i: 98 -> 2
datapoints in clusters: 991403
score: 96.2244


Runtimes: 
--------- 
finished computation at Thu Jan  3 14:57:41 2019
elapsed time: 158.126s


Finishing: 
---------- 
Beginning cleanup...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 4: Exiting... 
Destroying the enviroment now!
Node 3: Exiting... 
Cleanup done!
Node 5: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:257940] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:257940] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 991403
score: 96.2244
elapsed_time: 158.126

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 1592.59259259 percent_correct: 0.9521392156862745

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-08 --k 6 --epsilon 0.001 --threshold 1629.6296296296296 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[56137,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-08
k: 6
threshold: 1629.63


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1095s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022824
delta: 0.054655
delta: 0.0447983
delta: 0.0324013
delta: 0.0388888
delta: 0.0632285
delta: 0.0377143
delta: 0.0287348
delta: 0.0428178
delta: 0.0183149
delta: 0.0907478
delta: 0.0659976
delta: 0.0252465
delta: 0.01166
delta: 0.0179274
delta: 0.0535125
delta: 0.0197197
delta: 0.0187266
delta: 0.0407096
delta: 0.0332551
delta: 0.00892403
delta: 0.176134
delta: 0.0161878
delta: 0.0362868
delta: 0.00892277
delta: 0.0185978
delta: 0.0943776
delta: 0.00957028
delta: 0.0190834
delta: 0.00671978
delta: 0.00456047
delta: 0.14454
delta: 0.00625929
delta: 0.00897694
delta: 0.0264473
delta: 0.00563284
delta: 0.0375663
delta: 0.00775938
delta: 0.00517324
delta: 0.0038275
delta: 0.00344927
delta: 0.0177023
delta: 0.00704137
delta: 0.0168432
delta: 0.00196713
delta: 0.00476368
delta: 0.00768907
delta: 0.00381574
delta: 0.007635
delta: 0.00263083
delta: 0.0029548
delta: 0.00531891
delta: 0.04333
delta: 0.00179347
delta: 0.0100805
delta: 0.000867803
delta: 0.00638046
delta: 0.00176287
delta: 0.00322142
delta: 0.00495485
delta: 0.00133128
delta: 0.00211368
delta: 0.00132209
delta: 0.00386169
delta: 0.00105585
delta: 0.000662882
delta: 0.00186155
delta: 0.00453071
delta: 0.00145124
delta: 0.000977945
delta: 0.000784251
delta: 0.00142996
delta: 0.00112995
delta: 0.000706453
delta: 0.001775
delta: 0.000888574
delta: 0.00179161
delta: 0.00413438
delta: 0.000411687
delta: 0.000702279
delta: 0.000863176
delta: 0.00251699
delta: 0.00062442
delta: 0.0017856
delta: 0.000341035
delta: 0.000292769
delta: 0.00153692
delta: 0.00228971
delta: 0.000151074
delta: 0.000476778
delta: 0.000235121
delta: 0.00490859
delta: 0.000941854
delta: 0.000143297
delta: 0.000602618
delta: 0.000668396
delta: 0.00306429
delta: 0.000147019
delta: 0.000431867
delta: 0.000157602
delta: 0.000133284
delta: 0.00121653
delta: 0.000503165
delta: 6.85482e-05
delta: 0.000201328
delta: 0.000437459
delta: 0.000368706
delta: 4.25068e-05
delta: 9.60868e-05
delta: 0.000155025
delta: 0.000281025
delta: 0.000398421
delta: 8.01698e-05
delta: 8.51884e-05
delta: 0.000151936
delta: 5.38975e-05
delta: 0.000372069
delta: 9.21959e-05
delta: 0.000152923
delta: 3.43334e-05
delta: 0.000699871
delta: 0.000217284
delta: 0.000105193
delta: 4.98273e-05
delta: 2.43895e-05
delta: 0.000179457
delta: 0.000116146
delta: 3.81555e-05
delta: 2.84711e-05
delta: 8.56375e-05
delta: 2.15781e-05
delta: 0.000143088
delta: 4.09707e-05
delta: 0.00010422
delta: 1.71392e-05
delta: 0.000132291
delta: 1.55269e-05
delta: 0.000167585
delta: 2.28683e-05
delta: 1.60462e-05
delta: 0.000495427
delta: 2.52826e-05
delta: 2.27416e-05
delta: 2.29144e-05
delta: 2.38689e-05
delta: 3.55264e-05
delta: 3.37374e-05
delta: 2.02792e-05
delta: 9.9719e-06
delta: 4.22735e-05
delta: 0.000177551
delta: 5.03943e-06
delta: 2.8123e-05
delta: 2.99514e-05
delta: 1.02075e-05
delta: 4.57233e-06
delta: 6.12853e-06
delta: 1.92243e-05
delta: 1.95766e-05
delta: 3.1541e-06
delta: 1.06108e-05
delta: 2.22222e-05
delta: 4.47465e-06
delta: 1.41493e-05
delta: 1.0581e-05
delta: 3.29266e-06
delta: 3.66861e-06
delta: 1.14214e-05
delta: 5.215e-06
delta: 1.98621e-05
delta: 5.39559e-06
delta: 2.19961e-06
delta: 4.6792e-05
delta: 6.79779e-06
delta: 3.99799e-06
delta: 1.88622e-06
delta: 1.36361e-05
delta: 1.1098e-05
delta: 7.48838e-06
delta: 3.32497e-06
delta: 2.90449e-06
delta: 4.56754e-06
delta: 1.15411e-05
delta: 1.72488e-06
delta: 1.36324e-06
delta: 5.76643e-06
delta: 4.76948e-06
delta: 2.6853e-06
delta: 2.37828e-06
delta: 5.61869e-06
delta: 2.50133e-06
delta: 1.43527e-06
delta: 9.69454e-07
delta: 4.53323e-06
delta: 2.55756e-06
delta: 3.39942e-06
delta: 4.12829e-07
delta: 2.67851e-06
delta: 2.325e-07
delta: 6.11285e-06
delta: 1.2883e-06
delta: 2.29551e-06
delta: 1.01651e-06
delta: 8.47316e-07
delta: 6.08212e-06
delta: 5.92221e-07
delta: 4.60244e-07
delta: 2.81536e-06
delta: 7.15872e-07
delta: 9.94978e-07
delta: 1.43093e-06
delta: 1.17207e-06
delta: 2.27011e-06
delta: 5.73822e-07
delta: 6.7649e-07
delta: 5.17778e-07
delta: 1.28357e-06
delta: 3.43156e-07
delta: 9.80553e-07
delta: 2.15103e-06
delta: 1.37398e-07
delta: 4.43737e-06
delta: 3.80154e-07
delta: 2.12443e-07
delta: 1.17232e-06
delta: 3.91484e-07
delta: 4.31883e-07
delta: 3.24034e-07
delta: 3.01189e-07
delta: 6.66521e-07
delta: 3.28299e-07
delta: 1.48383e-07
delta: 2.40431e-07
delta: 7.76313e-07
delta: 1.01192e-06
delta: 1.26461e-07
delta: 5.36474e-08
delta: 8.37374e-08
delta: 1.74e-06
delta: 1.36869e-07
delta: 1.13728e-07
delta: 1.53504e-07
delta: 2.94216e-07
delta: 2.0125e-07
delta: 9.19184e-08
delta: 3.86234e-08
Number of iterations: 246 (max. 1000)
Final norm of residuum: 3.86234e-08
counted_mult_calls: 251
solver duration: 135.132s
Number of grid points:     397825
alpha min: -5914.4 max: 19188.7
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.95591s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38414
find clusters duration: 0.27882s
detected clusters: 98
size cluster i: 0 -> 10003
size cluster i: 1 -> 9923
size cluster i: 2 -> 20002
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9990
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9070
size cluster i: 31 -> 20006
size cluster i: 32 -> 10003
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9138
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 8615
size cluster i: 46 -> 10001
size cluster i: 47 -> 20003
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 10002
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10004
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10001
size cluster i: 83 -> 10002
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 4231
size cluster i: 92 -> 10002
size cluster i: 93 -> 9957
size cluster i: 94 -> 9475
size cluster i: 95 -> 10000
size cluster i: 96 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 990506
score: 95.1663


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:00:28 2019
elapsed time: 157.846s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:258288] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:258288] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 990506
score: 95.1663
elapsed_time: 157.846

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-08 threshold: 1629.62962963 percent_correct: 0.9512666666666667
thresholds:[0.0, 166.66666666666666, 333.33333333333331, 500.0, 666.66666666666663, 833.33333333333326, 1000.0, 1166.6666666666665, 1333.3333333333333, 1500.0], threshold_step:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 0.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[55799,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 0


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1113s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.584s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.02705s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37747
find clusters duration: 65.0641s
detected clusters: 1
size cluster i: 0 -> 1020000
datapoints in clusters: 1020000
score: 1


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:03:33 2019
elapsed time: 175.097s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 4: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Node 2: Exiting... 
Node 8: Exiting... 
Cleanup done!
Node 6: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:258638] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:258638] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 1
datapoints_clusters: 1020000
score: 1.0
elapsed_time: 175.097

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 0.0 percent_correct: 0.00980392156862745
-> new best_percent_correct:0.00980392156862745new best_threshold:0.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 166.66666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[55297,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 166.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1051s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.0464s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.01433s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39384
find clusters duration: 5.52028s
detected clusters: 282
size cluster i: 0 -> 510863
size cluster i: 1 -> 10016
size cluster i: 2 -> 30047
size cluster i: 3 -> 40059
size cluster i: 4 -> 10002
size cluster i: 5 -> 10018
size cluster i: 6 -> 30058
size cluster i: 7 -> 10010
size cluster i: 8 -> 10013
size cluster i: 9 -> 10016
size cluster i: 10 -> 20034
size cluster i: 11 -> 10004
size cluster i: 12 -> 10009
size cluster i: 13 -> 10024
size cluster i: 14 -> 10008
size cluster i: 15 -> 10018
size cluster i: 16 -> 10015
size cluster i: 17 -> 10010
size cluster i: 18 -> 30064
size cluster i: 19 -> 10006
size cluster i: 20 -> 10015
size cluster i: 21 -> 10006
size cluster i: 22 -> 10016
size cluster i: 23 -> 20021
size cluster i: 24 -> 10012
size cluster i: 25 -> 10010
size cluster i: 26 -> 10008
size cluster i: 27 -> 10014
size cluster i: 28 -> 10013
size cluster i: 29 -> 10015
size cluster i: 30 -> 10012
size cluster i: 31 -> 10022
size cluster i: 32 -> 10019
size cluster i: 33 -> 10001
size cluster i: 34 -> 10013
size cluster i: 35 -> 10005
size cluster i: 36 -> 10010
size cluster i: 37 -> 10010
size cluster i: 38 -> 10016
size cluster i: 39 -> 2
size cluster i: 40 -> 2
size cluster i: 41 -> 2
size cluster i: 42 -> 3
size cluster i: 43 -> 2
size cluster i: 44 -> 2
size cluster i: 45 -> 2
size cluster i: 46 -> 3
size cluster i: 47 -> 4
size cluster i: 48 -> 4
size cluster i: 49 -> 5
size cluster i: 50 -> 3
size cluster i: 51 -> 2
size cluster i: 52 -> 2
size cluster i: 53 -> 2
size cluster i: 54 -> 2
size cluster i: 55 -> 3
size cluster i: 56 -> 2
size cluster i: 57 -> 4
size cluster i: 58 -> 5
size cluster i: 59 -> 3
size cluster i: 60 -> 2
size cluster i: 61 -> 2
size cluster i: 62 -> 3
size cluster i: 63 -> 3
size cluster i: 64 -> 3
size cluster i: 65 -> 2
size cluster i: 67 -> 5
size cluster i: 69 -> 5
size cluster i: 70 -> 2
size cluster i: 71 -> 3
size cluster i: 72 -> 2
size cluster i: 73 -> 2
size cluster i: 74 -> 2
size cluster i: 75 -> 2
size cluster i: 78 -> 2
size cluster i: 79 -> 2
size cluster i: 80 -> 3
size cluster i: 81 -> 2
size cluster i: 82 -> 5
size cluster i: 83 -> 4
size cluster i: 84 -> 2
size cluster i: 85 -> 5
size cluster i: 86 -> 3
size cluster i: 87 -> 4
size cluster i: 88 -> 4
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 3
size cluster i: 92 -> 2
size cluster i: 93 -> 3
size cluster i: 94 -> 4
size cluster i: 95 -> 4
size cluster i: 96 -> 4
size cluster i: 97 -> 3
size cluster i: 99 -> 4
size cluster i: 100 -> 5
size cluster i: 101 -> 3
size cluster i: 102 -> 4
size cluster i: 103 -> 2
size cluster i: 104 -> 3
size cluster i: 105 -> 3
size cluster i: 106 -> 2
size cluster i: 107 -> 2
size cluster i: 108 -> 2
size cluster i: 109 -> 2
size cluster i: 110 -> 3
size cluster i: 112 -> 3
size cluster i: 113 -> 4
size cluster i: 114 -> 4
size cluster i: 116 -> 2
size cluster i: 117 -> 2
size cluster i: 119 -> 4
size cluster i: 121 -> 2
size cluster i: 122 -> 3
size cluster i: 123 -> 4
size cluster i: 124 -> 3
size cluster i: 125 -> 3
size cluster i: 126 -> 2
size cluster i: 127 -> 3
size cluster i: 128 -> 2
size cluster i: 129 -> 2
size cluster i: 130 -> 2
size cluster i: 131 -> 3
size cluster i: 132 -> 2
size cluster i: 134 -> 2
size cluster i: 135 -> 2
size cluster i: 136 -> 2
size cluster i: 137 -> 3
size cluster i: 138 -> 5
size cluster i: 140 -> 4
size cluster i: 142 -> 2
size cluster i: 144 -> 3
size cluster i: 145 -> 2
size cluster i: 146 -> 2
size cluster i: 148 -> 2
size cluster i: 149 -> 3
size cluster i: 150 -> 2
size cluster i: 151 -> 6
size cluster i: 152 -> 2
size cluster i: 153 -> 2
size cluster i: 154 -> 2
size cluster i: 155 -> 2
size cluster i: 156 -> 2
size cluster i: 157 -> 2
size cluster i: 158 -> 2
size cluster i: 159 -> 2
size cluster i: 160 -> 2
size cluster i: 161 -> 2
size cluster i: 162 -> 4
size cluster i: 163 -> 4
size cluster i: 164 -> 2
size cluster i: 166 -> 4
size cluster i: 167 -> 2
size cluster i: 168 -> 4
size cluster i: 169 -> 4
size cluster i: 170 -> 2
size cluster i: 173 -> 2
size cluster i: 174 -> 2
size cluster i: 175 -> 2
size cluster i: 176 -> 3
size cluster i: 178 -> 2
size cluster i: 179 -> 2
size cluster i: 180 -> 3
size cluster i: 181 -> 2
size cluster i: 182 -> 2
size cluster i: 183 -> 2
size cluster i: 184 -> 3
size cluster i: 185 -> 2
size cluster i: 188 -> 4
size cluster i: 190 -> 2
size cluster i: 191 -> 2
size cluster i: 193 -> 3
size cluster i: 195 -> 2
size cluster i: 196 -> 3
size cluster i: 197 -> 2
size cluster i: 199 -> 2
size cluster i: 200 -> 4
size cluster i: 201 -> 2
size cluster i: 202 -> 2
size cluster i: 203 -> 3
size cluster i: 205 -> 2
size cluster i: 206 -> 3
size cluster i: 208 -> 2
size cluster i: 210 -> 2
size cluster i: 212 -> 3
size cluster i: 216 -> 2
size cluster i: 217 -> 2
size cluster i: 218 -> 2
size cluster i: 219 -> 2
size cluster i: 220 -> 2
size cluster i: 224 -> 2
size cluster i: 225 -> 2
size cluster i: 227 -> 2
size cluster i: 230 -> 3
size cluster i: 233 -> 2
size cluster i: 235 -> 2
size cluster i: 237 -> 2
size cluster i: 240 -> 2
size cluster i: 243 -> 2
size cluster i: 252 -> 2
size cluster i: 258 -> 2
size cluster i: 265 -> 2
size cluster i: 266 -> 2
datapoints in clusters: 1002049
score: 0


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:05:39 2019
elapsed time: 115.011s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:259000] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:259000] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 282
datapoints_clusters: 1002049
score: 0.0
elapsed_time: 115.011

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 166.666666667 percent_correct: 0.3999519607843137
-> new best_percent_correct:0.3999519607843137new best_threshold:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[56973,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 333.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1594s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.3535s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.09078s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39020
find clusters duration: 0.343005s
detected clusters: 137
size cluster i: 0 -> 100102
size cluster i: 1 -> 10004
size cluster i: 2 -> 20018
size cluster i: 3 -> 30026
size cluster i: 4 -> 10002
size cluster i: 5 -> 10007
size cluster i: 6 -> 30035
size cluster i: 7 -> 10007
size cluster i: 8 -> 10005
size cluster i: 9 -> 10009
size cluster i: 10 -> 10006
size cluster i: 11 -> 10007
size cluster i: 12 -> 30019
size cluster i: 13 -> 10007
size cluster i: 14 -> 10004
size cluster i: 15 -> 10001
size cluster i: 16 -> 10010
size cluster i: 17 -> 60058
size cluster i: 18 -> 10012
size cluster i: 19 -> 10002
size cluster i: 20 -> 10004
size cluster i: 21 -> 20020
size cluster i: 22 -> 10004
size cluster i: 23 -> 10006
size cluster i: 24 -> 10012
size cluster i: 25 -> 10007
size cluster i: 26 -> 10007
size cluster i: 27 -> 10004
size cluster i: 28 -> 20018
size cluster i: 29 -> 10009
size cluster i: 30 -> 20016
size cluster i: 31 -> 10004
size cluster i: 32 -> 30032
size cluster i: 33 -> 20011
size cluster i: 34 -> 10008
size cluster i: 35 -> 20019
size cluster i: 36 -> 10003
size cluster i: 37 -> 10006
size cluster i: 38 -> 10006
size cluster i: 39 -> 10003
size cluster i: 40 -> 10005
size cluster i: 41 -> 20014
size cluster i: 42 -> 10003
size cluster i: 43 -> 10008
size cluster i: 44 -> 10005
size cluster i: 45 -> 10004
size cluster i: 46 -> 10004
size cluster i: 47 -> 10007
size cluster i: 48 -> 10008
size cluster i: 49 -> 10009
size cluster i: 50 -> 10007
size cluster i: 51 -> 10002
size cluster i: 52 -> 10008
size cluster i: 53 -> 10006
size cluster i: 54 -> 10009
size cluster i: 55 -> 10006
size cluster i: 56 -> 10005
size cluster i: 57 -> 10003
size cluster i: 58 -> 10019
size cluster i: 59 -> 10005
size cluster i: 60 -> 10006
size cluster i: 61 -> 10005
size cluster i: 62 -> 10007
size cluster i: 63 -> 10002
size cluster i: 64 -> 10000
size cluster i: 65 -> 10007
size cluster i: 66 -> 10008
size cluster i: 67 -> 10001
size cluster i: 68 -> 10003
size cluster i: 69 -> 10001
size cluster i: 70 -> 10008
size cluster i: 71 -> 2
size cluster i: 72 -> 4
size cluster i: 73 -> 2
size cluster i: 74 -> 3
size cluster i: 75 -> 2
size cluster i: 76 -> 3
size cluster i: 77 -> 2
size cluster i: 78 -> 2
size cluster i: 79 -> 5
size cluster i: 80 -> 2
size cluster i: 81 -> 3
size cluster i: 82 -> 2
size cluster i: 83 -> 2
size cluster i: 84 -> 4
size cluster i: 85 -> 4
size cluster i: 86 -> 3
size cluster i: 87 -> 5
size cluster i: 88 -> 2
size cluster i: 89 -> 2
size cluster i: 90 -> 3
size cluster i: 92 -> 3
size cluster i: 93 -> 2
size cluster i: 95 -> 3
size cluster i: 98 -> 2
size cluster i: 100 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 2
size cluster i: 103 -> 2
size cluster i: 105 -> 2
size cluster i: 107 -> 2
size cluster i: 108 -> 3
size cluster i: 109 -> 3
size cluster i: 110 -> 2
size cluster i: 112 -> 4
size cluster i: 115 -> 2
size cluster i: 116 -> 2
size cluster i: 117 -> 3
size cluster i: 119 -> 2
size cluster i: 120 -> 2
size cluster i: 121 -> 2
size cluster i: 123 -> 2
size cluster i: 124 -> 2
size cluster i: 131 -> 2
size cluster i: 134 -> 2
datapoints in clusters: 1000859
score: 61.8178


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:07:40 2019
elapsed time: 110.276s


Finishing: 
---------- 
Beginning cleanup...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 6: Exiting... 
Node 1: Exiting... 
Node 8: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:259380] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:259380] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 137
datapoints_clusters: 1000859
score: 61.8178
elapsed_time: 110.276

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 333.333333333 percent_correct: 0.7148441176470588
-> new best_percent_correct:0.7148441176470588new best_threshold:333.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[56636,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1794s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.3333s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.02628s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38868
find clusters duration: 0.290666s
detected clusters: 105
size cluster i: 0 -> 20017
size cluster i: 1 -> 10004
size cluster i: 2 -> 20011
size cluster i: 3 -> 20015
size cluster i: 4 -> 10000
size cluster i: 5 -> 10004
size cluster i: 6 -> 20012
size cluster i: 7 -> 10004
size cluster i: 8 -> 10004
size cluster i: 9 -> 10005
size cluster i: 10 -> 10004
size cluster i: 11 -> 10005
size cluster i: 12 -> 30016
size cluster i: 13 -> 10006
size cluster i: 14 -> 10003
size cluster i: 15 -> 10001
size cluster i: 16 -> 10007
size cluster i: 17 -> 40025
size cluster i: 18 -> 10009
size cluster i: 19 -> 10001
size cluster i: 20 -> 10001
size cluster i: 21 -> 10007
size cluster i: 22 -> 10003
size cluster i: 23 -> 10005
size cluster i: 24 -> 30020
size cluster i: 25 -> 10007
size cluster i: 26 -> 10004
size cluster i: 27 -> 10003
size cluster i: 28 -> 10001
size cluster i: 29 -> 20009
size cluster i: 30 -> 10007
size cluster i: 31 -> 10004
size cluster i: 32 -> 10003
size cluster i: 33 -> 10010
size cluster i: 34 -> 10005
size cluster i: 35 -> 10007
size cluster i: 36 -> 20013
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10003
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10004
size cluster i: 43 -> 20007
size cluster i: 44 -> 10002
size cluster i: 45 -> 10003
size cluster i: 46 -> 10007
size cluster i: 47 -> 10005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10006
size cluster i: 52 -> 10002
size cluster i: 53 -> 10003
size cluster i: 54 -> 20020
size cluster i: 55 -> 10003
size cluster i: 56 -> 10002
size cluster i: 57 -> 10004
size cluster i: 58 -> 10003
size cluster i: 59 -> 10007
size cluster i: 60 -> 10005
size cluster i: 61 -> 20011
size cluster i: 62 -> 10007
size cluster i: 63 -> 10004
size cluster i: 64 -> 10004
size cluster i: 65 -> 10007
size cluster i: 66 -> 10008
size cluster i: 67 -> 10000
size cluster i: 68 -> 10011
size cluster i: 69 -> 10004
size cluster i: 70 -> 10003
size cluster i: 71 -> 10003
size cluster i: 72 -> 10003
size cluster i: 73 -> 10006
size cluster i: 74 -> 10001
size cluster i: 75 -> 9993
size cluster i: 76 -> 10004
size cluster i: 77 -> 10002
size cluster i: 78 -> 10005
size cluster i: 79 -> 10000
size cluster i: 80 -> 10005
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10002
size cluster i: 84 -> 2
size cluster i: 85 -> 2
size cluster i: 86 -> 2
size cluster i: 87 -> 4
size cluster i: 88 -> 3
size cluster i: 89 -> 4
size cluster i: 90 -> 2
size cluster i: 92 -> 3
size cluster i: 93 -> 2
size cluster i: 94 -> 4
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 98 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 2
datapoints in clusters: 1000490
score: 93.1829


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:09:40 2019
elapsed time: 110.165s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 5: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:259717] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:259717] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!


--------------- SUMMARY -----------------
detected_clusters: 105
datapoints_clusters: 1000490
score: 93.1829
elapsed_time: 110.165

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 500.0 percent_correct: 0.8426431372549019
-> new best_percent_correct:0.8426431372549019new best_threshold:500.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 666.6666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[56431,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 666.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1731s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4886s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.0573s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38774
find clusters duration: 0.28907s
detected clusters: 99
size cluster i: 0 -> 10005
size cluster i: 1 -> 10002
size cluster i: 2 -> 20009
size cluster i: 3 -> 20009
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10001
size cluster i: 7 -> 10002
size cluster i: 8 -> 10003
size cluster i: 9 -> 10004
size cluster i: 10 -> 10002
size cluster i: 11 -> 10003
size cluster i: 12 -> 20010
size cluster i: 13 -> 10002
size cluster i: 14 -> 10002
size cluster i: 15 -> 10001
size cluster i: 16 -> 10006
size cluster i: 17 -> 30006
size cluster i: 18 -> 10007
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10004
size cluster i: 22 -> 10001
size cluster i: 23 -> 10004
size cluster i: 24 -> 30015
size cluster i: 25 -> 10004
size cluster i: 26 -> 10003
size cluster i: 27 -> 10003
size cluster i: 28 -> 9995
size cluster i: 29 -> 20008
size cluster i: 30 -> 10006
size cluster i: 31 -> 10001
size cluster i: 32 -> 10002
size cluster i: 33 -> 10005
size cluster i: 34 -> 10007
size cluster i: 35 -> 10003
size cluster i: 36 -> 10004
size cluster i: 37 -> 10003
size cluster i: 38 -> 9999
size cluster i: 39 -> 10003
size cluster i: 40 -> 10003
size cluster i: 41 -> 10000
size cluster i: 42 -> 10001
size cluster i: 43 -> 10003
size cluster i: 44 -> 20007
size cluster i: 45 -> 10001
size cluster i: 46 -> 10003
size cluster i: 47 -> 10002
size cluster i: 48 -> 10002
size cluster i: 49 -> 10001
size cluster i: 50 -> 10002
size cluster i: 51 -> 10002
size cluster i: 52 -> 10005
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 20013
size cluster i: 56 -> 10002
size cluster i: 57 -> 10001
size cluster i: 58 -> 10006
size cluster i: 59 -> 10004
size cluster i: 60 -> 10003
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10006
size cluster i: 64 -> 20009
size cluster i: 65 -> 10003
size cluster i: 66 -> 10004
size cluster i: 67 -> 10004
size cluster i: 68 -> 10006
size cluster i: 69 -> 10005
size cluster i: 70 -> 10000
size cluster i: 71 -> 10007
size cluster i: 72 -> 10004
size cluster i: 73 -> 10003
size cluster i: 74 -> 10003
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10003
size cluster i: 78 -> 10001
size cluster i: 79 -> 10005
size cluster i: 80 -> 9737
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 10003
size cluster i: 84 -> 10000
size cluster i: 85 -> 10003
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 4
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 1000052
score: 97.0639


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:11:40 2019
elapsed time: 110.353s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:260054] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:260054] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 1000052
score: 97.0639
elapsed_time: 110.353

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 666.666666667 percent_correct: 0.8915745098039216
-> new best_percent_correct:0.8915745098039216new best_threshold:666.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 833.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[53920,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 833.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1155s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.3823s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.08073s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38677
find clusters duration: 0.280597s
detected clusters: 97
size cluster i: 0 -> 10004
size cluster i: 1 -> 9999
size cluster i: 2 -> 20005
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10001
size cluster i: 8 -> 10002
size cluster i: 9 -> 10004
size cluster i: 10 -> 10001
size cluster i: 11 -> 10002
size cluster i: 12 -> 20008
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10001
size cluster i: 16 -> 10003
size cluster i: 17 -> 20002
size cluster i: 18 -> 10003
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10002
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20008
size cluster i: 25 -> 10005
size cluster i: 26 -> 10003
size cluster i: 27 -> 10002
size cluster i: 28 -> 10002
size cluster i: 29 -> 10001
size cluster i: 30 -> 9897
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10002
size cluster i: 36 -> 10005
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10004
size cluster i: 40 -> 10003
size cluster i: 41 -> 9931
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9938
size cluster i: 46 -> 10002
size cluster i: 47 -> 20006
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 10004
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 20009
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 20009
size cluster i: 68 -> 10003
size cluster i: 69 -> 10003
size cluster i: 70 -> 10001
size cluster i: 71 -> 10006
size cluster i: 72 -> 10004
size cluster i: 73 -> 10000
size cluster i: 74 -> 10006
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10002
size cluster i: 78 -> 10000
size cluster i: 79 -> 10003
size cluster i: 80 -> 10002
size cluster i: 81 -> 10001
size cluster i: 82 -> 10004
size cluster i: 83 -> 8575
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10002
size cluster i: 87 -> 10000
size cluster i: 88 -> 10002
size cluster i: 89 -> 10000
size cluster i: 90 -> 9983
size cluster i: 91 -> 10001
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 2
datapoints in clusters: 998549
score: 94.9601


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:13:40 2019
elapsed time: 110.219s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 5: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:260377] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:260377] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 998549
score: 94.9601
elapsed_time: 110.219

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 833.333333333 percent_correct: 0.9196990196078432
-> new best_percent_correct:0.9196990196078432new best_threshold:833.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 1000.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[53709,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1000


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0902s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4893s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.04287s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38607
find clusters duration: 0.277072s
detected clusters: 96
size cluster i: 0 -> 10004
size cluster i: 1 -> 9988
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20005
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9556
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9674
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9529
size cluster i: 46 -> 10001
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10004
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 20008
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10005
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10002
size cluster i: 73 -> 10000
size cluster i: 74 -> 10004
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10001
size cluster i: 82 -> 10001
size cluster i: 83 -> 10004
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 6580
size cluster i: 90 -> 10002
size cluster i: 91 -> 9997
size cluster i: 92 -> 9852
size cluster i: 93 -> 10000
size cluster i: 94 -> 2
size cluster i: 95 -> 2
datapoints in clusters: 995326
score: 93.6777


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:15:40 2019
elapsed time: 110.228s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:260724] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:260724] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 995326
score: 93.6777
elapsed_time: 110.228

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 1000.0 percent_correct: 0.9362980392156863
-> new best_percent_correct:0.9362980392156863new best_threshold:1000.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 1166.6666666666665 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[53364,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1166.67


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1664s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4666s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.00943s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38440
find clusters duration: 0.280531s
detected clusters: 96
size cluster i: 0 -> 10004
size cluster i: 1 -> 9915
size cluster i: 2 -> 20002
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9994
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10004
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20003
size cluster i: 25 -> 10003
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 8863
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9036
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 8481
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10002
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 10002
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 4278
size cluster i: 91 -> 10002
size cluster i: 92 -> 9950
size cluster i: 93 -> 9451
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
datapoints in clusters: 990086
score: 93.1846


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:17:40 2019
elapsed time: 110.261s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 7: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 6: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:261069] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:261069] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 990086
score: 93.1846
elapsed_time: 110.261

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 1166.66666667 percent_correct: 0.9410294117647059
-> new best_percent_correct:0.9410294117647059new best_threshold:1166.66666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 1333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[54913,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1333.33


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2081s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.5139s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.02597s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38225
find clusters duration: 0.274277s
detected clusters: 98
size cluster i: 0 -> 10003
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9956
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10004
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10001
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 20003
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10002
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 6835
size cluster i: 43 -> 10001
size cluster i: 44 -> 20003
size cluster i: 45 -> 10000
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10002
size cluster i: 59 -> 10002
size cluster i: 60 -> 10000
size cluster i: 61 -> 10002
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10000
size cluster i: 71 -> 10002
size cluster i: 72 -> 7856
size cluster i: 73 -> 10001
size cluster i: 74 -> 10003
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 9661
size cluster i: 79 -> 10002
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 8048
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 9795
size cluster i: 92 -> 8726
size cluster i: 93 -> 10000
size cluster i: 94 -> 2401
size cluster i: 95 -> 4
size cluster i: 97 -> 2
datapoints in clusters: 983377
score: 94.4813


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:19:40 2019
elapsed time: 110.363s


Finishing: 
---------- 
Beginning cleanup...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:261432] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:261432] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 983377
score: 94.4813
elapsed_time: 110.363

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 1333.33333333 percent_correct: 0.9344921568627451

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 1500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[54583,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.159s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4805s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.02476s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37972
find clusters duration: 0.262631s
detected clusters: 102
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9822
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9997
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 5033
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9979
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10002
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 6696
size cluster i: 76 -> 10001
size cluster i: 77 -> 10002
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 9128
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 6745
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 10000
size cluster i: 92 -> 10000
size cluster i: 93 -> 10001
size cluster i: 94 -> 9406
size cluster i: 95 -> 7697
size cluster i: 96 -> 10000
size cluster i: 97 -> 1099
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 2
datapoints in clusters: 975686
score: 93.7424


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:21:40 2019
elapsed time: 110.262s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:261774] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:261774] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 102
datapoints_clusters: 975686
score: 93.7424
elapsed_time: 110.262

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 1500.0 percent_correct: 0.9563970588235294
-> new best_percent_correct:0.9563970588235294new best_threshold:1500.0
thresholds:[ 1370.37037037  1407.40740741  1444.44444444  1481.48148148  1518.51851852
  1555.55555556  1592.59259259  1629.62962963], threshold_step:37.037037037

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 1370.3703703703702 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[54362,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1370.37


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2596s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4435s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.03402s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38176
find clusters duration: 0.268848s
detected clusters: 99
size cluster i: 0 -> 10003
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9932
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10004
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 10000
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10002
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 6439
size cluster i: 43 -> 10001
size cluster i: 44 -> 20003
size cluster i: 45 -> 10000
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10002
size cluster i: 59 -> 10002
size cluster i: 60 -> 10000
size cluster i: 61 -> 10002
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10002
size cluster i: 73 -> 7611
size cluster i: 74 -> 10001
size cluster i: 75 -> 10003
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 9575
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 7794
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 10000
size cluster i: 91 -> 10001
size cluster i: 92 -> 9736
size cluster i: 93 -> 8525
size cluster i: 94 -> 10000
size cluster i: 95 -> 2067
size cluster i: 96 -> 3
size cluster i: 97 -> 2
datapoints in clusters: 981775
score: 95.2899


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:23:40 2019
elapsed time: 110.357s


Finishing: 
---------- 
Beginning cleanup...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:262115] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:262115] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 981775
score: 95.2899
elapsed_time: 110.357

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 1370.37037037 percent_correct: 0.9427303921568627

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 1407.4074074074074 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[10900,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1407.41


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2314s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4463s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.92057s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38135
find clusters duration: 0.275269s
detected clusters: 100
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9911
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10004
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 10000
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10001
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 6035
size cluster i: 43 -> 10001
size cluster i: 44 -> 20003
size cluster i: 45 -> 9997
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10002
size cluster i: 59 -> 10002
size cluster i: 60 -> 10000
size cluster i: 61 -> 10002
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10002
size cluster i: 73 -> 7362
size cluster i: 74 -> 10001
size cluster i: 75 -> 10002
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 9459
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 7500
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 10000
size cluster i: 91 -> 10001
size cluster i: 92 -> 9661
size cluster i: 93 -> 8323
size cluster i: 94 -> 10000
size cluster i: 95 -> 1752
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 2
datapoints in clusters: 980091
score: 96.0874


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:25:40 2019
elapsed time: 110.234s


Finishing: 
---------- 
Node 6 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:262442] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:262442] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 100
datapoints_clusters: 980091
score: 96.0874
elapsed_time: 110.234

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 1407.40740741 percent_correct: 0.941092156862745

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 1444.4444444444443 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[10698,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1444.44


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1158s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4551s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.05274s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38073
find clusters duration: 0.271096s
detected clusters: 100
size cluster i: 0 -> 10001
size cluster i: 1 -> 10001
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9881
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 20005
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 5633
size cluster i: 44 -> 10001
size cluster i: 45 -> 20003
size cluster i: 46 -> 9991
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10001
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10001
size cluster i: 72 -> 10000
size cluster i: 73 -> 10002
size cluster i: 74 -> 7087
size cluster i: 75 -> 10001
size cluster i: 76 -> 10002
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 9328
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 7210
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 9560
size cluster i: 94 -> 8075
size cluster i: 95 -> 10000
size cluster i: 96 -> 1455
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 99 -> 2
datapoints in clusters: 978308
score: 95.9125


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:27:41 2019
elapsed time: 110.236s


Finishing: 
---------- 
Beginning cleanup...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with theWarning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
value given in the MPI config!
[argon-gtx:262772] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:262772] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 100
datapoints_clusters: 978308
score: 95.9125
elapsed_time: 110.236

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 1444.44444444 percent_correct: 0.9491529411764706

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 1481.4814814814815 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[10352,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1481.48


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.175s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.5394s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.02439s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38003
find clusters duration: 0.265099s
detected clusters: 101
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9839
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9997
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 20004
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 5217
size cluster i: 44 -> 10001
size cluster i: 45 -> 20003
size cluster i: 46 -> 9982
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10004
size cluster i: 66 -> 10001
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10001
size cluster i: 72 -> 10000
size cluster i: 73 -> 10001
size cluster i: 74 -> 6824
size cluster i: 75 -> 10001
size cluster i: 76 -> 10002
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 9191
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 6905
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 9471
size cluster i: 94 -> 7823
size cluster i: 95 -> 10000
size cluster i: 96 -> 1219
size cluster i: 98 -> 4
size cluster i: 99 -> 2
datapoints in clusters: 976553
score: 94.7831


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:29:41 2019
elapsed time: 110.345s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 6: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:263118] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:263118] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 101
datapoints_clusters: 976553
score: 94.7831
elapsed_time: 110.345

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 1481.48148148 percent_correct: 0.9474401960784313

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 1518.5185185185185 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[11944,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1518.52


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2313s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4541s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.97738s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37958
find clusters duration: 0.267153s
detected clusters: 103
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9803
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9996
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10002
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 4827
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9978
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10002
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 6541
size cluster i: 76 -> 10001
size cluster i: 77 -> 10002
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 9055
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 6589
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 10000
size cluster i: 92 -> 10000
size cluster i: 93 -> 10001
size cluster i: 94 -> 9356
size cluster i: 95 -> 7576
size cluster i: 96 -> 10000
size cluster i: 97 -> 984
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 100 -> 4
size cluster i: 101 -> 2
datapoints in clusters: 974790
score: 92.7006


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:31:41 2019
elapsed time: 110.268s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:263446] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:263446] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 103
datapoints_clusters: 974790
score: 92.7006
elapsed_time: 110.268

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 1518.51851852 percent_correct: 0.9555186274509804

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 1555.5555555555557 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[11717,1],1]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1555.56


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1464s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4958s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.05518s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37908
find clusters duration: 0.26517s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9737
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9993
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 4410
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9970
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10002
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 6292
size cluster i: 76 -> 10001
size cluster i: 77 -> 10001
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 8912
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 6268
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 10000
size cluster i: 92 -> 10000
size cluster i: 93 -> 10001
size cluster i: 94 -> 9247
size cluster i: 95 -> 7296
size cluster i: 96 -> 10000
size cluster i: 97 -> 811
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 2
datapoints in clusters: 973018
score: 91.5782


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:33:41 2019
elapsed time: 110.297s


Finishing: 
---------- 
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:263803] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:263803] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 973018
score: 91.5782
elapsed_time: 110.297

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 1555.55555556 percent_correct: 0.9537892156862745

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 1592.5925925925926 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[11363,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1592.59


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1171s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4416s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.08556s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37845
find clusters duration: 0.262044s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9669
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10003
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9992
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 3947
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9954
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10000
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 5996
size cluster i: 76 -> 10001
size cluster i: 77 -> 10001
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 8718
size cluster i: 82 -> 10001
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 9109
size cluster i: 94 -> 5930
size cluster i: 95 -> 7036
size cluster i: 96 -> 10000
size cluster i: 97 -> 637
size cluster i: 98 -> 3
size cluster i: 99 -> 2
datapoints in clusters: 971060
score: 91.3939


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:35:41 2019
elapsed time: 110.252s


Finishing: 
---------- 
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:264157] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:264157] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 971060
score: 91.3939
elapsed_time: 110.252

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 1592.59259259 percent_correct: 0.9518872549019608

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.0000000000000002e-07 --k 6 --epsilon 0.001 --threshold 1629.6296296296296 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[8846,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1629.63


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0824s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4622s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.0957s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37834
find clusters duration: 0.263173s
detected clusters: 105
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9602
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10003
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9991
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10002
size cluster i: 44 -> 3582
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9929
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 8524
size cluster i: 81 -> 10001
size cluster i: 82 -> 5695
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 8947
size cluster i: 94 -> 5646
size cluster i: 95 -> 6759
size cluster i: 96 -> 10000
size cluster i: 97 -> 500
size cluster i: 99 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 2
size cluster i: 102 -> 3
datapoints in clusters: 969248
score: 90.2731


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:37:41 2019
elapsed time: 110.239s


Finishing: 
---------- 
Beginning cleanup...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Node 4: Exiting... 
Cleanup done!
Node 5: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:264496] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:264496] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 105
datapoints_clusters: 969248
score: 90.2731
elapsed_time: 110.239

--------------- END RUN -----------------
attempt lambda: 1.0000000000000002e-07 threshold: 1629.62962963 percent_correct: 0.9501127450980392
thresholds:[0.0, 166.66666666666666, 333.33333333333331, 500.0, 666.66666666666663, 833.33333333333326, 1000.0, 1166.6666666666665, 1333.3333333333333, 1500.0], threshold_step:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 0.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[8640,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 0


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2649s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 51.0059s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.98406s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37747
find clusters duration: 65.0129s
detected clusters: 1
size cluster i: 0 -> 1020000
datapoints in clusters: 1020000
score: 1


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:40:09 2019
elapsed time: 138.575s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 4: Exiting... 
Node 1: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:264830] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:264830] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 1
datapoints_clusters: 1020000
score: 1.0
elapsed_time: 138.575

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 0.0 percent_correct: 0.00980392156862745
-> new best_percent_correct:0.00980392156862745new best_threshold:0.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 166.66666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[8299,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 166.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2913s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.7334s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.96317s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39313
find clusters duration: 0.991315s
detected clusters: 148
size cluster i: 0 -> 370518
size cluster i: 1 -> 10013
size cluster i: 2 -> 20026
size cluster i: 3 -> 40061
size cluster i: 4 -> 130223
size cluster i: 5 -> 10012
size cluster i: 6 -> 30042
size cluster i: 7 -> 10006
size cluster i: 8 -> 10013
size cluster i: 9 -> 10016
size cluster i: 10 -> 10019
size cluster i: 11 -> 70083
size cluster i: 12 -> 20033
size cluster i: 13 -> 10007
size cluster i: 14 -> 10022
size cluster i: 15 -> 10008
size cluster i: 16 -> 10006
size cluster i: 17 -> 10012
size cluster i: 18 -> 10010
size cluster i: 19 -> 10005
size cluster i: 20 -> 10016
size cluster i: 21 -> 10005
size cluster i: 22 -> 10014
size cluster i: 23 -> 20023
size cluster i: 24 -> 10009
size cluster i: 25 -> 10008
size cluster i: 26 -> 10012
size cluster i: 27 -> 10017
size cluster i: 28 -> 10008
size cluster i: 29 -> 10014
size cluster i: 30 -> 10004
size cluster i: 31 -> 10000
size cluster i: 32 -> 10012
size cluster i: 33 -> 10014
size cluster i: 34 -> 10003
size cluster i: 35 -> 10005
size cluster i: 36 -> 10006
size cluster i: 37 -> 10016
size cluster i: 38 -> 2
size cluster i: 39 -> 4
size cluster i: 40 -> 2
size cluster i: 41 -> 4
size cluster i: 42 -> 3
size cluster i: 43 -> 2
size cluster i: 44 -> 3
size cluster i: 45 -> 5
size cluster i: 46 -> 2
size cluster i: 47 -> 3
size cluster i: 48 -> 2
size cluster i: 49 -> 2
size cluster i: 51 -> 3
size cluster i: 52 -> 2
size cluster i: 53 -> 2
size cluster i: 54 -> 4
size cluster i: 55 -> 2
size cluster i: 56 -> 3
size cluster i: 57 -> 2
size cluster i: 58 -> 3
size cluster i: 59 -> 2
size cluster i: 60 -> 2
size cluster i: 61 -> 4
size cluster i: 62 -> 3
size cluster i: 63 -> 3
size cluster i: 64 -> 2
size cluster i: 65 -> 2
size cluster i: 66 -> 5
size cluster i: 67 -> 3
size cluster i: 68 -> 2
size cluster i: 69 -> 2
size cluster i: 71 -> 4
size cluster i: 72 -> 2
size cluster i: 73 -> 3
size cluster i: 74 -> 3
size cluster i: 75 -> 3
size cluster i: 76 -> 2
size cluster i: 77 -> 2
size cluster i: 80 -> 4
size cluster i: 81 -> 2
size cluster i: 82 -> 2
size cluster i: 83 -> 3
size cluster i: 84 -> 3
size cluster i: 85 -> 2
size cluster i: 86 -> 2
size cluster i: 87 -> 3
size cluster i: 88 -> 4
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 3
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 3
size cluster i: 101 -> 2
size cluster i: 103 -> 2
size cluster i: 105 -> 2
size cluster i: 106 -> 2
size cluster i: 107 -> 2
size cluster i: 108 -> 4
size cluster i: 109 -> 3
size cluster i: 111 -> 3
size cluster i: 113 -> 2
size cluster i: 114 -> 2
size cluster i: 116 -> 2
size cluster i: 119 -> 2
size cluster i: 120 -> 2
size cluster i: 122 -> 2
size cluster i: 126 -> 2
size cluster i: 127 -> 2
size cluster i: 130 -> 2
size cluster i: 135 -> 2
size cluster i: 140 -> 2
datapoints in clusters: 1001545
score: 51.0592


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:41:34 2019
elapsed time: 74.3236s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:265173] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:265173] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 148
datapoints_clusters: 1001545
score: 51.0592
elapsed_time: 74.3236

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 166.666666667 percent_correct: 0.3906421568627451
-> new best_percent_correct:0.3906421568627451new best_threshold:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[9892,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 333.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.3391s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.8506s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.98978s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38945
find clusters duration: 0.366349s
detected clusters: 99
size cluster i: 0 -> 120104
size cluster i: 1 -> 10004
size cluster i: 2 -> 20013
size cluster i: 3 -> 30024
size cluster i: 4 -> 10000
size cluster i: 5 -> 10005
size cluster i: 6 -> 50035
size cluster i: 7 -> 10005
size cluster i: 8 -> 10005
size cluster i: 9 -> 10006
size cluster i: 10 -> 10004
size cluster i: 11 -> 10006
size cluster i: 12 -> 30018
size cluster i: 13 -> 10007
size cluster i: 14 -> 10003
size cluster i: 15 -> 10001
size cluster i: 16 -> 10010
size cluster i: 17 -> 60052
size cluster i: 18 -> 10011
size cluster i: 19 -> 10001
size cluster i: 20 -> 10003
size cluster i: 21 -> 20015
size cluster i: 22 -> 10003
size cluster i: 23 -> 10006
size cluster i: 24 -> 10007
size cluster i: 25 -> 10006
size cluster i: 26 -> 10004
size cluster i: 27 -> 10001
size cluster i: 28 -> 20011
size cluster i: 29 -> 10008
size cluster i: 30 -> 10003
size cluster i: 31 -> 30028
size cluster i: 32 -> 10008
size cluster i: 33 -> 20014
size cluster i: 34 -> 10001
size cluster i: 35 -> 10005
size cluster i: 36 -> 10006
size cluster i: 37 -> 10002
size cluster i: 38 -> 10004
size cluster i: 39 -> 20013
size cluster i: 40 -> 10002
size cluster i: 41 -> 10004
size cluster i: 42 -> 10005
size cluster i: 43 -> 10004
size cluster i: 44 -> 10003
size cluster i: 45 -> 10008
size cluster i: 46 -> 10002
size cluster i: 47 -> 10005
size cluster i: 48 -> 10006
size cluster i: 49 -> 10002
size cluster i: 50 -> 10005
size cluster i: 51 -> 10004
size cluster i: 52 -> 10007
size cluster i: 53 -> 10004
size cluster i: 54 -> 10005
size cluster i: 55 -> 10003
size cluster i: 56 -> 10007
size cluster i: 57 -> 10004
size cluster i: 58 -> 10004
size cluster i: 59 -> 10005
size cluster i: 60 -> 10007
size cluster i: 61 -> 10001
size cluster i: 62 -> 9993
size cluster i: 63 -> 10006
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10000
size cluster i: 67 -> 10000
size cluster i: 68 -> 10004
size cluster i: 69 -> 2
size cluster i: 70 -> 3
size cluster i: 71 -> 2
size cluster i: 72 -> 2
size cluster i: 73 -> 3
size cluster i: 74 -> 3
size cluster i: 75 -> 4
size cluster i: 76 -> 2
size cluster i: 77 -> 5
size cluster i: 79 -> 3
size cluster i: 81 -> 2
size cluster i: 82 -> 2
size cluster i: 83 -> 2
size cluster i: 84 -> 3
size cluster i: 85 -> 2
size cluster i: 86 -> 2
size cluster i: 88 -> 2
size cluster i: 91 -> 2
size cluster i: 93 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 1000629
score: 97.1199


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:42:58 2019
elapsed time: 73.9041s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 5: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:265498] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:265498] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 1000629
score: 97.1199
elapsed_time: 73.9041

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 333.333333333 percent_correct: 0.6954480392156863
-> new best_percent_correct:0.6954480392156863new best_threshold:333.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[9690,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2821s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.9464s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.04988s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38787
find clusters duration: 0.291379s
detected clusters: 91
size cluster i: 0 -> 20012
size cluster i: 1 -> 10001
size cluster i: 2 -> 20007
size cluster i: 3 -> 20011
size cluster i: 4 -> 10000
size cluster i: 5 -> 10002
size cluster i: 6 -> 20012
size cluster i: 7 -> 10002
size cluster i: 8 -> 10002
size cluster i: 9 -> 10004
size cluster i: 10 -> 10001
size cluster i: 11 -> 10003
size cluster i: 12 -> 20010
size cluster i: 13 -> 10003
size cluster i: 14 -> 10002
size cluster i: 15 -> 10001
size cluster i: 16 -> 10004
size cluster i: 17 -> 40018
size cluster i: 18 -> 10008
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10005
size cluster i: 22 -> 10002
size cluster i: 23 -> 10004
size cluster i: 24 -> 30020
size cluster i: 25 -> 10005
size cluster i: 26 -> 10003
size cluster i: 27 -> 10003
size cluster i: 28 -> 9929
size cluster i: 29 -> 20008
size cluster i: 30 -> 10005
size cluster i: 31 -> 20006
size cluster i: 32 -> 10002
size cluster i: 33 -> 10003
size cluster i: 34 -> 10008
size cluster i: 35 -> 10003
size cluster i: 36 -> 10004
size cluster i: 37 -> 20010
size cluster i: 38 -> 9950
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10002
size cluster i: 42 -> 9988
size cluster i: 43 -> 10003
size cluster i: 44 -> 20007
size cluster i: 45 -> 10001
size cluster i: 46 -> 10003
size cluster i: 47 -> 10003
size cluster i: 48 -> 10003
size cluster i: 49 -> 10001
size cluster i: 50 -> 10002
size cluster i: 51 -> 10002
size cluster i: 52 -> 10005
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 20015
size cluster i: 56 -> 10002
size cluster i: 57 -> 10001
size cluster i: 58 -> 10004
size cluster i: 59 -> 10002
size cluster i: 60 -> 10006
size cluster i: 61 -> 10004
size cluster i: 62 -> 20010
size cluster i: 63 -> 20011
size cluster i: 64 -> 10003
size cluster i: 65 -> 10005
size cluster i: 66 -> 10007
size cluster i: 67 -> 10000
size cluster i: 68 -> 10005
size cluster i: 69 -> 10003
size cluster i: 70 -> 10002
size cluster i: 71 -> 10001
size cluster i: 72 -> 10003
size cluster i: 73 -> 10005
size cluster i: 74 -> 10001
size cluster i: 75 -> 9197
size cluster i: 76 -> 10000
size cluster i: 77 -> 10001
size cluster i: 78 -> 10000
size cluster i: 79 -> 10002
size cluster i: 80 -> 10000
size cluster i: 81 -> 9997
size cluster i: 82 -> 10001
size cluster i: 83 -> 2
size cluster i: 84 -> 2
size cluster i: 85 -> 2
size cluster i: 86 -> 2
size cluster i: 87 -> 2
size cluster i: 89 -> 2
datapoints in clusters: 999405
score: 89.1626


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:44:22 2019
elapsed time: 73.9083s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 7 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 6: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:265828] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:265828] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 91
datapoints_clusters: 999405
score: 89.1626
elapsed_time: 73.9083

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 500.0 percent_correct: 0.8320735294117647
-> new best_percent_correct:0.8320735294117647new best_threshold:500.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 666.6666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[9223,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 666.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2305s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.9584s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.0557s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38592
find clusters duration: 0.273893s
detected clusters: 98
size cluster i: 0 -> 10004
size cluster i: 1 -> 9969
size cluster i: 2 -> 20003
size cluster i: 3 -> 10001
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10001
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10006
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10003
size cluster i: 17 -> 20004
size cluster i: 18 -> 10005
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 10003
size cluster i: 24 -> 20008
size cluster i: 25 -> 10006
size cluster i: 26 -> 10003
size cluster i: 27 -> 10002
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9202
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10002
size cluster i: 34 -> 10000
size cluster i: 35 -> 10003
size cluster i: 36 -> 10002
size cluster i: 37 -> 10002
size cluster i: 38 -> 10002
size cluster i: 39 -> 10004
size cluster i: 40 -> 10003
size cluster i: 41 -> 9427
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9298
size cluster i: 46 -> 10002
size cluster i: 47 -> 20006
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10005
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 20010
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10005
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10004
size cluster i: 65 -> 10002
size cluster i: 66 -> 10002
size cluster i: 67 -> 20009
size cluster i: 68 -> 10003
size cluster i: 69 -> 10003
size cluster i: 70 -> 10002
size cluster i: 71 -> 10003
size cluster i: 72 -> 10004
size cluster i: 73 -> 10000
size cluster i: 74 -> 10005
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10003
size cluster i: 80 -> 10001
size cluster i: 81 -> 10001
size cluster i: 82 -> 10004
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10001
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 6149
size cluster i: 89 -> 10002
size cluster i: 90 -> 9993
size cluster i: 91 -> 9782
size cluster i: 92 -> 10000
size cluster i: 93 -> 2
size cluster i: 94 -> 4
size cluster i: 95 -> 2
datapoints in clusters: 994027
score: 95.5046


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:45:46 2019
elapsed time: 73.8617s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 4: Exiting... 
Node 6: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:266169] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:266169] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 994027
score: 95.5046
elapsed_time: 73.8617

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 666.666666667 percent_correct: 0.9251137254901961
-> new best_percent_correct:0.9251137254901961new best_threshold:666.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 833.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[15033,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 833.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1521s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.9251s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.99061s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38231
find clusters duration: 0.278207s
detected clusters: 95
size cluster i: 0 -> 10004
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9973
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10005
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10001
size cluster i: 16 -> 20002
size cluster i: 17 -> 10002
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10003
size cluster i: 23 -> 20004
size cluster i: 24 -> 10003
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10005
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 10001
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 7060
size cluster i: 43 -> 10001
size cluster i: 44 -> 20004
size cluster i: 45 -> 10000
size cluster i: 46 -> 10001
size cluster i: 47 -> 10001
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10002
size cluster i: 51 -> 10001
size cluster i: 52 -> 10003
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 20004
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10003
size cluster i: 59 -> 10003
size cluster i: 60 -> 10000
size cluster i: 61 -> 10003
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10003
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10000
size cluster i: 71 -> 10004
size cluster i: 72 -> 7560
size cluster i: 73 -> 10001
size cluster i: 74 -> 10003
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 9635
size cluster i: 79 -> 10002
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 7839
size cluster i: 83 -> 10003
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10002
size cluster i: 90 -> 9781
size cluster i: 91 -> 8814
size cluster i: 92 -> 10000
size cluster i: 93 -> 2786
size cluster i: 94 -> 3
datapoints in clusters: 983575
score: 91.6075


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:47:09 2019
elapsed time: 73.6856s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:266503] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:266503] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 95
datapoints_clusters: 983575
score: 91.6075
elapsed_time: 73.6856

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 833.333333333 percent_correct: 0.9248235294117647

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 1000.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[14838,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 1000


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0614s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.8466s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.08836s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37898
find clusters duration: 0.269279s
detected clusters: 102
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9735
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10004
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9992
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 20003
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 4051
size cluster i: 44 -> 10001
size cluster i: 45 -> 20003
size cluster i: 46 -> 9983
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20003
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10002
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 8534
size cluster i: 79 -> 10002
size cluster i: 80 -> 5387
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10003
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 8987
size cluster i: 91 -> 5503
size cluster i: 92 -> 7063
size cluster i: 93 -> 10000
size cluster i: 94 -> 780
size cluster i: 95 -> 2
size cluster i: 96 -> 3
size cluster i: 97 -> 2
size cluster i: 99 -> 2
size cluster i: 101 -> 2
datapoints in clusters: 970116
score: 93.2072


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:48:33 2019
elapsed time: 73.6088s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:266824] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:266824] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 102
datapoints_clusters: 970116
score: 93.2072
elapsed_time: 73.6088

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 1000.0 percent_correct: 0.9214970588235294

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 1166.6666666666665 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[14370,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 1166.67


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2099s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.9218s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.02165s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37435
find clusters duration: 0.262124s
detected clusters: 107
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 8941
size cluster i: 8 -> 10001
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10002
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 9950
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 20003
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10002
size cluster i: 44 -> 1664
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9818
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10004
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10000
size cluster i: 70 -> 10000
size cluster i: 71 -> 10001
size cluster i: 72 -> 9999
size cluster i: 73 -> 10002
size cluster i: 74 -> 10001
size cluster i: 75 -> 10001
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 6880
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 7498
size cluster i: 92 -> 3284
size cluster i: 93 -> 4975
size cluster i: 94 -> 10000
size cluster i: 95 -> 3314
size cluster i: 96 -> 107
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 101 -> 4
size cluster i: 102 -> 3
size cluster i: 103 -> 2
size cluster i: 104 -> 2
size cluster i: 106 -> 2
datapoints in clusters: 956511
score: 87.2113


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:49:57 2019
elapsed time: 73.7649s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:267164] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:267164] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 107
datapoints_clusters: 956511
score: 87.2113
elapsed_time: 73.7649

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 1166.66666667 percent_correct: 0.9278127450980392
-> new best_percent_correct:0.9278127450980392new best_threshold:1166.66666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 1333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[16202,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 1333.33


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.3121s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.9424s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.94051s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 36942
find clusters duration: 0.263362s
detected clusters: 113
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 9998
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 9716
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10000
size cluster i: 22 -> 10002
size cluster i: 23 -> 10000
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10000
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 9997
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10003
size cluster i: 38 -> 10001
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10001
size cluster i: 43 -> 10000
size cluster i: 44 -> 20002
size cluster i: 45 -> 9237
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10000
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10001
size cluster i: 59 -> 10000
size cluster i: 60 -> 10000
size cluster i: 61 -> 10001
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10004
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10000
size cluster i: 68 -> 10001
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 9994
size cluster i: 72 -> 10001
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 4906
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 9979
size cluster i: 89 -> 7699
size cluster i: 90 -> 10000
size cluster i: 91 -> 5644
size cluster i: 92 -> 3064
size cluster i: 93 -> 10000
size cluster i: 94 -> 433
size cluster i: 95 -> 1628
size cluster i: 96 -> 1733
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 99 -> 3
size cluster i: 100 -> 2
size cluster i: 101 -> 3
size cluster i: 102 -> 2
size cluster i: 104 -> 2
size cluster i: 105 -> 2
size cluster i: 107 -> 2
size cluster i: 108 -> 2
size cluster i: 110 -> 2
datapoints in clusters: 944101
score: 80.5263


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:51:20 2019
elapsed time: 73.8009s


Finishing: 
---------- 
Beginning cleanup...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Node 2: Exiting... 
Cleanup done!
Node 1: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:267508] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:267508] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 113
datapoints_clusters: 944101
score: 80.5263
elapsed_time: 73.8009

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 1333.33333333 percent_correct: 0.9254764705882353

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 1500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[15868,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 1500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1146s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.9703s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.04497s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 36550
find clusters duration: 0.249829s
detected clusters: 126
size cluster i: 0 -> 10001
size cluster i: 1 -> 9999
size cluster i: 2 -> 10000
size cluster i: 3 -> 9971
size cluster i: 4 -> 10000
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 9212
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 9998
size cluster i: 20 -> 10000
size cluster i: 21 -> 10000
size cluster i: 22 -> 10002
size cluster i: 23 -> 10000
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10000
size cluster i: 27 -> 10000
size cluster i: 28 -> 10000
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10000
size cluster i: 33 -> 9968
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10002
size cluster i: 38 -> 10001
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10001
size cluster i: 43 -> 10000
size cluster i: 44 -> 20002
size cluster i: 45 -> 8092
size cluster i: 46 -> 10000
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 9990
size cluster i: 59 -> 10001
size cluster i: 60 -> 10000
size cluster i: 61 -> 10000
size cluster i: 62 -> 10000
size cluster i: 63 -> 10001
size cluster i: 64 -> 10000
size cluster i: 65 -> 10003
size cluster i: 66 -> 10001
size cluster i: 67 -> 10001
size cluster i: 68 -> 10000
size cluster i: 69 -> 10001
size cluster i: 70 -> 10000
size cluster i: 71 -> 10000
size cluster i: 72 -> 9960
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 10001
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 3006
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 9863
size cluster i: 90 -> 10000
size cluster i: 91 -> 6047
size cluster i: 92 -> 3758
size cluster i: 93 -> 1593
size cluster i: 94 -> 10000
size cluster i: 95 -> 661
size cluster i: 96 -> 768
size cluster i: 97 -> 7
size cluster i: 98 -> 25
size cluster i: 99 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 24
size cluster i: 102 -> 2
size cluster i: 104 -> 2
size cluster i: 105 -> 2
size cluster i: 106 -> 3
size cluster i: 107 -> 2
size cluster i: 108 -> 2
size cluster i: 109 -> 2
size cluster i: 110 -> 3
size cluster i: 111 -> 3
size cluster i: 112 -> 3
size cluster i: 113 -> 2
size cluster i: 116 -> 2
size cluster i: 117 -> 2
datapoints in clusters: 933019
score: 67.6896


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:52:44 2019
elapsed time: 73.7257s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:267842] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:267842] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 126
datapoints_clusters: 933019
score: 67.6896
elapsed_time: 73.7257

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 1500.0 percent_correct: 0.9243921568627451
thresholds:[ 1037.03703704  1074.07407407  1111.11111111  1148.14814815  1185.18518519
  1222.22222222  1259.25925926  1296.2962963 ], threshold_step:37.037037037

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 1037.037037037037 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[15417,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 1037.04


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1821s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.9664s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.09396s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37791
find clusters duration: 0.269796s
detected clusters: 100
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9623
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10004
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9991
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 20003
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 3439
size cluster i: 44 -> 10001
size cluster i: 45 -> 20003
size cluster i: 46 -> 9969
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 20003
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10001
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 8232
size cluster i: 79 -> 10001
size cluster i: 80 -> 4910
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10002
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 8669
size cluster i: 91 -> 5006
size cluster i: 92 -> 6609
size cluster i: 93 -> 10000
size cluster i: 94 -> 543
size cluster i: 97 -> 3
size cluster i: 98 -> 2
size cluster i: 99 -> 3
datapoints in clusters: 967083
score: 94.8121


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:54:08 2019
elapsed time: 73.8418s


Finishing: 
---------- 
Beginning cleanup...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 2: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with theWarning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

value given in the MPI config!


[argon-gtx:268167] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:268167] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 100
datapoints_clusters: 967083
score: 94.8121
elapsed_time: 73.8418

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 1037.03703704 percent_correct: 0.9185382352941176

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 1074.074074074074 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[13173,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 1074.07


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1194s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.839s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.98638s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37729
find clusters duration: 0.272475s
detected clusters: 106
size cluster i: 0 -> 10001
size cluster i: 1 -> 10001
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9477
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 9984
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 20003
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10002
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 2844
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9935
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10002
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 20003
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10002
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10005
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10001
size cluster i: 72 -> 10000
size cluster i: 73 -> 10002
size cluster i: 74 -> 10001
size cluster i: 75 -> 10001
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 7848
size cluster i: 80 -> 10001
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 8369
size cluster i: 91 -> 4441
size cluster i: 92 -> 4443
size cluster i: 93 -> 6174
size cluster i: 94 -> 10000
size cluster i: 95 -> 364
size cluster i: 96 -> 4
size cluster i: 97 -> 5
size cluster i: 100 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 2
size cluster i: 104 -> 2
datapoints in clusters: 963980
score: 88.8374


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:55:31 2019
elapsed time: 73.5777s


Finishing: 
---------- 
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Node 8: Exiting... 
Cleanup done!
Node 2: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:268491] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:268491] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 106
datapoints_clusters: 963980
score: 88.8374
elapsed_time: 73.5777

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 1074.07407407 percent_correct: 0.9252931372549019

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 1111.111111111111 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[12716,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 1111.11


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1826s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.91s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.07429s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37561
find clusters duration: 0.267057s
detected clusters: 106
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9300
size cluster i: 8 -> 10001
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 9973
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 20003
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10001
size cluster i: 40 -> 10002
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 2326
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9892
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10002
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10004
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10001
size cluster i: 70 -> 10000
size cluster i: 71 -> 10001
size cluster i: 72 -> 9999
size cluster i: 73 -> 10002
size cluster i: 74 -> 10001
size cluster i: 75 -> 10001
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 7480
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 8024
size cluster i: 92 -> 3978
size cluster i: 93 -> 3971
size cluster i: 94 -> 5708
size cluster i: 95 -> 10000
size cluster i: 96 -> 242
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 100 -> 5
size cluster i: 101 -> 2
size cluster i: 102 -> 3
size cluster i: 103 -> 2
datapoints in clusters: 960980
score: 88.5609


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:56:55 2019
elapsed time: 73.7717s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 1: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:268818] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:268818] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 106
datapoints_clusters: 960980
score: 88.5609
elapsed_time: 73.7717

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 1111.11111111 percent_correct: 0.9321823529411765
-> new best_percent_correct:0.9321823529411765new best_threshold:1111.11111111

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 1148.148148148148 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[12527,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 1148.15


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.116s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.9551s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.93368s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37477
find clusters duration: 0.265132s
detected clusters: 109
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9068
size cluster i: 8 -> 10001
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10002
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 9956
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 20003
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10002
size cluster i: 44 -> 1860
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9847
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10004
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10000
size cluster i: 70 -> 10000
size cluster i: 71 -> 10001
size cluster i: 72 -> 9999
size cluster i: 73 -> 10002
size cluster i: 74 -> 10001
size cluster i: 75 -> 10001
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 7085
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 7689
size cluster i: 92 -> 3527
size cluster i: 93 -> 3494
size cluster i: 94 -> 5221
size cluster i: 95 -> 10000
size cluster i: 96 -> 146
size cluster i: 97 -> 7
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 101 -> 2
size cluster i: 104 -> 3
size cluster i: 105 -> 2
size cluster i: 106 -> 3
size cluster i: 108 -> 2
datapoints in clusters: 957980
score: 85.4668


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:58:18 2019
elapsed time: 73.6237s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5: Exiting... 
Node 3: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Node 7: Exiting... 
Node 4: Exiting... 
Cleanup done!
Node 6: Exiting... 
Node 8: Exiting... 
Node 1: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:269137] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:269137] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 109
datapoints_clusters: 957980
score: 85.4668
elapsed_time: 73.6237

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 1148.14814815 percent_correct: 0.9292460784313725

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 1185.185185185185 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[14120,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 1185.19


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1702s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.8947s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.9573s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37356
find clusters duration: 0.258456s
detected clusters: 114
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 8836
size cluster i: 8 -> 10001
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10001
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 9930
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10001
size cluster i: 44 -> 1461
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9781
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10004
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10001
size cluster i: 73 -> 9999
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 6652
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 9998
size cluster i: 91 -> 10001
size cluster i: 92 -> 7292
size cluster i: 93 -> 3095
size cluster i: 94 -> 4758
size cluster i: 95 -> 10000
size cluster i: 96 -> 3069
size cluster i: 97 -> 2
size cluster i: 98 -> 71
size cluster i: 99 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 3
size cluster i: 104 -> 3
size cluster i: 105 -> 2
size cluster i: 106 -> 3
size cluster i: 108 -> 5
size cluster i: 110 -> 2
size cluster i: 111 -> 2
datapoints in clusters: 955031
score: 80.5222


Runtimes: 
--------- 
finished computation at Thu Jan  3 15:59:42 2019
elapsed time: 73.6291s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 8: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:269462] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:269462] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 114
datapoints_clusters: 955031
score: 80.5222
elapsed_time: 73.6291

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 1185.18518519 percent_correct: 0.9361617647058823
-> new best_percent_correct:0.9361617647058823new best_threshold:1185.18518519

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 1222.2222222222222 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[13910,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 1222.22


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2709s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.982s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.9855s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37254
find clusters duration: 0.262921s
detected clusters: 111
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 8593
size cluster i: 8 -> 10001
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10001
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 9889
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10000
size cluster i: 22 -> 10000
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 9999
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10001
size cluster i: 44 -> 10001
size cluster i: 45 -> 20003
size cluster i: 46 -> 9670
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10004
size cluster i: 66 -> 10001
size cluster i: 67 -> 10001
size cluster i: 68 -> 10000
size cluster i: 69 -> 10001
size cluster i: 70 -> 10000
size cluster i: 71 -> 10001
size cluster i: 72 -> 9999
size cluster i: 73 -> 10002
size cluster i: 74 -> 10001
size cluster i: 75 -> 10001
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 6229
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 9997
size cluster i: 90 -> 10001
size cluster i: 91 -> 6886
size cluster i: 92 -> 4280
size cluster i: 93 -> 10000
size cluster i: 94 -> 1114
size cluster i: 95 -> 2681
size cluster i: 96 -> 2692
size cluster i: 97 -> 48
size cluster i: 98 -> 3
size cluster i: 99 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 2
size cluster i: 103 -> 3
size cluster i: 104 -> 3
size cluster i: 106 -> 3
datapoints in clusters: 952155
score: 83.0802


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:01:06 2019
elapsed time: 73.8427s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 3: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:269800] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:269800] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 111
datapoints_clusters: 952155
score: 83.0802
elapsed_time: 73.8427

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 1222.22222222 percent_correct: 0.9333558823529412

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 1259.2592592592591 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[13450,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 1259.26


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1562s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.9182s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.96072s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37137
find clusters duration: 0.258409s
detected clusters: 118
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 8352
size cluster i: 8 -> 10001
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10001
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 9842
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10000
size cluster i: 22 -> 10000
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 9999
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10001
size cluster i: 44 -> 10000
size cluster i: 45 -> 20002
size cluster i: 46 -> 9543
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10000
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10004
size cluster i: 66 -> 10001
size cluster i: 67 -> 10001
size cluster i: 68 -> 10000
size cluster i: 69 -> 10001
size cluster i: 70 -> 10000
size cluster i: 71 -> 10001
size cluster i: 72 -> 9998
size cluster i: 73 -> 10002
size cluster i: 74 -> 10001
size cluster i: 75 -> 10001
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 5769
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 9994
size cluster i: 90 -> 10000
size cluster i: 91 -> 6477
size cluster i: 92 -> 3842
size cluster i: 93 -> 10000
size cluster i: 94 -> 828
size cluster i: 95 -> 2289
size cluster i: 96 -> 2334
size cluster i: 97 -> 3
size cluster i: 98 -> 3
size cluster i: 99 -> 5
size cluster i: 101 -> 4
size cluster i: 102 -> 6
size cluster i: 103 -> 4
size cluster i: 104 -> 4
size cluster i: 105 -> 7
size cluster i: 109 -> 2
size cluster i: 110 -> 3
size cluster i: 111 -> 2
size cluster i: 112 -> 3
size cluster i: 117 -> 2
datapoints in clusters: 949373
score: 76.3221


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:02:29 2019
elapsed time: 73.6436s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 8: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:270132] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:270132] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 118
datapoints_clusters: 949373
score: 76.3221
elapsed_time: 73.6436

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 1259.25925926 percent_correct: 0.9306117647058824

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=3.16227766016838e-07 --k 6 --epsilon 0.001 --threshold 1296.296296296296 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[3028,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 3.16228e-07
k: 6
threshold: 1296.3


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0755s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.022807
delta: 0.0539175
delta: 0.0434656
delta: 0.030436
delta: 0.0346982
delta: 0.0525153
delta: 0.0320233
delta: 0.0221098
delta: 0.0309481
delta: 0.01205
delta: 0.0533502
delta: 0.0423547
delta: 0.0141201
delta: 0.005538
delta: 0.00722044
delta: 0.0220332
delta: 0.00650071
delta: 0.00547276
delta: 0.0117182
delta: 0.00823125
delta: 0.00218818
delta: 0.0311124
delta: 0.00314996
delta: 0.00613802
delta: 0.00128026
delta: 0.00265228
delta: 0.0129397
delta: 0.0010963
delta: 0.00206846
delta: 0.000599659
delta: 0.000363774
delta: 0.00485572
delta: 0.000326672
delta: 0.00055851
delta: 0.00127419
delta: 0.000214542
delta: 0.00276277
delta: 0.000233938
delta: 0.000173047
delta: 9.70132e-05
delta: 7.28934e-05
delta: 0.00046027
delta: 0.000156099
delta: 0.000370707
delta: 3.12194e-05
delta: 6.50739e-05
delta: 0.000202319
delta: 3.26549e-05
delta: 8.34846e-05
delta: 2.44133e-05
delta: 3.20182e-05
delta: 2.55607e-05
delta: 0.000217354
delta: 1.05339e-05
delta: 4.47689e-05
delta: 4.8677e-06
delta: 7.76048e-06
delta: 1.93081e-05
delta: 5.86503e-06
delta: 7.45994e-06
delta: 1.05852e-05
delta: 7.65514e-06
delta: 8.71495e-06
delta: 2.68376e-06
delta: 7.67961e-06
delta: 7.25031e-06
delta: 3.83354e-06
delta: 8.36162e-07
delta: 8.10438e-07
delta: 7.36929e-06
delta: 4.11229e-06
delta: 4.11272e-06
delta: 5.57331e-07
delta: 5.83078e-07
delta: 4.35197e-06
delta: 1.39181e-06
delta: 5.46238e-07
delta: 6.81095e-07
delta: 3.42484e-07
delta: 1.19113e-06
delta: 2.15831e-07
delta: 1.87419e-07
delta: 3.36802e-07
delta: 4.52124e-07
delta: 5.40583e-07
delta: 1.54272e-07
delta: 2.53571e-07
delta: 1.74752e-07
delta: 9.46151e-08
delta: 4.00511e-08
Number of iterations: 90 (max. 1000)
Final norm of residuum: 4.00511e-08
counted_mult_calls: 92
solver duration: 50.8973s
Number of grid points:     397825
alpha min: -1175.82 max: 3764.06
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.95145s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37032
find clusters duration: 0.257826s
detected clusters: 115
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 9999
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 9784
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10000
size cluster i: 22 -> 10002
size cluster i: 23 -> 10000
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10000
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 9998
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10003
size cluster i: 38 -> 10001
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10000
size cluster i: 42 -> 10001
size cluster i: 43 -> 10000
size cluster i: 44 -> 20002
size cluster i: 45 -> 9400
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10000
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10001
size cluster i: 59 -> 10001
size cluster i: 60 -> 10000
size cluster i: 61 -> 10001
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10004
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10000
size cluster i: 68 -> 10001
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 9996
size cluster i: 72 -> 10001
size cluster i: 73 -> 10001
size cluster i: 74 -> 10001
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 5332
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10001
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 9990
size cluster i: 89 -> 8047
size cluster i: 90 -> 10000
size cluster i: 91 -> 6014
size cluster i: 92 -> 3408
size cluster i: 93 -> 10000
size cluster i: 94 -> 609
size cluster i: 95 -> 1934
size cluster i: 96 -> 2014
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 2
size cluster i: 102 -> 5
size cluster i: 103 -> 3
size cluster i: 104 -> 3
size cluster i: 105 -> 2
size cluster i: 106 -> 3
size cluster i: 107 -> 2
size cluster i: 108 -> 4
size cluster i: 110 -> 2
datapoints in clusters: 946612
score: 78.8843


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:03:53 2019
elapsed time: 73.531s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 1: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:270442] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:270442] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 115
datapoints_clusters: 946612
score: 78.8843
elapsed_time: 73.531

--------------- END RUN -----------------
attempt lambda: 3.16227766016838e-07 threshold: 1296.2962963 percent_correct: 0.9279235294117647
lambda_factor: 1.7782794100389228 lambda_start_lower:3.162277660168379e-08 overall_best_lambda_value: 1e-07
lambda_values:[5.6234132519034905e-08, 9.999999999999998e-08, 1.7782794100389227e-07]
thresholds:[0.0, 166.66666666666666, 333.33333333333331, 500.0, 666.66666666666663, 833.33333333333326, 1000.0, 1166.6666666666665, 1333.3333333333333, 1500.0], threshold_step:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 0.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[2677,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 0


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0953s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 113.806s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.93444s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37747
find clusters duration: 64.0887s
detected clusters: 1
size cluster i: 0 -> 1020000
datapoints in clusters: 1020000
score: 1


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:07:23 2019
elapsed time: 200.26s


Finishing: 
---------- 
Beginning cleanup...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 7: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Node 5: Exiting... 
Node 3: Exiting... 
Cleanup done!
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:270795] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:270795] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 1
datapoints_clusters: 1020000
score: 1.0
elapsed_time: 200.26

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 0.0 percent_correct: 0.00980392156862745
-> new best_percent_correct:0.00980392156862745new best_threshold:0.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 166.66666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[2202,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 166.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1064s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 113.449s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.00943s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39411
find clusters duration: 6.35701s
detected clusters: 388
size cluster i: 0 -> 581037
size cluster i: 1 -> 10019
size cluster i: 2 -> 40059
size cluster i: 3 -> 10024
size cluster i: 4 -> 30058
size cluster i: 5 -> 10011
size cluster i: 6 -> 10014
size cluster i: 7 -> 10017
size cluster i: 8 -> 10006
size cluster i: 9 -> 10021
size cluster i: 10 -> 10004
size cluster i: 11 -> 10010
size cluster i: 12 -> 10012
size cluster i: 13 -> 10025
size cluster i: 14 -> 10014
size cluster i: 15 -> 10019
size cluster i: 16 -> 10018
size cluster i: 17 -> 10011
size cluster i: 18 -> 30062
size cluster i: 19 -> 10006
size cluster i: 20 -> 10011
size cluster i: 21 -> 10017
size cluster i: 22 -> 20028
size cluster i: 23 -> 10015
size cluster i: 24 -> 10011
size cluster i: 25 -> 10008
size cluster i: 26 -> 10014
size cluster i: 27 -> 10014
size cluster i: 28 -> 10017
size cluster i: 29 -> 10021
size cluster i: 30 -> 10021
size cluster i: 31 -> 10010
size cluster i: 32 -> 10002
size cluster i: 33 -> 10005
size cluster i: 34 -> 10013
size cluster i: 35 -> 2
size cluster i: 36 -> 3
size cluster i: 37 -> 2
size cluster i: 38 -> 2
size cluster i: 39 -> 2
size cluster i: 40 -> 2
size cluster i: 41 -> 3
size cluster i: 42 -> 2
size cluster i: 43 -> 4
size cluster i: 44 -> 2
size cluster i: 45 -> 2
size cluster i: 46 -> 2
size cluster i: 47 -> 3
size cluster i: 48 -> 5
size cluster i: 49 -> 4
size cluster i: 50 -> 5
size cluster i: 51 -> 2
size cluster i: 52 -> 5
size cluster i: 53 -> 3
size cluster i: 54 -> 2
size cluster i: 55 -> 4
size cluster i: 56 -> 2
size cluster i: 57 -> 3
size cluster i: 58 -> 3
size cluster i: 59 -> 3
size cluster i: 60 -> 2
size cluster i: 61 -> 2
size cluster i: 62 -> 6
size cluster i: 63 -> 4
size cluster i: 64 -> 4
size cluster i: 65 -> 9
size cluster i: 66 -> 2
size cluster i: 67 -> 2
size cluster i: 68 -> 2
size cluster i: 69 -> 6
size cluster i: 70 -> 3
size cluster i: 71 -> 7
size cluster i: 72 -> 3
size cluster i: 73 -> 2
size cluster i: 74 -> 4
size cluster i: 75 -> 2
size cluster i: 76 -> 3
size cluster i: 77 -> 3
size cluster i: 78 -> 3
size cluster i: 80 -> 5
size cluster i: 81 -> 2
size cluster i: 82 -> 6
size cluster i: 83 -> 2
size cluster i: 84 -> 4
size cluster i: 85 -> 3
size cluster i: 87 -> 2
size cluster i: 88 -> 2
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 2
size cluster i: 95 -> 3
size cluster i: 96 -> 2
size cluster i: 97 -> 3
size cluster i: 98 -> 4
size cluster i: 99 -> 3
size cluster i: 100 -> 5
size cluster i: 101 -> 4
size cluster i: 102 -> 3
size cluster i: 103 -> 3
size cluster i: 104 -> 9
size cluster i: 105 -> 2
size cluster i: 106 -> 5
size cluster i: 107 -> 2
size cluster i: 108 -> 4
size cluster i: 110 -> 5
size cluster i: 111 -> 3
size cluster i: 112 -> 5
size cluster i: 113 -> 2
size cluster i: 114 -> 2
size cluster i: 115 -> 3
size cluster i: 116 -> 4
size cluster i: 117 -> 2
size cluster i: 118 -> 2
size cluster i: 119 -> 3
size cluster i: 121 -> 3
size cluster i: 122 -> 2
size cluster i: 123 -> 4
size cluster i: 124 -> 2
size cluster i: 125 -> 3
size cluster i: 128 -> 5
size cluster i: 129 -> 5
size cluster i: 130 -> 2
size cluster i: 131 -> 3
size cluster i: 132 -> 6
size cluster i: 133 -> 3
size cluster i: 134 -> 2
size cluster i: 135 -> 2
size cluster i: 136 -> 2
size cluster i: 137 -> 3
size cluster i: 138 -> 2
size cluster i: 139 -> 3
size cluster i: 142 -> 2
size cluster i: 143 -> 3
size cluster i: 144 -> 5
size cluster i: 145 -> 4
size cluster i: 146 -> 2
size cluster i: 148 -> 2
size cluster i: 149 -> 2
size cluster i: 150 -> 2
size cluster i: 151 -> 4
size cluster i: 152 -> 2
size cluster i: 153 -> 3
size cluster i: 154 -> 2
size cluster i: 155 -> 2
size cluster i: 157 -> 3
size cluster i: 158 -> 2
size cluster i: 159 -> 2
size cluster i: 160 -> 3
size cluster i: 161 -> 2
size cluster i: 162 -> 4
size cluster i: 163 -> 3
size cluster i: 164 -> 3
size cluster i: 165 -> 2
size cluster i: 166 -> 3
size cluster i: 167 -> 2
size cluster i: 168 -> 3
size cluster i: 169 -> 2
size cluster i: 170 -> 3
size cluster i: 171 -> 2
size cluster i: 172 -> 3
size cluster i: 173 -> 2
size cluster i: 174 -> 2
size cluster i: 176 -> 2
size cluster i: 177 -> 3
size cluster i: 178 -> 2
size cluster i: 179 -> 2
size cluster i: 180 -> 3
size cluster i: 181 -> 2
size cluster i: 182 -> 2
size cluster i: 183 -> 3
size cluster i: 184 -> 3
size cluster i: 185 -> 4
size cluster i: 186 -> 2
size cluster i: 187 -> 2
size cluster i: 189 -> 3
size cluster i: 190 -> 2
size cluster i: 191 -> 2
size cluster i: 193 -> 2
size cluster i: 194 -> 3
size cluster i: 197 -> 2
size cluster i: 198 -> 3
size cluster i: 199 -> 2
size cluster i: 200 -> 2
size cluster i: 201 -> 2
size cluster i: 202 -> 2
size cluster i: 203 -> 2
size cluster i: 204 -> 3
size cluster i: 205 -> 2
size cluster i: 206 -> 2
size cluster i: 207 -> 2
size cluster i: 208 -> 2
size cluster i: 209 -> 2
size cluster i: 210 -> 3
size cluster i: 211 -> 2
size cluster i: 212 -> 2
size cluster i: 213 -> 2
size cluster i: 214 -> 2
size cluster i: 215 -> 2
size cluster i: 216 -> 2
size cluster i: 217 -> 5
size cluster i: 218 -> 2
size cluster i: 219 -> 2
size cluster i: 222 -> 4
size cluster i: 223 -> 2
size cluster i: 224 -> 5
size cluster i: 226 -> 4
size cluster i: 227 -> 2
size cluster i: 230 -> 2
size cluster i: 231 -> 2
size cluster i: 232 -> 3
size cluster i: 233 -> 2
size cluster i: 234 -> 3
size cluster i: 236 -> 2
size cluster i: 238 -> 3
size cluster i: 239 -> 2
size cluster i: 240 -> 3
size cluster i: 241 -> 2
size cluster i: 242 -> 4
size cluster i: 243 -> 3
size cluster i: 247 -> 3
size cluster i: 248 -> 4
size cluster i: 250 -> 3
size cluster i: 251 -> 2
size cluster i: 253 -> 2
size cluster i: 255 -> 2
size cluster i: 256 -> 3
size cluster i: 259 -> 2
size cluster i: 260 -> 3
size cluster i: 261 -> 2
size cluster i: 263 -> 2
size cluster i: 264 -> 4
size cluster i: 265 -> 2
size cluster i: 266 -> 2
size cluster i: 267 -> 2
size cluster i: 268 -> 3
size cluster i: 270 -> 2
size cluster i: 271 -> 2
size cluster i: 272 -> 2
size cluster i: 273 -> 2
size cluster i: 275 -> 2
size cluster i: 276 -> 2
size cluster i: 277 -> 2
size cluster i: 278 -> 2
size cluster i: 283 -> 3
size cluster i: 284 -> 2
size cluster i: 286 -> 4
size cluster i: 290 -> 3
size cluster i: 295 -> 2
size cluster i: 296 -> 2
size cluster i: 297 -> 2
size cluster i: 298 -> 2
size cluster i: 299 -> 2
size cluster i: 301 -> 3
size cluster i: 302 -> 2
size cluster i: 305 -> 2
size cluster i: 306 -> 4
size cluster i: 307 -> 4
size cluster i: 308 -> 2
size cluster i: 309 -> 2
size cluster i: 313 -> 3
size cluster i: 315 -> 2
size cluster i: 317 -> 2
size cluster i: 320 -> 2
size cluster i: 321 -> 2
size cluster i: 324 -> 2
size cluster i: 325 -> 2
size cluster i: 326 -> 4
size cluster i: 328 -> 2
size cluster i: 329 -> 3
size cluster i: 330 -> 2
size cluster i: 332 -> 2
size cluster i: 334 -> 2
size cluster i: 337 -> 2
size cluster i: 340 -> 2
size cluster i: 343 -> 2
size cluster i: 350 -> 2
size cluster i: 355 -> 2
size cluster i: 358 -> 2
size cluster i: 359 -> 2
size cluster i: 364 -> 2
size cluster i: 365 -> 2
size cluster i: 368 -> 2
size cluster i: 370 -> 2
size cluster i: 380 -> 2
size cluster i: 384 -> 2
datapoints in clusters: 1002444
score: 0


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:09:56 2019
elapsed time: 142.263s


Finishing: 
---------- 
Beginning cleanup...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 4: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 5: Exiting... 
Node 8: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:271140] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:271140] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 388
datapoints_clusters: 1002444
score: 0.0
elapsed_time: 142.263

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 166.666666667 percent_correct: 0.36034901960784316
-> new best_percent_correct:0.36034901960784316new best_threshold:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[4038,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 333.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1742s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 113.835s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.03753s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39055
find clusters duration: 0.330441s
detected clusters: 172
size cluster i: 0 -> 60063
size cluster i: 1 -> 10008
size cluster i: 2 -> 20020
size cluster i: 3 -> 30027
size cluster i: 4 -> 10002
size cluster i: 5 -> 10008
size cluster i: 6 -> 20021
size cluster i: 7 -> 10012
size cluster i: 8 -> 10007
size cluster i: 9 -> 10009
size cluster i: 10 -> 10008
size cluster i: 11 -> 10009
size cluster i: 12 -> 40025
size cluster i: 13 -> 10007
size cluster i: 14 -> 10006
size cluster i: 15 -> 10001
size cluster i: 16 -> 10010
size cluster i: 17 -> 60058
size cluster i: 18 -> 10012
size cluster i: 19 -> 10003
size cluster i: 20 -> 10004
size cluster i: 21 -> 20024
size cluster i: 22 -> 10004
size cluster i: 23 -> 10007
size cluster i: 24 -> 10014
size cluster i: 25 -> 10007
size cluster i: 26 -> 10008
size cluster i: 27 -> 10005
size cluster i: 28 -> 20019
size cluster i: 29 -> 10009
size cluster i: 30 -> 10009
size cluster i: 31 -> 10004
size cluster i: 32 -> 30031
size cluster i: 33 -> 20011
size cluster i: 34 -> 10009
size cluster i: 35 -> 20019
size cluster i: 36 -> 10003
size cluster i: 37 -> 10005
size cluster i: 38 -> 50052
size cluster i: 39 -> 10005
size cluster i: 40 -> 10003
size cluster i: 41 -> 10007
size cluster i: 42 -> 20015
size cluster i: 43 -> 10005
size cluster i: 44 -> 10009
size cluster i: 45 -> 10006
size cluster i: 46 -> 10007
size cluster i: 47 -> 10004
size cluster i: 48 -> 10007
size cluster i: 49 -> 10008
size cluster i: 50 -> 10009
size cluster i: 51 -> 10007
size cluster i: 52 -> 10002
size cluster i: 53 -> 10008
size cluster i: 54 -> 10006
size cluster i: 55 -> 10011
size cluster i: 56 -> 10006
size cluster i: 57 -> 10015
size cluster i: 58 -> 10002
size cluster i: 59 -> 10020
size cluster i: 60 -> 10010
size cluster i: 61 -> 10005
size cluster i: 62 -> 10007
size cluster i: 63 -> 10003
size cluster i: 64 -> 10000
size cluster i: 65 -> 10007
size cluster i: 66 -> 10008
size cluster i: 67 -> 10012
size cluster i: 68 -> 10002
size cluster i: 69 -> 10004
size cluster i: 70 -> 10001
size cluster i: 71 -> 10009
size cluster i: 72 -> 2
size cluster i: 73 -> 2
size cluster i: 74 -> 4
size cluster i: 75 -> 2
size cluster i: 76 -> 4
size cluster i: 77 -> 2
size cluster i: 78 -> 2
size cluster i: 79 -> 2
size cluster i: 80 -> 3
size cluster i: 81 -> 2
size cluster i: 82 -> 2
size cluster i: 83 -> 2
size cluster i: 84 -> 2
size cluster i: 85 -> 6
size cluster i: 86 -> 2
size cluster i: 87 -> 3
size cluster i: 88 -> 4
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 3
size cluster i: 92 -> 2
size cluster i: 93 -> 3
size cluster i: 94 -> 3
size cluster i: 95 -> 2
size cluster i: 96 -> 4
size cluster i: 97 -> 3
size cluster i: 98 -> 4
size cluster i: 100 -> 2
size cluster i: 101 -> 3
size cluster i: 103 -> 3
size cluster i: 104 -> 2
size cluster i: 105 -> 3
size cluster i: 106 -> 2
size cluster i: 108 -> 3
size cluster i: 109 -> 3
size cluster i: 111 -> 2
size cluster i: 113 -> 2
size cluster i: 114 -> 2
size cluster i: 116 -> 2
size cluster i: 117 -> 2
size cluster i: 119 -> 2
size cluster i: 120 -> 2
size cluster i: 121 -> 2
size cluster i: 123 -> 2
size cluster i: 125 -> 2
size cluster i: 126 -> 2
size cluster i: 128 -> 2
size cluster i: 129 -> 3
size cluster i: 130 -> 2
size cluster i: 131 -> 3
size cluster i: 132 -> 2
size cluster i: 135 -> 2
size cluster i: 137 -> 2
size cluster i: 138 -> 4
size cluster i: 141 -> 2
size cluster i: 142 -> 3
size cluster i: 144 -> 2
size cluster i: 145 -> 2
size cluster i: 146 -> 2
size cluster i: 149 -> 2
size cluster i: 151 -> 2
size cluster i: 158 -> 2
size cluster i: 163 -> 2
size cluster i: 168 -> 2
datapoints in clusters: 1000984
score: 27.478


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:12:23 2019
elapsed time: 136.709s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!



[argon-gtx:271480] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:271480] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 172
datapoints_clusters: 1000984
score: 27.478
elapsed_time: 136.709

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 333.333333333 percent_correct: 0.7245254901960785
-> new best_percent_correct:0.7245254901960785new best_threshold:333.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[3674,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2513s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 113.79s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.0527s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38904
find clusters duration: 0.293502s
detected clusters: 117
size cluster i: 0 -> 20017
size cluster i: 1 -> 10004
size cluster i: 2 -> 20014
size cluster i: 3 -> 20017
size cluster i: 4 -> 10001
size cluster i: 5 -> 10003
size cluster i: 6 -> 20012
size cluster i: 7 -> 10004
size cluster i: 8 -> 10004
size cluster i: 9 -> 10005
size cluster i: 10 -> 10005
size cluster i: 11 -> 10005
size cluster i: 12 -> 30018
size cluster i: 13 -> 10007
size cluster i: 14 -> 10003
size cluster i: 15 -> 10001
size cluster i: 16 -> 10006
size cluster i: 17 -> 40027
size cluster i: 18 -> 10009
size cluster i: 19 -> 10001
size cluster i: 20 -> 10002
size cluster i: 21 -> 10006
size cluster i: 22 -> 10004
size cluster i: 23 -> 10005
size cluster i: 24 -> 30022
size cluster i: 25 -> 10008
size cluster i: 26 -> 10005
size cluster i: 27 -> 10003
size cluster i: 28 -> 10003
size cluster i: 29 -> 20010
size cluster i: 30 -> 10007
size cluster i: 31 -> 10004
size cluster i: 32 -> 10004
size cluster i: 33 -> 10012
size cluster i: 34 -> 20010
size cluster i: 35 -> 10008
size cluster i: 36 -> 10006
size cluster i: 37 -> 10001
size cluster i: 38 -> 10005
size cluster i: 39 -> 10004
size cluster i: 40 -> 10003
size cluster i: 41 -> 10003
size cluster i: 42 -> 10004
size cluster i: 43 -> 20007
size cluster i: 44 -> 10002
size cluster i: 45 -> 10004
size cluster i: 46 -> 10007
size cluster i: 47 -> 10005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10006
size cluster i: 52 -> 10002
size cluster i: 53 -> 10004
size cluster i: 54 -> 20025
size cluster i: 55 -> 10005
size cluster i: 56 -> 10002
size cluster i: 57 -> 10007
size cluster i: 58 -> 10005
size cluster i: 59 -> 10004
size cluster i: 60 -> 10008
size cluster i: 61 -> 10006
size cluster i: 62 -> 20012
size cluster i: 63 -> 10008
size cluster i: 64 -> 10004
size cluster i: 65 -> 10004
size cluster i: 66 -> 10007
size cluster i: 67 -> 10008
size cluster i: 68 -> 10002
size cluster i: 69 -> 10014
size cluster i: 70 -> 10004
size cluster i: 71 -> 10005
size cluster i: 72 -> 10003
size cluster i: 73 -> 10006
size cluster i: 74 -> 10002
size cluster i: 75 -> 9999
size cluster i: 76 -> 10005
size cluster i: 77 -> 10008
size cluster i: 78 -> 10005
size cluster i: 79 -> 10001
size cluster i: 80 -> 10005
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10003
size cluster i: 84 -> 3
size cluster i: 85 -> 2
size cluster i: 86 -> 2
size cluster i: 87 -> 2
size cluster i: 88 -> 2
size cluster i: 89 -> 5
size cluster i: 90 -> 2
size cluster i: 91 -> 3
size cluster i: 92 -> 3
size cluster i: 93 -> 4
size cluster i: 94 -> 3
size cluster i: 95 -> 2
size cluster i: 97 -> 3
size cluster i: 99 -> 2
size cluster i: 100 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 2
size cluster i: 104 -> 2
size cluster i: 105 -> 4
size cluster i: 106 -> 2
size cluster i: 107 -> 2
size cluster i: 111 -> 2
size cluster i: 112 -> 2
size cluster i: 113 -> 2
datapoints in clusters: 1000581
score: 81.4198


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:14:50 2019
elapsed time: 136.72s


Finishing: 
---------- 
Beginning cleanup...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 6: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 2: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:271844] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:271844] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 117
datapoints_clusters: 1000581
score: 81.4198
elapsed_time: 136.72

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 500.0 percent_correct: 0.8425656862745098
-> new best_percent_correct:0.8425656862745098new best_threshold:500.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 666.6666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[3202,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 666.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2087s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 113.889s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.96642s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38798
find clusters duration: 0.295854s
detected clusters: 103
size cluster i: 0 -> 10005
size cluster i: 1 -> 10003
size cluster i: 2 -> 20009
size cluster i: 3 -> 20011
size cluster i: 4 -> 10000
size cluster i: 5 -> 10002
size cluster i: 6 -> 10001
size cluster i: 7 -> 10003
size cluster i: 8 -> 10004
size cluster i: 9 -> 10004
size cluster i: 10 -> 10004
size cluster i: 11 -> 10004
size cluster i: 12 -> 30016
size cluster i: 13 -> 10002
size cluster i: 14 -> 10003
size cluster i: 15 -> 10001
size cluster i: 16 -> 10006
size cluster i: 17 -> 30007
size cluster i: 18 -> 10007
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10005
size cluster i: 22 -> 10002
size cluster i: 23 -> 10004
size cluster i: 24 -> 30017
size cluster i: 25 -> 10006
size cluster i: 26 -> 10003
size cluster i: 27 -> 10003
size cluster i: 28 -> 10001
size cluster i: 29 -> 20009
size cluster i: 30 -> 10006
size cluster i: 31 -> 10003
size cluster i: 32 -> 10002
size cluster i: 33 -> 10008
size cluster i: 34 -> 10004
size cluster i: 35 -> 10004
size cluster i: 36 -> 10004
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10003
size cluster i: 40 -> 10000
size cluster i: 41 -> 10002
size cluster i: 42 -> 10004
size cluster i: 43 -> 20007
size cluster i: 44 -> 10002
size cluster i: 45 -> 10003
size cluster i: 46 -> 10002
size cluster i: 47 -> 10003
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10005
size cluster i: 52 -> 10002
size cluster i: 53 -> 10002
size cluster i: 54 -> 20015
size cluster i: 55 -> 10002
size cluster i: 56 -> 10002
size cluster i: 57 -> 10007
size cluster i: 58 -> 10004
size cluster i: 59 -> 10003
size cluster i: 60 -> 10006
size cluster i: 61 -> 10004
size cluster i: 62 -> 10008
size cluster i: 63 -> 20009
size cluster i: 64 -> 10004
size cluster i: 65 -> 10004
size cluster i: 66 -> 10003
size cluster i: 67 -> 10006
size cluster i: 68 -> 10005
size cluster i: 69 -> 10000
size cluster i: 70 -> 10007
size cluster i: 71 -> 10010
size cluster i: 72 -> 10004
size cluster i: 73 -> 10003
size cluster i: 74 -> 10002
size cluster i: 75 -> 10003
size cluster i: 76 -> 10005
size cluster i: 77 -> 10001
size cluster i: 78 -> 10007
size cluster i: 79 -> 9964
size cluster i: 80 -> 10002
size cluster i: 81 -> 10002
size cluster i: 82 -> 10003
size cluster i: 83 -> 10000
size cluster i: 84 -> 10003
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10002
size cluster i: 88 -> 2
size cluster i: 89 -> 4
size cluster i: 90 -> 3
size cluster i: 91 -> 4
size cluster i: 92 -> 2
size cluster i: 94 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 101 -> 2
datapoints in clusters: 1000351
score: 95.1314


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:17:17 2019
elapsed time: 136.709s


Finishing: 
---------- 
Beginning cleanup...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node Warning: Node Warning: Node Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!


Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:272188] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:272188] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 103
datapoints_clusters: 1000351
score: 95.1314
elapsed_time: 136.709

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 666.666666667 percent_correct: 0.8819382352941176
-> new best_percent_correct:0.8819382352941176new best_threshold:666.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 833.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[795,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 833.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1849s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 113.844s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.02487s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38723
find clusters duration: 0.281979s
detected clusters: 100
size cluster i: 0 -> 10005
size cluster i: 1 -> 10001
size cluster i: 2 -> 20005
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10001
size cluster i: 7 -> 10001
size cluster i: 8 -> 10002
size cluster i: 9 -> 10004
size cluster i: 10 -> 10002
size cluster i: 11 -> 10003
size cluster i: 12 -> 20010
size cluster i: 13 -> 10002
size cluster i: 14 -> 10002
size cluster i: 15 -> 10001
size cluster i: 16 -> 10003
size cluster i: 17 -> 20002
size cluster i: 18 -> 10003
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10002
size cluster i: 22 -> 10001
size cluster i: 23 -> 10004
size cluster i: 24 -> 20007
size cluster i: 25 -> 10005
size cluster i: 26 -> 10003
size cluster i: 27 -> 10002
size cluster i: 28 -> 10003
size cluster i: 29 -> 10002
size cluster i: 30 -> 9986
size cluster i: 31 -> 20007
size cluster i: 32 -> 10006
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10005
size cluster i: 36 -> 10005
size cluster i: 37 -> 10002
size cluster i: 38 -> 10004
size cluster i: 39 -> 10004
size cluster i: 40 -> 10003
size cluster i: 41 -> 9993
size cluster i: 42 -> 10003
size cluster i: 43 -> 10003
size cluster i: 44 -> 10000
size cluster i: 45 -> 9999
size cluster i: 46 -> 10003
size cluster i: 47 -> 20007
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 10004
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 20009
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10004
size cluster i: 63 -> 10002
size cluster i: 64 -> 10006
size cluster i: 65 -> 10003
size cluster i: 66 -> 10005
size cluster i: 67 -> 20009
size cluster i: 68 -> 10002
size cluster i: 69 -> 10003
size cluster i: 70 -> 10003
size cluster i: 71 -> 10006
size cluster i: 72 -> 10005
size cluster i: 73 -> 10000
size cluster i: 74 -> 10007
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10002
size cluster i: 78 -> 10000
size cluster i: 79 -> 10003
size cluster i: 80 -> 10002
size cluster i: 81 -> 10001
size cluster i: 82 -> 10004
size cluster i: 83 -> 9570
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10003
size cluster i: 87 -> 10000
size cluster i: 88 -> 10003
size cluster i: 89 -> 10000
size cluster i: 90 -> 10000
size cluster i: 91 -> 10001
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 98 -> 2
datapoints in clusters: 999811
score: 98.0207


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:19:43 2019
elapsed time: 136.685s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:272549] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:272549] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 100
datapoints_clusters: 999811
score: 98.0207
elapsed_time: 136.685

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 833.333333333 percent_correct: 0.9208656862745098
-> new best_percent_correct:0.9208656862745098new best_threshold:833.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 1000.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[429,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 1000


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1169s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 113.872s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.9846s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38664
find clusters duration: 0.280554s
detected clusters: 97
size cluster i: 0 -> 10004
size cluster i: 1 -> 9999
size cluster i: 2 -> 20004
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10002
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 20008
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10002
size cluster i: 17 -> 20001
size cluster i: 18 -> 10003
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20006
size cluster i: 25 -> 10005
size cluster i: 26 -> 10002
size cluster i: 27 -> 10002
size cluster i: 28 -> 10002
size cluster i: 29 -> 10001
size cluster i: 30 -> 9911
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10001
size cluster i: 36 -> 10005
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10004
size cluster i: 40 -> 10002
size cluster i: 41 -> 9934
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9940
size cluster i: 46 -> 10002
size cluster i: 47 -> 20006
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 10004
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 10005
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 20007
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10004
size cluster i: 72 -> 10004
size cluster i: 73 -> 10000
size cluster i: 74 -> 10005
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10003
size cluster i: 80 -> 10002
size cluster i: 81 -> 10003
size cluster i: 82 -> 10001
size cluster i: 83 -> 10004
size cluster i: 84 -> 8536
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10002
size cluster i: 88 -> 10000
size cluster i: 89 -> 10002
size cluster i: 90 -> 10000
size cluster i: 91 -> 9982
size cluster i: 92 -> 10001
size cluster i: 93 -> 2
size cluster i: 94 -> 2
datapoints in clusters: 998498
score: 94.9552


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:22:10 2019
elapsed time: 136.599s


Finishing: 
---------- 
Beginning cleanup...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:272915] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:272915] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 998498
score: 94.9552
elapsed_time: 136.599

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 1000.0 percent_correct: 0.9295117647058824
-> new best_percent_correct:0.9295117647058824new best_threshold:1000.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 1166.6666666666665 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[220,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 1166.67


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1396s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 114.022s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.03892s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38602
find clusters duration: 0.285515s
detected clusters: 96
size cluster i: 0 -> 10004
size cluster i: 1 -> 9992
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 20006
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9664
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9747
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9661
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10003
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10001
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10004
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10002
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10001
size cluster i: 82 -> 10003
size cluster i: 83 -> 10001
size cluster i: 84 -> 10003
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 6853
size cluster i: 90 -> 10002
size cluster i: 91 -> 9998
size cluster i: 92 -> 9888
size cluster i: 93 -> 10000
size cluster i: 94 -> 2
size cluster i: 95 -> 2
datapoints in clusters: 995940
score: 93.7355


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:24:37 2019
elapsed time: 136.842s


Finishing: 
---------- 
Beginning cleanup...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:273250] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:273250] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with theWarning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
value given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 995940
score: 93.7355
elapsed_time: 136.842

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 1166.66666667 percent_correct: 0.9369235294117647
-> new best_percent_correct:0.9369235294117647new best_threshold:1166.66666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 1333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[1916,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 1333.33


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.275s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 113.964s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.94213s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38483
find clusters duration: 0.272005s
detected clusters: 97
size cluster i: 0 -> 10003
size cluster i: 1 -> 9950
size cluster i: 2 -> 20002
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9998
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10004
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20004
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9199
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9360
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 8938
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10002
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 10002
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 4930
size cluster i: 91 -> 10002
size cluster i: 92 -> 9981
size cluster i: 93 -> 9632
size cluster i: 94 -> 10000
size cluster i: 95 -> 3
size cluster i: 96 -> 2
datapoints in clusters: 992108
score: 94.3475


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:27:03 2019
elapsed time: 136.774s


Finishing: 
---------- 
Beginning cleanup...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!


Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:273602] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:273602] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 992108
score: 94.3475
elapsed_time: 136.774

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 1333.33333333 percent_correct: 0.9430088235294117
-> new best_percent_correct:0.9430088235294117new best_threshold:1333.33333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 1500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[1438,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 1500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1807s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 113.958s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.00467s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38330
find clusters duration: 0.273855s
detected clusters: 97
size cluster i: 0 -> 10003
size cluster i: 1 -> 9821
size cluster i: 2 -> 20002
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9975
size cluster i: 9 -> 10002
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 20006
size cluster i: 31 -> 10002
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 7806
size cluster i: 44 -> 10001
size cluster i: 45 -> 20003
size cluster i: 46 -> 10000
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10004
size cluster i: 66 -> 10001
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10001
size cluster i: 72 -> 10000
size cluster i: 73 -> 10001
size cluster i: 74 -> 8560
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 8667
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10002
size cluster i: 92 -> 9906
size cluster i: 93 -> 9151
size cluster i: 94 -> 10000
size cluster i: 95 -> 3239
size cluster i: 96 -> 2
datapoints in clusters: 987220
score: 93.8827


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:29:30 2019
elapsed time: 136.754s


Finishing: 
---------- 
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!


Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:273952] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:273952] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 987220
score: 93.8827
elapsed_time: 136.754

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 1500.0 percent_correct: 0.9480666666666666
-> new best_percent_correct:0.9480666666666666new best_threshold:1500.0
thresholds:[ 1370.37037037  1407.40740741  1444.44444444  1481.48148148  1518.51851852
  1555.55555556  1592.59259259  1629.62962963], threshold_step:37.037037037

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 1370.3703703703702 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[1216,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 1370.37


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.148s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 113.908s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.03252s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38444
find clusters duration: 0.277034s
detected clusters: 99
size cluster i: 0 -> 10003
size cluster i: 1 -> 9934
size cluster i: 2 -> 20002
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9997
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10004
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9087
size cluster i: 31 -> 20006
size cluster i: 32 -> 10003
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9223
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 8727
size cluster i: 46 -> 10001
size cluster i: 47 -> 20003
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10002
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 10002
size cluster i: 84 -> 10000
size cluster i: 85 -> 10002
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 4548
size cluster i: 92 -> 10002
size cluster i: 93 -> 9968
size cluster i: 94 -> 9530
size cluster i: 95 -> 10000
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 2
datapoints in clusters: 991129
score: 96.1978


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:31:56 2019
elapsed time: 136.726s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 7 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config! is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:274302] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:274302] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 991129
score: 96.1978
elapsed_time: 136.726

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 1370.37037037 percent_correct: 0.951863725490196
-> new best_percent_correct:0.951863725490196new best_threshold:1370.37037037

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 1407.4074074074074 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[7003,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 1407.41


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0908s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 113.947s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.05331s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38419
find clusters duration: 0.268214s
detected clusters: 99
size cluster i: 0 -> 10003
size cluster i: 1 -> 9911
size cluster i: 2 -> 20002
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9992
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 8949
size cluster i: 31 -> 20006
size cluster i: 32 -> 10003
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9061
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 8478
size cluster i: 46 -> 10001
size cluster i: 47 -> 20003
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 10002
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10004
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10002
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 10002
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 4146
size cluster i: 92 -> 10002
size cluster i: 93 -> 9950
size cluster i: 94 -> 9442
size cluster i: 95 -> 10000
size cluster i: 96 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 990040
score: 96.0921


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:34:23 2019
elapsed time: 136.707s


Finishing: 
---------- 
Beginning cleanup...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:274661] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:274661] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 990040
score: 96.0921
elapsed_time: 136.707

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 1407.40740741 percent_correct: 0.9508029411764706

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 1444.4444444444443 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[6640,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 1444.44


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2145s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 113.999s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.01805s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38403
find clusters duration: 0.266748s
detected clusters: 97
size cluster i: 0 -> 10003
size cluster i: 1 -> 9880
size cluster i: 2 -> 20002
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9987
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 8801
size cluster i: 31 -> 20006
size cluster i: 32 -> 10003
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 8927
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 8245
size cluster i: 46 -> 10001
size cluster i: 47 -> 20003
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10001
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 10002
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10004
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10002
size cluster i: 73 -> 10001
size cluster i: 74 -> 10000
size cluster i: 75 -> 10002
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 10002
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 3786
size cluster i: 92 -> 10002
size cluster i: 93 -> 9936
size cluster i: 94 -> 9338
size cluster i: 95 -> 10000
datapoints in clusters: 989001
score: 94.0521


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:36:49 2019
elapsed time: 136.831s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 4: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Node 3: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:275022] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:275022] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 989001
score: 94.0521
elapsed_time: 136.831

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 1444.44444444 percent_correct: 0.9498019607843137

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 1481.4814814814815 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[6145,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 1481.48


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1861s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 114.023s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.99638s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38349
find clusters duration: 0.274845s
detected clusters: 97
size cluster i: 0 -> 10003
size cluster i: 1 -> 9842
size cluster i: 2 -> 20002
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9977
size cluster i: 9 -> 10002
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 20006
size cluster i: 31 -> 10002
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 8750
size cluster i: 41 -> 10003
size cluster i: 42 -> 10002
size cluster i: 43 -> 10000
size cluster i: 44 -> 7958
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 10000
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10001
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10003
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10004
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 8625
size cluster i: 76 -> 10001
size cluster i: 77 -> 10003
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 10002
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 3447
size cluster i: 92 -> 10002
size cluster i: 93 -> 9919
size cluster i: 94 -> 9212
size cluster i: 95 -> 10000
size cluster i: 96 -> 2
datapoints in clusters: 987827
score: 93.9404


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:39:16 2019
elapsed time: 136.863s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 5: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 1: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:275391] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:275391] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 987827
score: 93.9404
elapsed_time: 136.863

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 1481.48148148 percent_correct: 0.9486578431372549

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 1518.5185185185185 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[7844,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 1518.52


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1778s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 113.946s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.02918s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38297
find clusters duration: 0.270692s
detected clusters: 96
size cluster i: 0 -> 10003
size cluster i: 1 -> 9804
size cluster i: 2 -> 20002
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9972
size cluster i: 9 -> 10002
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 20005
size cluster i: 31 -> 10002
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 7632
size cluster i: 44 -> 10001
size cluster i: 45 -> 20003
size cluster i: 46 -> 10000
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10004
size cluster i: 66 -> 10001
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10001
size cluster i: 72 -> 10000
size cluster i: 73 -> 10001
size cluster i: 74 -> 8471
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 8584
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10002
size cluster i: 92 -> 9888
size cluster i: 93 -> 9079
size cluster i: 94 -> 10000
size cluster i: 95 -> 3082
datapoints in clusters: 986603
score: 92.8568


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:41:43 2019
elapsed time: 136.745s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 6: Exiting... 
Node 8: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:275738] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:275738] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 986603
score: 92.8568
elapsed_time: 136.745

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 1518.51851852 percent_correct: 0.9474676470588236

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 1555.5555555555557 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[7619,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 1555.56


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0801s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 113.815s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.92895s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38272
find clusters duration: 0.271655s
detected clusters: 98
size cluster i: 0 -> 10003
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9963
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10004
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10001
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 10000
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10002
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 7334
size cluster i: 43 -> 10001
size cluster i: 44 -> 20003
size cluster i: 45 -> 10000
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10002
size cluster i: 59 -> 10002
size cluster i: 60 -> 10000
size cluster i: 61 -> 10002
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10004
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10001
size cluster i: 73 -> 8300
size cluster i: 74 -> 10001
size cluster i: 75 -> 10003
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 9750
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 8387
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10001
size cluster i: 92 -> 9858
size cluster i: 93 -> 8920
size cluster i: 94 -> 10000
size cluster i: 95 -> 2720
size cluster i: 96 -> 3
size cluster i: 97 -> 3
datapoints in clusters: 985327
score: 94.6687


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:44:09 2019
elapsed time: 136.424s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 7: Exiting... 
Node 2: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 1: Exiting... 
Node 6: Exiting... 
Node 5: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:276093] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:276093] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 985327
score: 94.6687
elapsed_time: 136.424

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 1555.55555556 percent_correct: 0.9462147058823529

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 1592.5925925925926 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[7281,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 1592.59


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2336s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 114.015s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.06032s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38255
find clusters duration: 0.268001s
detected clusters: 99
size cluster i: 0 -> 10003
size cluster i: 1 -> 10001
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9946
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 20005
size cluster i: 31 -> 10002
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 7000
size cluster i: 44 -> 10001
size cluster i: 45 -> 20003
size cluster i: 46 -> 10000
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10004
size cluster i: 66 -> 10001
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10001
size cluster i: 72 -> 10000
size cluster i: 73 -> 10001
size cluster i: 74 -> 8124
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 9696
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 10002
size cluster i: 84 -> 10000
size cluster i: 85 -> 8198
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 9825
size cluster i: 94 -> 8780
size cluster i: 95 -> 10000
size cluster i: 96 -> 2428
size cluster i: 97 -> 4
size cluster i: 98 -> 2
datapoints in clusters: 984090
score: 95.5146


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:46:36 2019
elapsed time: 136.918s


Finishing: 
---------- 
Beginning cleanup...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:276431] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:276431] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 984090
score: 95.5146
elapsed_time: 136.918

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 1592.59259259 percent_correct: 0.9548098039215687
-> new best_percent_correct:0.9548098039215687new best_threshold:1592.59259259

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=5.6234132519034905e-08 --k 6 --epsilon 0.001 --threshold 1629.6296296296296 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[4765,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 5.62341e-08
k: 6
threshold: 1629.63


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1657s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228226
delta: 0.0545906
delta: 0.0446807
delta: 0.0322244
delta: 0.0385008
delta: 0.0625125
delta: 0.0370536
delta: 0.0280623
delta: 0.0415686
delta: 0.0175993
delta: 0.0858022
delta: 0.0620559
delta: 0.0241583
delta: 0.0108501
delta: 0.0164013
delta: 0.06333
delta: 0.016276
delta: 0.0160755
delta: 0.0362838
delta: 0.0290143
delta: 0.00806728
delta: 0.0922699
delta: 0.0135645
delta: 0.0307706
delta: 0.00680065
delta: 0.0193171
delta: 0.0489107
delta: 0.0066517
delta: 0.0192371
delta: 0.00509702
delta: 0.00441396
delta: 0.0129013
delta: 0.0040529
delta: 0.00732863
delta: 0.0136265
delta: 0.00463516
delta: 0.0233712
delta: 0.00464439
delta: 0.00388176
delta: 0.00218704
delta: 0.00448784
delta: 0.00278856
delta: 0.00474638
delta: 0.00819526
delta: 0.00118163
delta: 0.00321962
delta: 0.00988152
delta: 0.00148698
delta: 0.0046578
delta: 0.00151464
delta: 0.00291776
delta: 0.00203841
delta: 0.00478362
delta: 0.000983265
delta: 0.00374196
delta: 0.0245373
delta: 0.000396777
delta: 0.000637023
delta: 0.00340995
delta: 0.00273944
delta: 0.0010805
delta: 0.000954874
delta: 0.000543443
delta: 0.00214188
delta: 0.00107176
delta: 0.000454751
delta: 0.000400667
delta: 0.00072513
delta: 0.00110683
delta: 0.000381491
delta: 0.000565582
delta: 0.00031748
delta: 0.00067753
delta: 0.000253303
delta: 0.000458493
delta: 0.00047005
delta: 0.00145873
delta: 0.000246774
delta: 0.000673895
delta: 0.000174245
delta: 0.00160704
delta: 0.000221371
delta: 0.00022916
delta: 0.000619003
delta: 0.00031659
delta: 0.000436378
delta: 0.000123049
delta: 8.67099e-05
delta: 0.000334612
delta: 0.000264777
delta: 4.33508e-05
delta: 0.000507789
delta: 0.000174277
delta: 8.7385e-05
delta: 0.00014791
delta: 4.32628e-05
delta: 0.000334317
delta: 8.12214e-05
delta: 0.00016299
delta: 0.000141884
delta: 3.97128e-05
delta: 0.000197388
delta: 3.30753e-05
delta: 3.74781e-05
delta: 7.92717e-05
delta: 0.00018335
delta: 3.98784e-05
delta: 6.09458e-05
delta: 2.03812e-05
delta: 0.000226983
delta: 7.33531e-05
delta: 8.643e-06
delta: 1.71425e-05
delta: 0.000221103
delta: 5.38047e-05
delta: 2.69501e-05
delta: 3.79096e-05
delta: 1.001e-05
delta: 1.94535e-05
delta: 9.97783e-06
delta: 3.93518e-05
delta: 2.67394e-05
delta: 5.37486e-05
delta: 2.142e-05
delta: 6.24829e-06
delta: 2.14897e-05
delta: 0.000120749
delta: 1.23049e-05
delta: 1.9159e-05
delta: 9.26209e-06
delta: 8.49376e-06
delta: 7.44368e-06
delta: 6.81885e-06
delta: 2.73067e-06
delta: 3.48416e-05
delta: 2.72194e-06
delta: 1.20527e-05
delta: 8.59219e-06
delta: 1.2464e-05
delta: 5.07708e-05
delta: 5.834e-06
delta: 1.31375e-06
delta: 8.68092e-06
delta: 9.80095e-06
delta: 7.49233e-06
delta: 7.8557e-06
delta: 1.90672e-06
delta: 6.2574e-06
delta: 6.57672e-06
delta: 1.97336e-06
delta: 3.29227e-06
delta: 3.74487e-06
delta: 1.32185e-05
delta: 8.15658e-07
delta: 4.66962e-06
delta: 2.29604e-06
delta: 1.07829e-06
delta: 1.83221e-06
delta: 7.34546e-07
delta: 1.39917e-06
delta: 8.2953e-06
delta: 1.29713e-06
delta: 6.74475e-07
delta: 4.47889e-07
delta: 7.0516e-06
delta: 8.31103e-07
delta: 8.51532e-07
delta: 4.9682e-07
delta: 3.65457e-06
delta: 3.41374e-07
delta: 4.39271e-07
delta: 3.55068e-07
delta: 1.60834e-06
delta: 9.69993e-07
delta: 3.08476e-07
delta: 3.82363e-07
delta: 4.14097e-07
delta: 1.117e-06
delta: 2.60435e-07
delta: 9.94991e-07
delta: 5.56452e-07
delta: 4.66076e-07
delta: 3.12295e-07
delta: 2.01214e-07
delta: 1.56608e-07
delta: 8.60215e-07
delta: 3.09494e-07
delta: 1.63184e-07
delta: 5.06086e-07
delta: 1.54619e-07
delta: 1.57723e-07
delta: 3.58235e-07
delta: 6.65752e-08
delta: 5.26545e-07
delta: 9.13859e-08
delta: 2.60297e-07
delta: 1.44491e-07
delta: 2.9696e-07
delta: 7.84842e-07
delta: 1.16134e-07
delta: 5.17368e-08
delta: 1.78226e-07
delta: 6.28128e-07
delta: 2.76833e-07
delta: 8.68076e-08
delta: 2.22206e-08
Number of iterations: 206 (max. 1000)
Final norm of residuum: 2.22206e-08
counted_mult_calls: 211
solver duration: 113.995s
Number of grid points:     397825
alpha min: -4169.38 max: 12724.8
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.98343s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38207
find clusters duration: 0.267319s
detected clusters: 102
size cluster i: 0 -> 10001
size cluster i: 1 -> 10001
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9931
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 20005
size cluster i: 31 -> 10002
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 6669
size cluster i: 44 -> 10001
size cluster i: 45 -> 20003
size cluster i: 46 -> 9999
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10004
size cluster i: 66 -> 10001
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10001
size cluster i: 72 -> 10000
size cluster i: 73 -> 10001
size cluster i: 74 -> 7901
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 9631
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 8002
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 9774
size cluster i: 94 -> 8621
size cluster i: 95 -> 10000
size cluster i: 96 -> 2149
size cluster i: 97 -> 3
size cluster i: 99 -> 2
datapoints in clusters: 982768
score: 94.4228


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:49:02 2019
elapsed time: 136.733s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 8: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Node 1: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:276771] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:276771] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 102
datapoints_clusters: 982768
score: 94.4228
elapsed_time: 136.733

--------------- END RUN -----------------
attempt lambda: 5.6234132519034905e-08 threshold: 1629.62962963 percent_correct: 0.9535196078431373
thresholds:[0.0, 166.66666666666666, 333.33333333333331, 500.0, 666.66666666666663, 833.33333333333326, 1000.0, 1166.6666666666665, 1333.3333333333333, 1500.0], threshold_step:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 0.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[4553,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 0


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2064s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.5235s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.00125s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37747
find clusters duration: 64.1946s
detected clusters: 1
size cluster i: 0 -> 1020000
datapoints in clusters: 1020000
score: 1


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:52:06 2019
elapsed time: 174.273s


Finishing: 
---------- 
Beginning cleanup...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7: Exiting... 
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:277111] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:277111] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 1
datapoints_clusters: 1020000
score: 1.0
elapsed_time: 174.273

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 0.0 percent_correct: 0.00980392156862745
-> new best_percent_correct:0.00980392156862745new best_threshold:0.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 166.66666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[4197,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 166.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2512s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.1462s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.0574s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39384
find clusters duration: 5.59077s
detected clusters: 282
size cluster i: 0 -> 510863
size cluster i: 1 -> 10016
size cluster i: 2 -> 30047
size cluster i: 3 -> 40059
size cluster i: 4 -> 10002
size cluster i: 5 -> 10018
size cluster i: 6 -> 30058
size cluster i: 7 -> 10010
size cluster i: 8 -> 10013
size cluster i: 9 -> 10016
size cluster i: 10 -> 20034
size cluster i: 11 -> 10004
size cluster i: 12 -> 10009
size cluster i: 13 -> 10024
size cluster i: 14 -> 10008
size cluster i: 15 -> 10018
size cluster i: 16 -> 10015
size cluster i: 17 -> 10010
size cluster i: 18 -> 30064
size cluster i: 19 -> 10006
size cluster i: 20 -> 10015
size cluster i: 21 -> 10006
size cluster i: 22 -> 10016
size cluster i: 23 -> 20021
size cluster i: 24 -> 10012
size cluster i: 25 -> 10010
size cluster i: 26 -> 10008
size cluster i: 27 -> 10014
size cluster i: 28 -> 10013
size cluster i: 29 -> 10015
size cluster i: 30 -> 10012
size cluster i: 31 -> 10022
size cluster i: 32 -> 10019
size cluster i: 33 -> 10001
size cluster i: 34 -> 10013
size cluster i: 35 -> 10005
size cluster i: 36 -> 10010
size cluster i: 37 -> 10010
size cluster i: 38 -> 10016
size cluster i: 39 -> 2
size cluster i: 40 -> 2
size cluster i: 41 -> 2
size cluster i: 42 -> 3
size cluster i: 43 -> 2
size cluster i: 44 -> 2
size cluster i: 45 -> 2
size cluster i: 46 -> 3
size cluster i: 47 -> 4
size cluster i: 48 -> 4
size cluster i: 49 -> 5
size cluster i: 50 -> 3
size cluster i: 51 -> 2
size cluster i: 52 -> 2
size cluster i: 53 -> 2
size cluster i: 54 -> 2
size cluster i: 55 -> 3
size cluster i: 56 -> 2
size cluster i: 57 -> 4
size cluster i: 58 -> 5
size cluster i: 59 -> 3
size cluster i: 60 -> 2
size cluster i: 61 -> 2
size cluster i: 62 -> 3
size cluster i: 63 -> 3
size cluster i: 64 -> 3
size cluster i: 65 -> 2
size cluster i: 67 -> 5
size cluster i: 69 -> 5
size cluster i: 70 -> 2
size cluster i: 71 -> 3
size cluster i: 72 -> 2
size cluster i: 73 -> 2
size cluster i: 74 -> 2
size cluster i: 75 -> 2
size cluster i: 78 -> 2
size cluster i: 79 -> 2
size cluster i: 80 -> 3
size cluster i: 81 -> 2
size cluster i: 82 -> 5
size cluster i: 83 -> 4
size cluster i: 84 -> 2
size cluster i: 85 -> 5
size cluster i: 86 -> 3
size cluster i: 87 -> 4
size cluster i: 88 -> 4
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 3
size cluster i: 92 -> 2
size cluster i: 93 -> 3
size cluster i: 94 -> 4
size cluster i: 95 -> 4
size cluster i: 96 -> 4
size cluster i: 97 -> 3
size cluster i: 99 -> 4
size cluster i: 100 -> 5
size cluster i: 101 -> 3
size cluster i: 102 -> 4
size cluster i: 103 -> 2
size cluster i: 104 -> 3
size cluster i: 105 -> 3
size cluster i: 106 -> 2
size cluster i: 107 -> 2
size cluster i: 108 -> 2
size cluster i: 109 -> 2
size cluster i: 110 -> 3
size cluster i: 112 -> 3
size cluster i: 113 -> 4
size cluster i: 114 -> 4
size cluster i: 116 -> 2
size cluster i: 117 -> 2
size cluster i: 119 -> 4
size cluster i: 121 -> 2
size cluster i: 122 -> 3
size cluster i: 123 -> 4
size cluster i: 124 -> 3
size cluster i: 125 -> 3
size cluster i: 126 -> 2
size cluster i: 127 -> 3
size cluster i: 128 -> 2
size cluster i: 129 -> 2
size cluster i: 130 -> 2
size cluster i: 131 -> 3
size cluster i: 132 -> 2
size cluster i: 134 -> 2
size cluster i: 135 -> 2
size cluster i: 136 -> 2
size cluster i: 137 -> 3
size cluster i: 138 -> 5
size cluster i: 140 -> 4
size cluster i: 142 -> 2
size cluster i: 144 -> 3
size cluster i: 145 -> 2
size cluster i: 146 -> 2
size cluster i: 148 -> 2
size cluster i: 149 -> 3
size cluster i: 150 -> 2
size cluster i: 151 -> 6
size cluster i: 152 -> 2
size cluster i: 153 -> 2
size cluster i: 154 -> 2
size cluster i: 155 -> 2
size cluster i: 156 -> 2
size cluster i: 157 -> 2
size cluster i: 158 -> 2
size cluster i: 159 -> 2
size cluster i: 160 -> 2
size cluster i: 161 -> 2
size cluster i: 162 -> 4
size cluster i: 163 -> 4
size cluster i: 164 -> 2
size cluster i: 166 -> 4
size cluster i: 167 -> 2
size cluster i: 168 -> 4
size cluster i: 169 -> 4
size cluster i: 170 -> 2
size cluster i: 173 -> 2
size cluster i: 174 -> 2
size cluster i: 175 -> 2
size cluster i: 176 -> 3
size cluster i: 178 -> 2
size cluster i: 179 -> 2
size cluster i: 180 -> 3
size cluster i: 181 -> 2
size cluster i: 182 -> 2
size cluster i: 183 -> 2
size cluster i: 184 -> 3
size cluster i: 185 -> 2
size cluster i: 188 -> 4
size cluster i: 190 -> 2
size cluster i: 191 -> 2
size cluster i: 193 -> 3
size cluster i: 195 -> 2
size cluster i: 196 -> 3
size cluster i: 197 -> 2
size cluster i: 199 -> 2
size cluster i: 200 -> 4
size cluster i: 201 -> 2
size cluster i: 202 -> 2
size cluster i: 203 -> 3
size cluster i: 205 -> 2
size cluster i: 206 -> 3
size cluster i: 208 -> 2
size cluster i: 210 -> 2
size cluster i: 212 -> 3
size cluster i: 216 -> 2
size cluster i: 217 -> 2
size cluster i: 218 -> 2
size cluster i: 219 -> 2
size cluster i: 220 -> 2
size cluster i: 224 -> 2
size cluster i: 225 -> 2
size cluster i: 227 -> 2
size cluster i: 230 -> 3
size cluster i: 233 -> 2
size cluster i: 235 -> 2
size cluster i: 237 -> 2
size cluster i: 240 -> 2
size cluster i: 243 -> 2
size cluster i: 252 -> 2
size cluster i: 258 -> 2
size cluster i: 265 -> 2
size cluster i: 266 -> 2
datapoints in clusters: 1002049
score: 0


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:54:12 2019
elapsed time: 115.393s


Finishing: 
---------- 
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 8: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:277467] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:277467] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 282
datapoints_clusters: 1002049
score: 0.0
elapsed_time: 115.393

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 166.666666667 percent_correct: 0.3999519607843137
-> new best_percent_correct:0.3999519607843137new best_threshold:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[5774,1],1]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 333.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1538s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.3561s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.92191s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39020
find clusters duration: 0.337549s
detected clusters: 137
size cluster i: 0 -> 100102
size cluster i: 1 -> 10004
size cluster i: 2 -> 20018
size cluster i: 3 -> 30026
size cluster i: 4 -> 10002
size cluster i: 5 -> 10007
size cluster i: 6 -> 30035
size cluster i: 7 -> 10007
size cluster i: 8 -> 10005
size cluster i: 9 -> 10009
size cluster i: 10 -> 10006
size cluster i: 11 -> 10007
size cluster i: 12 -> 30019
size cluster i: 13 -> 10007
size cluster i: 14 -> 10004
size cluster i: 15 -> 10001
size cluster i: 16 -> 10010
size cluster i: 17 -> 60058
size cluster i: 18 -> 10012
size cluster i: 19 -> 10002
size cluster i: 20 -> 10004
size cluster i: 21 -> 20020
size cluster i: 22 -> 10004
size cluster i: 23 -> 10006
size cluster i: 24 -> 10012
size cluster i: 25 -> 10007
size cluster i: 26 -> 10007
size cluster i: 27 -> 10004
size cluster i: 28 -> 20018
size cluster i: 29 -> 10009
size cluster i: 30 -> 20016
size cluster i: 31 -> 10004
size cluster i: 32 -> 30032
size cluster i: 33 -> 20011
size cluster i: 34 -> 10008
size cluster i: 35 -> 20019
size cluster i: 36 -> 10003
size cluster i: 37 -> 10006
size cluster i: 38 -> 10006
size cluster i: 39 -> 10003
size cluster i: 40 -> 10005
size cluster i: 41 -> 20014
size cluster i: 42 -> 10003
size cluster i: 43 -> 10008
size cluster i: 44 -> 10005
size cluster i: 45 -> 10004
size cluster i: 46 -> 10004
size cluster i: 47 -> 10007
size cluster i: 48 -> 10008
size cluster i: 49 -> 10009
size cluster i: 50 -> 10007
size cluster i: 51 -> 10002
size cluster i: 52 -> 10008
size cluster i: 53 -> 10006
size cluster i: 54 -> 10009
size cluster i: 55 -> 10006
size cluster i: 56 -> 10005
size cluster i: 57 -> 10003
size cluster i: 58 -> 10019
size cluster i: 59 -> 10005
size cluster i: 60 -> 10006
size cluster i: 61 -> 10005
size cluster i: 62 -> 10007
size cluster i: 63 -> 10002
size cluster i: 64 -> 10000
size cluster i: 65 -> 10007
size cluster i: 66 -> 10008
size cluster i: 67 -> 10001
size cluster i: 68 -> 10003
size cluster i: 69 -> 10001
size cluster i: 70 -> 10008
size cluster i: 71 -> 2
size cluster i: 72 -> 4
size cluster i: 73 -> 2
size cluster i: 74 -> 3
size cluster i: 75 -> 2
size cluster i: 76 -> 3
size cluster i: 77 -> 2
size cluster i: 78 -> 2
size cluster i: 79 -> 5
size cluster i: 80 -> 2
size cluster i: 81 -> 3
size cluster i: 82 -> 2
size cluster i: 83 -> 2
size cluster i: 84 -> 4
size cluster i: 85 -> 4
size cluster i: 86 -> 3
size cluster i: 87 -> 5
size cluster i: 88 -> 2
size cluster i: 89 -> 2
size cluster i: 90 -> 3
size cluster i: 92 -> 3
size cluster i: 93 -> 2
size cluster i: 95 -> 3
size cluster i: 98 -> 2
size cluster i: 100 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 2
size cluster i: 103 -> 2
size cluster i: 105 -> 2
size cluster i: 107 -> 2
size cluster i: 108 -> 3
size cluster i: 109 -> 3
size cluster i: 110 -> 2
size cluster i: 112 -> 4
size cluster i: 115 -> 2
size cluster i: 116 -> 2
size cluster i: 117 -> 3
size cluster i: 119 -> 2
size cluster i: 120 -> 2
size cluster i: 121 -> 2
size cluster i: 123 -> 2
size cluster i: 124 -> 2
size cluster i: 131 -> 2
size cluster i: 134 -> 2
datapoints in clusters: 1000859
score: 61.8178


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:56:13 2019
elapsed time: 110.1s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 5 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 5: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:277808] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:277808] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 137
datapoints_clusters: 1000859
score: 61.8178
elapsed_time: 110.1

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 333.333333333 percent_correct: 0.7148441176470588
-> new best_percent_correct:0.7148441176470588new best_threshold:333.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[5410,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1728s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.5092s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.87223s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38868
find clusters duration: 0.292772s
detected clusters: 105
size cluster i: 0 -> 20017
size cluster i: 1 -> 10004
size cluster i: 2 -> 20011
size cluster i: 3 -> 20015
size cluster i: 4 -> 10000
size cluster i: 5 -> 10004
size cluster i: 6 -> 20012
size cluster i: 7 -> 10004
size cluster i: 8 -> 10004
size cluster i: 9 -> 10005
size cluster i: 10 -> 10004
size cluster i: 11 -> 10005
size cluster i: 12 -> 30016
size cluster i: 13 -> 10006
size cluster i: 14 -> 10003
size cluster i: 15 -> 10001
size cluster i: 16 -> 10007
size cluster i: 17 -> 40025
size cluster i: 18 -> 10009
size cluster i: 19 -> 10001
size cluster i: 20 -> 10001
size cluster i: 21 -> 10007
size cluster i: 22 -> 10003
size cluster i: 23 -> 10005
size cluster i: 24 -> 30020
size cluster i: 25 -> 10007
size cluster i: 26 -> 10004
size cluster i: 27 -> 10003
size cluster i: 28 -> 10001
size cluster i: 29 -> 20009
size cluster i: 30 -> 10007
size cluster i: 31 -> 10004
size cluster i: 32 -> 10003
size cluster i: 33 -> 10010
size cluster i: 34 -> 10005
size cluster i: 35 -> 10007
size cluster i: 36 -> 20013
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10003
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10004
size cluster i: 43 -> 20007
size cluster i: 44 -> 10002
size cluster i: 45 -> 10003
size cluster i: 46 -> 10007
size cluster i: 47 -> 10005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10006
size cluster i: 52 -> 10002
size cluster i: 53 -> 10003
size cluster i: 54 -> 20020
size cluster i: 55 -> 10003
size cluster i: 56 -> 10002
size cluster i: 57 -> 10004
size cluster i: 58 -> 10003
size cluster i: 59 -> 10007
size cluster i: 60 -> 10005
size cluster i: 61 -> 20011
size cluster i: 62 -> 10007
size cluster i: 63 -> 10004
size cluster i: 64 -> 10004
size cluster i: 65 -> 10007
size cluster i: 66 -> 10008
size cluster i: 67 -> 10000
size cluster i: 68 -> 10011
size cluster i: 69 -> 10004
size cluster i: 70 -> 10003
size cluster i: 71 -> 10003
size cluster i: 72 -> 10003
size cluster i: 73 -> 10006
size cluster i: 74 -> 10001
size cluster i: 75 -> 9993
size cluster i: 76 -> 10004
size cluster i: 77 -> 10002
size cluster i: 78 -> 10005
size cluster i: 79 -> 10000
size cluster i: 80 -> 10005
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10002
size cluster i: 84 -> 2
size cluster i: 85 -> 2
size cluster i: 86 -> 2
size cluster i: 87 -> 4
size cluster i: 88 -> 3
size cluster i: 89 -> 4
size cluster i: 90 -> 2
size cluster i: 92 -> 3
size cluster i: 93 -> 2
size cluster i: 94 -> 4
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 98 -> 2
size cluster i: 101 -> 2
size cluster i: 102 -> 2
datapoints in clusters: 1000490
score: 93.1829


Runtimes: 
--------- 
finished computation at Thu Jan  3 16:58:13 2019
elapsed time: 110.184s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:278172] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:278172] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 105
datapoints_clusters: 1000490
score: 93.1829
elapsed_time: 110.184

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 500.0 percent_correct: 0.8426431372549019
-> new best_percent_correct:0.8426431372549019new best_threshold:500.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 666.6666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[5200,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 666.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0987s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4059s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.00038s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38774
find clusters duration: 0.287659s
detected clusters: 99
size cluster i: 0 -> 10005
size cluster i: 1 -> 10002
size cluster i: 2 -> 20009
size cluster i: 3 -> 20009
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10001
size cluster i: 7 -> 10002
size cluster i: 8 -> 10003
size cluster i: 9 -> 10004
size cluster i: 10 -> 10002
size cluster i: 11 -> 10003
size cluster i: 12 -> 20010
size cluster i: 13 -> 10002
size cluster i: 14 -> 10002
size cluster i: 15 -> 10001
size cluster i: 16 -> 10006
size cluster i: 17 -> 30006
size cluster i: 18 -> 10007
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10004
size cluster i: 22 -> 10001
size cluster i: 23 -> 10004
size cluster i: 24 -> 30015
size cluster i: 25 -> 10004
size cluster i: 26 -> 10003
size cluster i: 27 -> 10003
size cluster i: 28 -> 9995
size cluster i: 29 -> 20008
size cluster i: 30 -> 10006
size cluster i: 31 -> 10001
size cluster i: 32 -> 10002
size cluster i: 33 -> 10005
size cluster i: 34 -> 10007
size cluster i: 35 -> 10003
size cluster i: 36 -> 10004
size cluster i: 37 -> 10003
size cluster i: 38 -> 9999
size cluster i: 39 -> 10003
size cluster i: 40 -> 10003
size cluster i: 41 -> 10000
size cluster i: 42 -> 10001
size cluster i: 43 -> 10003
size cluster i: 44 -> 20007
size cluster i: 45 -> 10001
size cluster i: 46 -> 10003
size cluster i: 47 -> 10002
size cluster i: 48 -> 10002
size cluster i: 49 -> 10001
size cluster i: 50 -> 10002
size cluster i: 51 -> 10002
size cluster i: 52 -> 10005
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 20013
size cluster i: 56 -> 10002
size cluster i: 57 -> 10001
size cluster i: 58 -> 10006
size cluster i: 59 -> 10004
size cluster i: 60 -> 10003
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10006
size cluster i: 64 -> 20009
size cluster i: 65 -> 10003
size cluster i: 66 -> 10004
size cluster i: 67 -> 10004
size cluster i: 68 -> 10006
size cluster i: 69 -> 10005
size cluster i: 70 -> 10000
size cluster i: 71 -> 10007
size cluster i: 72 -> 10004
size cluster i: 73 -> 10003
size cluster i: 74 -> 10003
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10003
size cluster i: 78 -> 10001
size cluster i: 79 -> 10005
size cluster i: 80 -> 9737
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 10003
size cluster i: 84 -> 10000
size cluster i: 85 -> 10003
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 4
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 2
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
datapoints in clusters: 1000052
score: 97.0639


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:00:13 2019
elapsed time: 110.128s


Finishing: 
---------- 
Beginning cleanup...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
 and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:278510] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:278510] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 1000052
score: 97.0639
elapsed_time: 110.128

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 666.666666667 percent_correct: 0.8915745098039216
-> new best_percent_correct:0.8915745098039216new best_threshold:666.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 833.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[27279,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 833.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1389s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4052s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.08874s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38677
find clusters duration: 0.278939s
detected clusters: 97
size cluster i: 0 -> 10004
size cluster i: 1 -> 9999
size cluster i: 2 -> 20005
size cluster i: 3 -> 10002
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10001
size cluster i: 8 -> 10002
size cluster i: 9 -> 10004
size cluster i: 10 -> 10001
size cluster i: 11 -> 10002
size cluster i: 12 -> 20008
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10001
size cluster i: 16 -> 10003
size cluster i: 17 -> 20002
size cluster i: 18 -> 10003
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10002
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20008
size cluster i: 25 -> 10005
size cluster i: 26 -> 10003
size cluster i: 27 -> 10002
size cluster i: 28 -> 10002
size cluster i: 29 -> 10001
size cluster i: 30 -> 9897
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10001
size cluster i: 35 -> 10002
size cluster i: 36 -> 10005
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10004
size cluster i: 40 -> 10003
size cluster i: 41 -> 9931
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9938
size cluster i: 46 -> 10002
size cluster i: 47 -> 20006
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 10004
size cluster i: 56 -> 10002
size cluster i: 57 -> 10002
size cluster i: 58 -> 20009
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 20009
size cluster i: 68 -> 10003
size cluster i: 69 -> 10003
size cluster i: 70 -> 10001
size cluster i: 71 -> 10006
size cluster i: 72 -> 10004
size cluster i: 73 -> 10000
size cluster i: 74 -> 10006
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10002
size cluster i: 78 -> 10000
size cluster i: 79 -> 10003
size cluster i: 80 -> 10002
size cluster i: 81 -> 10001
size cluster i: 82 -> 10004
size cluster i: 83 -> 8575
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10002
size cluster i: 87 -> 10000
size cluster i: 88 -> 10002
size cluster i: 89 -> 10000
size cluster i: 90 -> 9983
size cluster i: 91 -> 10001
size cluster i: 92 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 2
datapoints in clusters: 998549
score: 94.9601


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:02:13 2019
elapsed time: 110.238s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:278833] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:278833] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 998549
score: 94.9601
elapsed_time: 110.238

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 833.333333333 percent_correct: 0.9196990196078432
-> new best_percent_correct:0.9196990196078432new best_threshold:833.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 1000.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[26933,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1000


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2271s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4076s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.98453s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38607
find clusters duration: 0.274872s
detected clusters: 96
size cluster i: 0 -> 10004
size cluster i: 1 -> 9988
size cluster i: 2 -> 20003
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10005
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10002
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20005
size cluster i: 25 -> 10003
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9556
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9674
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9529
size cluster i: 46 -> 10001
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10004
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 20008
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10005
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10002
size cluster i: 73 -> 10000
size cluster i: 74 -> 10004
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10001
size cluster i: 82 -> 10001
size cluster i: 83 -> 10004
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 6580
size cluster i: 90 -> 10002
size cluster i: 91 -> 9997
size cluster i: 92 -> 9852
size cluster i: 93 -> 10000
size cluster i: 94 -> 2
size cluster i: 95 -> 2
datapoints in clusters: 995326
score: 93.6777


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:04:13 2019
elapsed time: 110.236s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with theWarning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
value given in the MPI config!

Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:279179] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:279179] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 995326
score: 93.6777
elapsed_time: 110.236

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 1000.0 percent_correct: 0.9362980392156863
-> new best_percent_correct:0.9362980392156863new best_threshold:1000.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 1166.6666666666665 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[26718,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1166.67


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0464s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.5127s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.09593s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38440
find clusters duration: 0.271037s
detected clusters: 96
size cluster i: 0 -> 10004
size cluster i: 1 -> 9915
size cluster i: 2 -> 20002
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9994
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10004
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20003
size cluster i: 25 -> 10003
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 8863
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10001
size cluster i: 39 -> 10004
size cluster i: 40 -> 10001
size cluster i: 41 -> 9036
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 8481
size cluster i: 46 -> 10001
size cluster i: 47 -> 20004
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10002
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 10000
size cluster i: 59 -> 10000
size cluster i: 60 -> 10001
size cluster i: 61 -> 10003
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 10003
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 10005
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 10002
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 4278
size cluster i: 91 -> 10002
size cluster i: 92 -> 9950
size cluster i: 93 -> 9451
size cluster i: 94 -> 10000
size cluster i: 95 -> 2
datapoints in clusters: 990086
score: 93.1846


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:06:13 2019
elapsed time: 110.284s


Finishing: 
---------- 
Node 2 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 1: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:279520] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:279520] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 990086
score: 93.1846
elapsed_time: 110.284

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 1166.66666667 percent_correct: 0.9410294117647059
-> new best_percent_correct:0.9410294117647059new best_threshold:1166.66666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 1333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[28305,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1333.33


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1787s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4425s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.07s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38225
find clusters duration: 0.277131s
detected clusters: 98
size cluster i: 0 -> 10003
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9956
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10004
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10001
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 20003
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10002
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 6835
size cluster i: 43 -> 10001
size cluster i: 44 -> 20003
size cluster i: 45 -> 10000
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10002
size cluster i: 59 -> 10002
size cluster i: 60 -> 10000
size cluster i: 61 -> 10002
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10000
size cluster i: 71 -> 10002
size cluster i: 72 -> 7856
size cluster i: 73 -> 10001
size cluster i: 74 -> 10003
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 9661
size cluster i: 79 -> 10002
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 8048
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 9795
size cluster i: 92 -> 8726
size cluster i: 93 -> 10000
size cluster i: 94 -> 2401
size cluster i: 95 -> 4
size cluster i: 97 -> 2
datapoints in clusters: 983377
score: 94.4813


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:08:14 2019
elapsed time: 110.314s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 4: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:279855] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:279855] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 98
datapoints_clusters: 983377
score: 94.4813
elapsed_time: 110.314

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 1333.33333333 percent_correct: 0.9344921568627451

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 1500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[27958,1],5]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0721s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4773s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.09184s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37972
find clusters duration: 0.266065s
detected clusters: 102
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9822
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9997
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 5033
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9979
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10002
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 6696
size cluster i: 76 -> 10001
size cluster i: 77 -> 10002
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 9128
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 6745
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 10000
size cluster i: 92 -> 10000
size cluster i: 93 -> 10001
size cluster i: 94 -> 9406
size cluster i: 95 -> 7697
size cluster i: 96 -> 10000
size cluster i: 97 -> 1099
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 2
datapoints in clusters: 975686
score: 93.7424


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:10:14 2019
elapsed time: 110.229s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:280200] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:280200] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 102
datapoints_clusters: 975686
score: 93.7424
elapsed_time: 110.229

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 1500.0 percent_correct: 0.9563970588235294
-> new best_percent_correct:0.9563970588235294new best_threshold:1500.0
thresholds:[ 1370.37037037  1407.40740741  1444.44444444  1481.48148148  1518.51851852
  1555.55555556  1592.59259259  1629.62962963], threshold_step:37.037037037

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 1370.3703703703702 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[27750,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1370.37


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1696s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.5098s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.06858s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38176
find clusters duration: 0.27565s
detected clusters: 99
size cluster i: 0 -> 10003
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9932
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10004
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 10000
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10002
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 6439
size cluster i: 43 -> 10001
size cluster i: 44 -> 20003
size cluster i: 45 -> 10000
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10002
size cluster i: 59 -> 10002
size cluster i: 60 -> 10000
size cluster i: 61 -> 10002
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10002
size cluster i: 73 -> 7611
size cluster i: 74 -> 10001
size cluster i: 75 -> 10003
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 9575
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 7794
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 10000
size cluster i: 91 -> 10001
size cluster i: 92 -> 9736
size cluster i: 93 -> 8525
size cluster i: 94 -> 10000
size cluster i: 95 -> 2067
size cluster i: 96 -> 3
size cluster i: 97 -> 2
datapoints in clusters: 981775
score: 95.2899


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:12:14 2019
elapsed time: 110.35s


Finishing: 
---------- 
Beginning cleanup...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 7: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:280536] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:280536] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 981775
score: 95.2899
elapsed_time: 110.35

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 1370.37037037 percent_correct: 0.9427303921568627

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 1407.4074074074074 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[25237,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1407.41


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1177s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4797s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.08394s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38135
find clusters duration: 0.272448s
detected clusters: 100
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9911
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10004
size cluster i: 12 -> 10000
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 10000
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10001
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 6035
size cluster i: 43 -> 10001
size cluster i: 44 -> 20003
size cluster i: 45 -> 9997
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10002
size cluster i: 59 -> 10002
size cluster i: 60 -> 10000
size cluster i: 61 -> 10002
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10002
size cluster i: 73 -> 7362
size cluster i: 74 -> 10001
size cluster i: 75 -> 10002
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 9459
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 7500
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 10000
size cluster i: 91 -> 10001
size cluster i: 92 -> 9661
size cluster i: 93 -> 8323
size cluster i: 94 -> 10000
size cluster i: 95 -> 1752
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 2
datapoints in clusters: 980091
score: 96.0874


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:14:14 2019
elapsed time: 110.275s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:280875] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:280875] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 100
datapoints_clusters: 980091
score: 96.0874
elapsed_time: 110.275

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 1407.40740741 percent_correct: 0.941092156862745

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 1444.4444444444443 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[25024,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1444.44


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1493s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4577s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.01142s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38073
find clusters duration: 0.26922s
detected clusters: 100
size cluster i: 0 -> 10001
size cluster i: 1 -> 10001
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9881
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 20005
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 5633
size cluster i: 44 -> 10001
size cluster i: 45 -> 20003
size cluster i: 46 -> 9991
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10001
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10001
size cluster i: 72 -> 10000
size cluster i: 73 -> 10002
size cluster i: 74 -> 7087
size cluster i: 75 -> 10001
size cluster i: 76 -> 10002
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 9328
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 7210
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 9560
size cluster i: 94 -> 8075
size cluster i: 95 -> 10000
size cluster i: 96 -> 1455
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 99 -> 2
datapoints in clusters: 978308
score: 95.9125


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:16:14 2019
elapsed time: 110.245s


Finishing: 
---------- 
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Destroying the enviroment now!
Cleanup done!
Node 3: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 TiWarning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config! and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:281214] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:281214] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 100
datapoints_clusters: 978308
score: 95.9125
elapsed_time: 110.245

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 1444.44444444 percent_correct: 0.9491529411764706

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 1481.4814814814815 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[24687,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1481.48


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0862s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.3609s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.03777s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38003
find clusters duration: 0.274058s
detected clusters: 101
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9839
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9997
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 20004
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 5217
size cluster i: 44 -> 10001
size cluster i: 45 -> 20003
size cluster i: 46 -> 9982
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10004
size cluster i: 66 -> 10001
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10002
size cluster i: 71 -> 10001
size cluster i: 72 -> 10000
size cluster i: 73 -> 10001
size cluster i: 74 -> 6824
size cluster i: 75 -> 10001
size cluster i: 76 -> 10002
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 9191
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 6905
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 9471
size cluster i: 94 -> 7823
size cluster i: 95 -> 10000
size cluster i: 96 -> 1219
size cluster i: 98 -> 4
size cluster i: 99 -> 2
datapoints in clusters: 976553
score: 94.7831


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:18:14 2019
elapsed time: 110.082s


Finishing: 
---------- 
Beginning cleanup...
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:281553] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:281553] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 101
datapoints_clusters: 976553
score: 94.7831
elapsed_time: 110.082

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 1481.48148148 percent_correct: 0.9474401960784313

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 1518.5185185185185 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[26266,1],5]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1518.52


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1745s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.399s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.99398s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37958
find clusters duration: 0.272574s
detected clusters: 103
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9803
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9996
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10002
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 4827
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9978
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10002
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 6541
size cluster i: 76 -> 10001
size cluster i: 77 -> 10002
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 9055
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 6589
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 10000
size cluster i: 92 -> 10000
size cluster i: 93 -> 10001
size cluster i: 94 -> 9356
size cluster i: 95 -> 7576
size cluster i: 96 -> 10000
size cluster i: 97 -> 984
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 100 -> 4
size cluster i: 101 -> 2
datapoints in clusters: 974790
score: 92.7006


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:20:14 2019
elapsed time: 110.177s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 6: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:281892] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:281892] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 103
datapoints_clusters: 974790
score: 92.7006
elapsed_time: 110.177

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 1518.51851852 percent_correct: 0.9555186274509804

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 1555.5555555555557 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[26060,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1555.56


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1524s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.529s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.98378s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37908
find clusters duration: 0.271113s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9737
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9993
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 4410
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9970
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10002
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 6292
size cluster i: 76 -> 10001
size cluster i: 77 -> 10001
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 8912
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 6268
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 10000
size cluster i: 92 -> 10000
size cluster i: 93 -> 10001
size cluster i: 94 -> 9247
size cluster i: 95 -> 7296
size cluster i: 96 -> 10000
size cluster i: 97 -> 811
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 2
datapoints in clusters: 973018
score: 91.5782


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:22:15 2019
elapsed time: 110.266s


Finishing: 
---------- 
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:282226] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:282226] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 973018
score: 91.5782
elapsed_time: 110.266

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 1555.55555556 percent_correct: 0.9537892156862745

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 1592.5925925925926 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[25723,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1592.59


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1113s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4073s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.03847s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37845
find clusters duration: 0.266594s
detected clusters: 104
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9669
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10003
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9992
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 3947
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9954
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10000
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 5996
size cluster i: 76 -> 10001
size cluster i: 77 -> 10001
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 10000
size cluster i: 81 -> 8718
size cluster i: 82 -> 10001
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 9109
size cluster i: 94 -> 5930
size cluster i: 95 -> 7036
size cluster i: 96 -> 10000
size cluster i: 97 -> 637
size cluster i: 98 -> 3
size cluster i: 99 -> 2
datapoints in clusters: 971060
score: 91.3939


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:24:15 2019
elapsed time: 110.163s


Finishing: 
---------- 
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:282565] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:282565] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 104
datapoints_clusters: 971060
score: 91.3939
elapsed_time: 110.163

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 1592.59259259 percent_correct: 0.9518872549019608

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=9.999999999999998e-08 --k 6 --epsilon 0.001 --threshold 1629.6296296296296 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[31401,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1e-07
k: 6
threshold: 1629.63


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1126s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228199
delta: 0.0544764
delta: 0.0444728
delta: 0.0319131
delta: 0.0378208
delta: 0.0605145
delta: 0.0361845
delta: 0.026919
delta: 0.0394675
delta: 0.0164576
delta: 0.079264
delta: 0.058309
delta: 0.0218162
delta: 0.00958552
delta: 0.0140642
delta: 0.0492664
delta: 0.0138616
delta: 0.0130742
delta: 0.0296211
delta: 0.0221802
delta: 0.00620545
delta: 0.0828858
delta: 0.0101582
delta: 0.0220459
delta: 0.00500067
delta: 0.0112924
delta: 0.051359
delta: 0.00444604
delta: 0.0125649
delta: 0.00325877
delta: 0.00228613
delta: 0.0187257
delta: 0.00229531
delta: 0.00448618
delta: 0.0069899
delta: 0.00225407
delta: 0.0288797
delta: 0.00237326
delta: 0.00191755
delta: 0.00114263
delta: 0.00130417
delta: 0.005553
delta: 0.00143174
delta: 0.0031418
delta: 0.000543386
delta: 0.00114738
delta: 0.00319962
delta: 0.00071799
delta: 0.0019662
delta: 0.000565262
delta: 0.000541955
delta: 0.0014537
delta: 0.00809065
delta: 0.00038189
delta: 0.00113799
delta: 0.000130535
delta: 0.000703775
delta: 0.000427226
delta: 0.000418701
delta: 0.000681088
delta: 0.000185434
delta: 0.000377307
delta: 0.000743238
delta: 0.000146572
delta: 0.000155124
delta: 0.000156999
delta: 0.000132598
delta: 0.00058422
delta: 7.49437e-05
delta: 0.000342358
delta: 4.94315e-05
delta: 7.23398e-05
delta: 0.000234562
delta: 0.0002166
delta: 0.000126841
delta: 5.69656e-05
delta: 6.72163e-05
delta: 0.000542551
delta: 8.81835e-05
delta: 3.14911e-05
delta: 3.21294e-05
delta: 0.000171831
delta: 0.000105192
delta: 3.49529e-05
delta: 2.69986e-05
delta: 3.99729e-05
delta: 1.23462e-05
delta: 0.000339274
delta: 1.75528e-05
delta: 1.22719e-05
delta: 3.72632e-05
delta: 7.42506e-06
delta: 0.000245855
delta: 2.53737e-05
delta: 8.68102e-06
delta: 8.98116e-06
delta: 3.09757e-05
delta: 7.0761e-05
delta: 3.75847e-06
delta: 4.55994e-06
delta: 1.24099e-05
delta: 5.4544e-06
delta: 2.96912e-05
delta: 6.04619e-06
delta: 8.5671e-06
delta: 7.31427e-06
delta: 1.67825e-06
delta: 1.83417e-05
delta: 8.42005e-07
delta: 1.67434e-06
delta: 6.50579e-06
delta: 3.89075e-06
delta: 8.27291e-06
delta: 4.40491e-06
delta: 2.35449e-06
delta: 7.37442e-07
delta: 1.86539e-06
delta: 4.29718e-06
delta: 1.49687e-06
delta: 2.94837e-06
delta: 1.05759e-06
delta: 5.89155e-07
delta: 3.66628e-06
delta: 7.90943e-07
delta: 1.28856e-06
delta: 8.83086e-07
delta: 1.59712e-06
delta: 2.44783e-07
delta: 6.51229e-07
delta: 4.24896e-07
delta: 3.80794e-07
delta: 2.24382e-07
delta: 6.46631e-07
delta: 3.66294e-07
delta: 4.40203e-07
delta: 1.03909e-06
delta: 2.94918e-07
delta: 2.2308e-07
delta: 1.41394e-07
delta: 1.3621e-07
delta: 1.59328e-07
delta: 3.47371e-07
delta: 1.04331e-07
delta: 9.86249e-07
delta: 2.7481e-07
delta: 7.40222e-08
delta: 8.92444e-08
delta: 3.01791e-07
delta: 4.80688e-08
delta: 1.92823e-07
delta: 1.90409e-07
delta: 5.88292e-08
delta: 6.28485e-08
delta: 4.65723e-07
delta: 6.56662e-08
delta: 1.05376e-07
delta: 1.58958e-08
Number of iterations: 157 (max. 1000)
Final norm of residuum: 1.58958e-08
counted_mult_calls: 161
solver duration: 87.4687s
Number of grid points:     397825
alpha min: -2859.21 max: 8285.96
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.95833s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37834
find clusters duration: 0.265981s
detected clusters: 105
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9602
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10003
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9991
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10002
size cluster i: 44 -> 3582
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9929
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10003
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 8524
size cluster i: 81 -> 10001
size cluster i: 82 -> 5695
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 8947
size cluster i: 94 -> 5646
size cluster i: 95 -> 6759
size cluster i: 96 -> 10000
size cluster i: 97 -> 500
size cluster i: 99 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 2
size cluster i: 102 -> 3
datapoints in clusters: 969248
score: 90.2731


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:26:15 2019
elapsed time: 110.148s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
 and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!


[argon-gtx:282903] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:282903] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 105
datapoints_clusters: 969248
score: 90.2731
elapsed_time: 110.148

--------------- END RUN -----------------
attempt lambda: 9.999999999999998e-08 threshold: 1629.62962963 percent_correct: 0.9501127450980392
thresholds:[0.0, 166.66666666666666, 333.33333333333331, 500.0, 666.66666666666663, 833.33333333333326, 1000.0, 1166.6666666666665, 1333.3333333333333, 1500.0], threshold_step:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 0.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[31185,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 0


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0055s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.8881s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.06708s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37747
find clusters duration: 64.12s
detected clusters: 1
size cluster i: 0 -> 1020000
datapoints in clusters: 1020000
score: 1


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:28:59 2019
elapsed time: 154.42s


Finishing: 
---------- 
Beginning cleanup...
Node 4 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 6 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 4: Exiting... 
Node 3: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:283247] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:283247] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 1
datapoints_clusters: 1020000
score: 1.0
elapsed_time: 154.42

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 0.0 percent_correct: 0.00980392156862745
-> new best_percent_correct:0.00980392156862745new best_threshold:0.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 166.66666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[30847,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 166.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2054s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.5058s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.96982s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 39339
find clusters duration: 3.19949s
detected clusters: 213
size cluster i: 0 -> 450690
size cluster i: 1 -> 10012
size cluster i: 2 -> 20032
size cluster i: 3 -> 30041
size cluster i: 4 -> 10017
size cluster i: 5 -> 30058
size cluster i: 6 -> 10008
size cluster i: 7 -> 10013
size cluster i: 8 -> 10016
size cluster i: 9 -> 10019
size cluster i: 10 -> 70090
size cluster i: 11 -> 20036
size cluster i: 12 -> 10005
size cluster i: 13 -> 10010
size cluster i: 14 -> 20022
size cluster i: 15 -> 10022
size cluster i: 16 -> 10008
size cluster i: 17 -> 10014
size cluster i: 18 -> 10010
size cluster i: 19 -> 30061
size cluster i: 20 -> 10005
size cluster i: 21 -> 10014
size cluster i: 22 -> 10006
size cluster i: 23 -> 10016
size cluster i: 24 -> 20022
size cluster i: 25 -> 10008
size cluster i: 26 -> 10009
size cluster i: 27 -> 10008
size cluster i: 28 -> 10012
size cluster i: 29 -> 10017
size cluster i: 30 -> 10011
size cluster i: 31 -> 10025
size cluster i: 32 -> 10017
size cluster i: 33 -> 10001
size cluster i: 34 -> 10013
size cluster i: 35 -> 10014
size cluster i: 36 -> 10005
size cluster i: 37 -> 10009
size cluster i: 38 -> 10006
size cluster i: 39 -> 10015
size cluster i: 40 -> 2
size cluster i: 41 -> 4
size cluster i: 42 -> 2
size cluster i: 43 -> 4
size cluster i: 44 -> 5
size cluster i: 45 -> 3
size cluster i: 46 -> 2
size cluster i: 47 -> 2
size cluster i: 48 -> 2
size cluster i: 49 -> 3
size cluster i: 50 -> 3
size cluster i: 51 -> 3
size cluster i: 52 -> 5
size cluster i: 53 -> 2
size cluster i: 54 -> 3
size cluster i: 55 -> 3
size cluster i: 56 -> 2
size cluster i: 58 -> 2
size cluster i: 59 -> 2
size cluster i: 60 -> 2
size cluster i: 62 -> 4
size cluster i: 63 -> 2
size cluster i: 64 -> 3
size cluster i: 65 -> 2
size cluster i: 66 -> 3
size cluster i: 67 -> 4
size cluster i: 68 -> 2
size cluster i: 69 -> 2
size cluster i: 70 -> 3
size cluster i: 71 -> 2
size cluster i: 72 -> 3
size cluster i: 73 -> 4
size cluster i: 74 -> 3
size cluster i: 75 -> 3
size cluster i: 76 -> 4
size cluster i: 77 -> 2
size cluster i: 79 -> 3
size cluster i: 80 -> 3
size cluster i: 81 -> 2
size cluster i: 82 -> 2
size cluster i: 83 -> 2
size cluster i: 84 -> 4
size cluster i: 85 -> 4
size cluster i: 86 -> 2
size cluster i: 87 -> 2
size cluster i: 89 -> 4
size cluster i: 91 -> 2
size cluster i: 92 -> 3
size cluster i: 93 -> 3
size cluster i: 94 -> 3
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 2
size cluster i: 100 -> 2
size cluster i: 102 -> 2
size cluster i: 105 -> 3
size cluster i: 106 -> 4
size cluster i: 108 -> 4
size cluster i: 111 -> 2
size cluster i: 112 -> 2
size cluster i: 114 -> 3
size cluster i: 115 -> 2
size cluster i: 116 -> 2
size cluster i: 117 -> 2
size cluster i: 118 -> 2
size cluster i: 120 -> 4
size cluster i: 121 -> 4
size cluster i: 122 -> 3
size cluster i: 123 -> 2
size cluster i: 124 -> 4
size cluster i: 125 -> 3
size cluster i: 127 -> 2
size cluster i: 128 -> 2
size cluster i: 129 -> 3
size cluster i: 132 -> 3
size cluster i: 133 -> 2
size cluster i: 134 -> 2
size cluster i: 135 -> 3
size cluster i: 139 -> 2
size cluster i: 140 -> 2
size cluster i: 142 -> 2
size cluster i: 145 -> 2
size cluster i: 146 -> 2
size cluster i: 147 -> 2
size cluster i: 149 -> 2
size cluster i: 150 -> 4
size cluster i: 151 -> 2
size cluster i: 152 -> 3
size cluster i: 154 -> 3
size cluster i: 155 -> 3
size cluster i: 156 -> 2
size cluster i: 160 -> 3
size cluster i: 164 -> 2
size cluster i: 165 -> 2
size cluster i: 168 -> 2
size cluster i: 170 -> 2
size cluster i: 174 -> 3
size cluster i: 175 -> 2
size cluster i: 176 -> 2
size cluster i: 186 -> 2
size cluster i: 191 -> 2
size cluster i: 199 -> 2
datapoints in clusters: 1001758
score: 0


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:30:43 2019
elapsed time: 93.2279s


Finishing: 
---------- 
Beginning cleanup...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node Warning: Node Warning: Node Warning: Node Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:283585] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:283585] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 213
datapoints_clusters: 1001758
score: 0.0
elapsed_time: 93.2279

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 166.666666667 percent_correct: 0.41004117647058824
-> new best_percent_correct:0.41004117647058824new best_threshold:166.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[32407,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 333.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1311s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.8345s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.03563s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38990
find clusters duration: 0.352259s
detected clusters: 113
size cluster i: 0 -> 100101
size cluster i: 1 -> 10004
size cluster i: 2 -> 20017
size cluster i: 3 -> 30025
size cluster i: 4 -> 10001
size cluster i: 5 -> 10005
size cluster i: 6 -> 50042
size cluster i: 7 -> 10005
size cluster i: 8 -> 10005
size cluster i: 9 -> 10009
size cluster i: 10 -> 10006
size cluster i: 11 -> 10006
size cluster i: 12 -> 30018
size cluster i: 13 -> 10007
size cluster i: 14 -> 10003
size cluster i: 15 -> 10001
size cluster i: 16 -> 10010
size cluster i: 17 -> 60057
size cluster i: 18 -> 10011
size cluster i: 19 -> 10001
size cluster i: 20 -> 10003
size cluster i: 21 -> 20018
size cluster i: 22 -> 10004
size cluster i: 23 -> 10006
size cluster i: 24 -> 10009
size cluster i: 25 -> 10007
size cluster i: 26 -> 10006
size cluster i: 27 -> 10004
size cluster i: 28 -> 20015
size cluster i: 29 -> 10009
size cluster i: 30 -> 10003
size cluster i: 31 -> 30031
size cluster i: 32 -> 20011
size cluster i: 33 -> 10008
size cluster i: 34 -> 20015
size cluster i: 35 -> 10001
size cluster i: 36 -> 10007
size cluster i: 37 -> 10006
size cluster i: 38 -> 10003
size cluster i: 39 -> 10005
size cluster i: 40 -> 20014
size cluster i: 41 -> 10003
size cluster i: 42 -> 10006
size cluster i: 43 -> 10005
size cluster i: 44 -> 10004
size cluster i: 45 -> 10004
size cluster i: 46 -> 10008
size cluster i: 47 -> 10002
size cluster i: 48 -> 10007
size cluster i: 49 -> 10006
size cluster i: 50 -> 10002
size cluster i: 51 -> 10007
size cluster i: 52 -> 10006
size cluster i: 53 -> 10009
size cluster i: 54 -> 10005
size cluster i: 55 -> 10005
size cluster i: 56 -> 10003
size cluster i: 57 -> 10013
size cluster i: 58 -> 10005
size cluster i: 59 -> 10006
size cluster i: 60 -> 10005
size cluster i: 61 -> 10007
size cluster i: 62 -> 10002
size cluster i: 63 -> 10000
size cluster i: 64 -> 10007
size cluster i: 65 -> 10008
size cluster i: 66 -> 10001
size cluster i: 67 -> 10003
size cluster i: 68 -> 10001
size cluster i: 69 -> 10006
size cluster i: 70 -> 2
size cluster i: 71 -> 2
size cluster i: 72 -> 3
size cluster i: 73 -> 2
size cluster i: 74 -> 4
size cluster i: 75 -> 2
size cluster i: 76 -> 2
size cluster i: 77 -> 2
size cluster i: 78 -> 3
size cluster i: 79 -> 3
size cluster i: 80 -> 4
size cluster i: 81 -> 3
size cluster i: 82 -> 5
size cluster i: 83 -> 2
size cluster i: 84 -> 3
size cluster i: 85 -> 3
size cluster i: 87 -> 2
size cluster i: 88 -> 2
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 93 -> 2
size cluster i: 94 -> 3
size cluster i: 95 -> 2
size cluster i: 96 -> 2
size cluster i: 98 -> 3
size cluster i: 101 -> 2
size cluster i: 102 -> 2
size cluster i: 103 -> 3
size cluster i: 104 -> 2
size cluster i: 106 -> 2
size cluster i: 107 -> 2
size cluster i: 110 -> 2
datapoints in clusters: 1000757
score: 85.3587


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:32:24 2019
elapsed time: 90.7143s


Finishing: 
---------- 
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:283945] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:283945] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 113
datapoints_clusters: 1000757
score: 85.3587
elapsed_time: 90.7143

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 333.333333333 percent_correct: 0.7051401960784314
-> new best_percent_correct:0.7051401960784314new best_threshold:333.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[32060,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0743s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.9306s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.96488s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38843
find clusters duration: 0.292591s
detected clusters: 95
size cluster i: 0 -> 20013
size cluster i: 1 -> 10003
size cluster i: 2 -> 20010
size cluster i: 3 -> 20014
size cluster i: 4 -> 10000
size cluster i: 5 -> 10004
size cluster i: 6 -> 20011
size cluster i: 7 -> 10003
size cluster i: 8 -> 10004
size cluster i: 9 -> 10004
size cluster i: 10 -> 10004
size cluster i: 11 -> 10004
size cluster i: 12 -> 30017
size cluster i: 13 -> 10003
size cluster i: 14 -> 10003
size cluster i: 15 -> 10001
size cluster i: 16 -> 10007
size cluster i: 17 -> 40023
size cluster i: 18 -> 10008
size cluster i: 19 -> 10001
size cluster i: 20 -> 10001
size cluster i: 21 -> 10006
size cluster i: 22 -> 10002
size cluster i: 23 -> 10005
size cluster i: 24 -> 30022
size cluster i: 25 -> 10006
size cluster i: 26 -> 10003
size cluster i: 27 -> 10003
size cluster i: 28 -> 9999
size cluster i: 29 -> 20010
size cluster i: 30 -> 10006
size cluster i: 31 -> 10003
size cluster i: 32 -> 10002
size cluster i: 33 -> 10011
size cluster i: 34 -> 10004
size cluster i: 35 -> 10004
size cluster i: 36 -> 20013
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10003
size cluster i: 40 -> 10003
size cluster i: 41 -> 10001
size cluster i: 42 -> 10004
size cluster i: 43 -> 20007
size cluster i: 44 -> 10002
size cluster i: 45 -> 10003
size cluster i: 46 -> 10003
size cluster i: 47 -> 10005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10005
size cluster i: 52 -> 10002
size cluster i: 53 -> 10003
size cluster i: 54 -> 20019
size cluster i: 55 -> 10002
size cluster i: 56 -> 10002
size cluster i: 57 -> 10004
size cluster i: 58 -> 10003
size cluster i: 59 -> 10006
size cluster i: 60 -> 10004
size cluster i: 61 -> 20011
size cluster i: 62 -> 20012
size cluster i: 63 -> 10004
size cluster i: 64 -> 10005
size cluster i: 65 -> 10007
size cluster i: 66 -> 10000
size cluster i: 67 -> 10005
size cluster i: 68 -> 10004
size cluster i: 69 -> 10003
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10006
size cluster i: 73 -> 10001
size cluster i: 74 -> 9916
size cluster i: 75 -> 10001
size cluster i: 76 -> 10002
size cluster i: 77 -> 10005
size cluster i: 78 -> 10000
size cluster i: 79 -> 10004
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 2
size cluster i: 84 -> 4
size cluster i: 85 -> 2
size cluster i: 87 -> 3
size cluster i: 88 -> 2
size cluster i: 89 -> 4
size cluster i: 90 -> 2
size cluster i: 91 -> 2
size cluster i: 93 -> 2
datapoints in clusters: 1000345
score: 93.1694


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:34:04 2019
elapsed time: 90.6014s


Finishing: 
---------- 
Beginning cleanup...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 3: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:284290] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:284290] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 95
datapoints_clusters: 1000345
score: 93.1694
elapsed_time: 90.6014

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 500.0 percent_correct: 0.8328264705882353
-> new best_percent_correct:0.8328264705882353new best_threshold:500.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 666.6666666666666 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[31847,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 666.667


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1788s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.8761s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.05045s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38719
find clusters duration: 0.286536s
detected clusters: 94
size cluster i: 0 -> 10005
size cluster i: 1 -> 10000
size cluster i: 2 -> 20005
size cluster i: 3 -> 20009
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10001
size cluster i: 7 -> 10001
size cluster i: 8 -> 10002
size cluster i: 9 -> 10004
size cluster i: 10 -> 10001
size cluster i: 11 -> 10003
size cluster i: 12 -> 20008
size cluster i: 13 -> 10002
size cluster i: 14 -> 10002
size cluster i: 15 -> 10001
size cluster i: 16 -> 10004
size cluster i: 17 -> 30005
size cluster i: 18 -> 10007
size cluster i: 19 -> 10001
size cluster i: 20 -> 10000
size cluster i: 21 -> 10002
size cluster i: 22 -> 10002
size cluster i: 23 -> 10004
size cluster i: 24 -> 30014
size cluster i: 25 -> 10004
size cluster i: 26 -> 10003
size cluster i: 27 -> 10001
size cluster i: 28 -> 9895
size cluster i: 29 -> 20007
size cluster i: 30 -> 10005
size cluster i: 31 -> 10002
size cluster i: 32 -> 10001
size cluster i: 33 -> 10003
size cluster i: 34 -> 10006
size cluster i: 35 -> 10002
size cluster i: 36 -> 10003
size cluster i: 37 -> 10003
size cluster i: 38 -> 9934
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 9958
size cluster i: 43 -> 10002
size cluster i: 44 -> 20006
size cluster i: 45 -> 10001
size cluster i: 46 -> 10003
size cluster i: 47 -> 10002
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10002
size cluster i: 51 -> 10002
size cluster i: 52 -> 10005
size cluster i: 53 -> 10002
size cluster i: 54 -> 10002
size cluster i: 55 -> 20011
size cluster i: 56 -> 10002
size cluster i: 57 -> 10001
size cluster i: 58 -> 10006
size cluster i: 59 -> 10003
size cluster i: 60 -> 10001
size cluster i: 61 -> 10006
size cluster i: 62 -> 10003
size cluster i: 63 -> 10005
size cluster i: 64 -> 20009
size cluster i: 65 -> 10003
size cluster i: 66 -> 10003
size cluster i: 67 -> 10004
size cluster i: 68 -> 10006
size cluster i: 69 -> 10005
size cluster i: 70 -> 10000
size cluster i: 71 -> 10007
size cluster i: 72 -> 10001
size cluster i: 73 -> 10003
size cluster i: 74 -> 10002
size cluster i: 75 -> 10000
size cluster i: 76 -> 10003
size cluster i: 77 -> 10003
size cluster i: 78 -> 10001
size cluster i: 79 -> 10004
size cluster i: 80 -> 8782
size cluster i: 81 -> 10000
size cluster i: 82 -> 10001
size cluster i: 83 -> 10002
size cluster i: 84 -> 10000
size cluster i: 85 -> 10002
size cluster i: 86 -> 10000
size cluster i: 87 -> 9987
size cluster i: 88 -> 10001
size cluster i: 89 -> 2
size cluster i: 90 -> 2
size cluster i: 91 -> 2
datapoints in clusters: 998820
score: 92.0481


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:35:44 2019
elapsed time: 90.7321s


Finishing: 
---------- 
Beginning cleanup...
Node 8 received cleanup signal...
Node 2 received cleanup signal...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 4: Exiting... 
Node 2: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Node 6: Exiting... 
Cleanup done!
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:284633] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:284633] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 94
datapoints_clusters: 998820
score: 92.0481
elapsed_time: 90.7321

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 666.666666667 percent_correct: 0.890478431372549
-> new best_percent_correct:0.890478431372549new best_threshold:666.666666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 833.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[29331,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 833.333


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2044s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.8781s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.03686s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38602
find clusters duration: 0.27237s
detected clusters: 96
size cluster i: 0 -> 10004
size cluster i: 1 -> 9980
size cluster i: 2 -> 20003
size cluster i: 3 -> 10001
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10001
size cluster i: 8 -> 10000
size cluster i: 9 -> 10003
size cluster i: 10 -> 10000
size cluster i: 11 -> 10001
size cluster i: 12 -> 10006
size cluster i: 13 -> 10002
size cluster i: 14 -> 10001
size cluster i: 15 -> 10000
size cluster i: 16 -> 10002
size cluster i: 17 -> 20002
size cluster i: 18 -> 10003
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20007
size cluster i: 25 -> 10003
size cluster i: 26 -> 10003
size cluster i: 27 -> 10002
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 9378
size cluster i: 31 -> 20006
size cluster i: 32 -> 10005
size cluster i: 33 -> 10001
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10002
size cluster i: 37 -> 10001
size cluster i: 38 -> 10002
size cluster i: 39 -> 10004
size cluster i: 40 -> 10003
size cluster i: 41 -> 9550
size cluster i: 42 -> 10003
size cluster i: 43 -> 10002
size cluster i: 44 -> 10000
size cluster i: 45 -> 9388
size cluster i: 46 -> 10001
size cluster i: 47 -> 20005
size cluster i: 48 -> 10001
size cluster i: 49 -> 10003
size cluster i: 50 -> 10002
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10002
size cluster i: 54 -> 10001
size cluster i: 55 -> 10004
size cluster i: 56 -> 10001
size cluster i: 57 -> 10001
size cluster i: 58 -> 20008
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10005
size cluster i: 62 -> 10003
size cluster i: 63 -> 10000
size cluster i: 64 -> 10004
size cluster i: 65 -> 10001
size cluster i: 66 -> 10001
size cluster i: 67 -> 20008
size cluster i: 68 -> 10003
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10003
size cluster i: 72 -> 10002
size cluster i: 73 -> 10000
size cluster i: 74 -> 10005
size cluster i: 75 -> 10001
size cluster i: 76 -> 10003
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10003
size cluster i: 80 -> 10001
size cluster i: 81 -> 10001
size cluster i: 82 -> 10004
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10001
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 6236
size cluster i: 89 -> 10002
size cluster i: 90 -> 9994
size cluster i: 91 -> 9814
size cluster i: 92 -> 10000
size cluster i: 93 -> 4
datapoints in clusters: 994517
score: 93.6016


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:37:25 2019
elapsed time: 90.7357s


Finishing: 
---------- 
Beginning cleanup...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 8 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 7: Exiting... 
Node 8: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 

4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:284973] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:284973] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 96
datapoints_clusters: 994517
score: 93.6016
elapsed_time: 90.7357

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 833.333333333 percent_correct: 0.9256519607843138
-> new best_percent_correct:0.9256519607843138new best_threshold:833.333333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 1000.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[29122,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 1000


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2453s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.7958s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.98693s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38352
find clusters duration: 0.285096s
detected clusters: 95
size cluster i: 0 -> 10004
size cluster i: 1 -> 9813
size cluster i: 2 -> 20002
size cluster i: 3 -> 10000
size cluster i: 4 -> 10000
size cluster i: 5 -> 10001
size cluster i: 6 -> 10000
size cluster i: 7 -> 10000
size cluster i: 8 -> 9988
size cluster i: 9 -> 10002
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10005
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 10001
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10003
size cluster i: 24 -> 20003
size cluster i: 25 -> 10003
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 20005
size cluster i: 31 -> 10005
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10001
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10003
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 7823
size cluster i: 44 -> 10001
size cluster i: 45 -> 20003
size cluster i: 46 -> 10001
size cluster i: 47 -> 10001
size cluster i: 48 -> 10001
size cluster i: 49 -> 10001
size cluster i: 50 -> 10001
size cluster i: 51 -> 10002
size cluster i: 52 -> 10001
size cluster i: 53 -> 10003
size cluster i: 54 -> 10001
size cluster i: 55 -> 10000
size cluster i: 56 -> 20003
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10003
size cluster i: 60 -> 10003
size cluster i: 61 -> 10000
size cluster i: 62 -> 10003
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10002
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10002
size cluster i: 73 -> 8328
size cluster i: 74 -> 10001
size cluster i: 75 -> 10003
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10002
size cluster i: 80 -> 10000
size cluster i: 81 -> 10000
size cluster i: 82 -> 8546
size cluster i: 83 -> 10003
size cluster i: 84 -> 10000
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 3527
size cluster i: 90 -> 10002
size cluster i: 91 -> 9901
size cluster i: 92 -> 9176
size cluster i: 93 -> 10000
datapoints in clusters: 987220
score: 91.947


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:39:06 2019
elapsed time: 90.6566s


Finishing: 
---------- 
Beginning cleanup...
Node 5 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:285308] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:285308] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 95
datapoints_clusters: 987220
score: 91.947
elapsed_time: 90.6566

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 1000.0 percent_correct: 0.928414705882353
-> new best_percent_correct:0.928414705882353new best_threshold:1000.0

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 1166.6666666666665 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[28786,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 1166.67


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1434s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.8475s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.0644s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38053
find clusters duration: 0.269267s
detected clusters: 99
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9895
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10004
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 20003
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10001
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 5559
size cluster i: 43 -> 10001
size cluster i: 44 -> 20003
size cluster i: 45 -> 9997
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10002
size cluster i: 59 -> 10002
size cluster i: 60 -> 10000
size cluster i: 61 -> 10002
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10000
size cluster i: 71 -> 10002
size cluster i: 72 -> 6840
size cluster i: 73 -> 10001
size cluster i: 74 -> 10002
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 9271
size cluster i: 79 -> 10002
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 6984
size cluster i: 84 -> 10002
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 9523
size cluster i: 92 -> 8053
size cluster i: 93 -> 10000
size cluster i: 94 -> 1516
size cluster i: 95 -> 2
size cluster i: 98 -> 2
datapoints in clusters: 977731
score: 94.8974


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:40:46 2019
elapsed time: 90.6766s


Finishing: 
---------- 
Beginning cleanup...
Node 5 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 4: Exiting... 
Node 8: Exiting... 
Node 7: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 2: Exiting... 
Node 6: Exiting... 
Node 1: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:285644] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:285644] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 99
datapoints_clusters: 977731
score: 94.8974
elapsed_time: 90.6766

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 1166.66666667 percent_correct: 0.9289696078431372
-> new best_percent_correct:0.9289696078431372new best_threshold:1166.66666667

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 1333.3333333333333 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[30358,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 1333.33


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 3: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2018s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.8363s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.06736s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37803
find clusters duration: 0.265911s
detected clusters: 106
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9587
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 9989
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10002
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 3302
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9934
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10002
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10004
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10000
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 8317
size cluster i: 81 -> 10001
size cluster i: 82 -> 5160
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 8726
size cluster i: 94 -> 5210
size cluster i: 95 -> 6549
size cluster i: 96 -> 10000
size cluster i: 97 -> 453
size cluster i: 100 -> 3
size cluster i: 102 -> 2
size cluster i: 103 -> 3
datapoints in clusters: 967310
score: 89.1443


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:42:27 2019
elapsed time: 90.7016s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 1 received cleanup signal...
Node 3 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 5: Exiting... 
Node 4: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 1: Exiting... 
Node 6: Exiting... 
Node 8: Exiting... 
Node 2: Exiting... 
Node 3: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:285992] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:285992] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 106
datapoints_clusters: 967310
score: 89.1443
elapsed_time: 90.7016

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 1333.33333333 percent_correct: 0.948193137254902
-> new best_percent_correct:0.948193137254902new best_threshold:1333.33333333

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 1500.0 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[30154,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 1500


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0777s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.8968s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.96217s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37433
find clusters duration: 0.261569s
detected clusters: 114
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 8897
size cluster i: 8 -> 10001
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9947
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10001
size cluster i: 44 -> 1566
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9723
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10001
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10004
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10001
size cluster i: 73 -> 9999
size cluster i: 74 -> 10001
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 7027
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 10000
size cluster i: 91 -> 10001
size cluster i: 92 -> 7614
size cluster i: 93 -> 3568
size cluster i: 94 -> 3491
size cluster i: 95 -> 4937
size cluster i: 96 -> 10000
size cluster i: 97 -> 71
size cluster i: 98 -> 2
size cluster i: 99 -> 6
size cluster i: 101 -> 2
size cluster i: 103 -> 3
size cluster i: 104 -> 3
size cluster i: 105 -> 2
size cluster i: 106 -> 3
size cluster i: 107 -> 6
size cluster i: 108 -> 2
size cluster i: 109 -> 2
size cluster i: 112 -> 2
datapoints in clusters: 956931
score: 80.6824


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:44:07 2019
elapsed time: 90.5501s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 3 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Beginning cleanup...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 2: Exiting... 
Node 4: Exiting... 
Node 8: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 6: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:286324] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:286324] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 114
datapoints_clusters: 956931
score: 80.6824
elapsed_time: 90.5501

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 1500.0 percent_correct: 0.9380264705882353
thresholds:[ 1203.7037037   1240.74074074  1277.77777778  1314.81481481  1351.85185185
  1388.88888889  1425.92592593  1462.96296296], threshold_step:37.037037037

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 1203.7037037037035 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[29818,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 1203.7


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1297s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.9334s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 8.97466s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 38002
find clusters duration: 0.270225s
detected clusters: 97
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9851
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10004
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9997
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 20003
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 20005
size cluster i: 30 -> 10001
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10003
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 5064
size cluster i: 43 -> 10001
size cluster i: 44 -> 20003
size cluster i: 45 -> 9989
size cluster i: 46 -> 10000
size cluster i: 47 -> 10001
size cluster i: 48 -> 10000
size cluster i: 49 -> 10000
size cluster i: 50 -> 10001
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10000
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10001
size cluster i: 58 -> 10002
size cluster i: 59 -> 10002
size cluster i: 60 -> 10000
size cluster i: 61 -> 10002
size cluster i: 62 -> 10001
size cluster i: 63 -> 10001
size cluster i: 64 -> 10005
size cluster i: 65 -> 10001
size cluster i: 66 -> 10002
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10000
size cluster i: 71 -> 10002
size cluster i: 72 -> 6472
size cluster i: 73 -> 10001
size cluster i: 74 -> 10002
size cluster i: 75 -> 10000
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 9083
size cluster i: 79 -> 10002
size cluster i: 80 -> 10000
size cluster i: 81 -> 10002
size cluster i: 82 -> 10000
size cluster i: 83 -> 6592
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10000
size cluster i: 87 -> 10001
size cluster i: 88 -> 10000
size cluster i: 89 -> 10000
size cluster i: 90 -> 10001
size cluster i: 91 -> 9377
size cluster i: 92 -> 7735
size cluster i: 93 -> 10000
size cluster i: 94 -> 1218
size cluster i: 95 -> 3
datapoints in clusters: 975468
score: 92.7651


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:45:48 2019
elapsed time: 90.6325s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 4 received cleanup signal...
Beginning cleanup...
Node 8 received cleanup signal...
Node 7 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 4: Exiting... 
Node 3: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:286660] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:286660] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 97
datapoints_clusters: 975468
score: 92.7651
elapsed_time: 90.6325

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 1203.7037037 percent_correct: 0.9267549019607844

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 1240.7407407407406 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[19115,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 1240.74


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1394s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.8456s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.00346s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37918
find clusters duration: 0.266965s
detected clusters: 103
size cluster i: 0 -> 10001
size cluster i: 1 -> 20002
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9801
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10004
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 9994
size cluster i: 15 -> 10000
size cluster i: 16 -> 20001
size cluster i: 17 -> 10001
size cluster i: 18 -> 10000
size cluster i: 19 -> 10000
size cluster i: 20 -> 10001
size cluster i: 21 -> 10001
size cluster i: 22 -> 10002
size cluster i: 23 -> 20003
size cluster i: 24 -> 10002
size cluster i: 25 -> 10000
size cluster i: 26 -> 10001
size cluster i: 27 -> 10000
size cluster i: 28 -> 10001
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10001
size cluster i: 32 -> 10000
size cluster i: 33 -> 10000
size cluster i: 34 -> 10002
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10004
size cluster i: 38 -> 10001
size cluster i: 39 -> 10002
size cluster i: 40 -> 10002
size cluster i: 41 -> 10000
size cluster i: 42 -> 10003
size cluster i: 43 -> 4541
size cluster i: 44 -> 10001
size cluster i: 45 -> 20003
size cluster i: 46 -> 9979
size cluster i: 47 -> 10000
size cluster i: 48 -> 10001
size cluster i: 49 -> 10000
size cluster i: 50 -> 10000
size cluster i: 51 -> 10001
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10001
size cluster i: 59 -> 10002
size cluster i: 60 -> 10002
size cluster i: 61 -> 10000
size cluster i: 62 -> 10002
size cluster i: 63 -> 10001
size cluster i: 64 -> 10001
size cluster i: 65 -> 10005
size cluster i: 66 -> 10001
size cluster i: 67 -> 10002
size cluster i: 68 -> 10001
size cluster i: 69 -> 10002
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10002
size cluster i: 73 -> 6117
size cluster i: 74 -> 10001
size cluster i: 75 -> 10002
size cluster i: 76 -> 10000
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 8904
size cluster i: 80 -> 10002
size cluster i: 81 -> 10000
size cluster i: 82 -> 10002
size cluster i: 83 -> 10000
size cluster i: 84 -> 6176
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 10000
size cluster i: 91 -> 10001
size cluster i: 92 -> 9247
size cluster i: 93 -> 7409
size cluster i: 94 -> 10000
size cluster i: 95 -> 933
size cluster i: 96 -> 2
size cluster i: 97 -> 2
size cluster i: 98 -> 4
size cluster i: 99 -> 3
datapoints in clusters: 973198
score: 92.5492


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:47:28 2019
elapsed time: 90.6035s


Finishing: 
---------- 
Beginning cleanup...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 5 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Node 3: Exiting... 
Node 4: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with theWarning: Node Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
value given in the MPI config!
3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

[argon-gtx:286997] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:286997] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 103
datapoints_clusters: 973198
score: 92.5492
elapsed_time: 90.6035

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 1240.74074074 percent_correct: 0.9343294117647059

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 1277.7777777777776 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[18905,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 1277.78


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 6: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.0706s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.7846s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.06379s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37866
find clusters duration: 0.271012s
detected clusters: 102
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9707
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 9992
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10002
size cluster i: 41 -> 10002
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 3995
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9972
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10001
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10002
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10005
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 8660
size cluster i: 81 -> 10001
size cluster i: 82 -> 5741
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 9067
size cluster i: 94 -> 5746
size cluster i: 95 -> 7060
size cluster i: 96 -> 10000
size cluster i: 97 -> 705
size cluster i: 98 -> 3
datapoints in clusters: 970727
score: 93.2659


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:49:09 2019
elapsed time: 90.506s


Finishing: 
---------- 
Node 2 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 4 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Beginning cleanup...
Node 6 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3: Exiting... 
Node 7: Exiting... 
Node 1: Exiting... 
Node 2: Exiting... 
Node 5: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:287335] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:287335] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 102
datapoints_clusters: 970727
score: 93.2659
elapsed_time: 90.506

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 1277.77777778 percent_correct: 0.9515382352941176
-> new best_percent_correct:0.9515382352941176new best_threshold:1277.77777778

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 1314.8148148148148 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[18451,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 1314.81


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.2085s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 68.0112s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.01783s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37822
find clusters duration: 0.262075s
detected clusters: 103
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9628
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10004
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 9991
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10002
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10002
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 3521
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9954
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10002
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10004
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10002
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 8425
size cluster i: 81 -> 10001
size cluster i: 82 -> 5332
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 8848
size cluster i: 94 -> 5394
size cluster i: 95 -> 6720
size cluster i: 96 -> 10000
size cluster i: 97 -> 531
size cluster i: 99 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 2
size cluster i: 102 -> 3
datapoints in clusters: 968428
score: 92.0956


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:50:50 2019
elapsed time: 90.8331s


Finishing: 
---------- 
Node 6 received cleanup signal...
Node 4 received cleanup signal...
Node 8 received cleanup signal...
Node 1 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 2 received cleanup signal...
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 4: Exiting... 
Node 2: Exiting... 
Node 8: Exiting... 
Node 6: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:287661] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:287661] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 103
datapoints_clusters: 968428
score: 92.0956
elapsed_time: 90.8331

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 1314.81481481 percent_correct: 0.9492852941176471

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 1351.8518518518517 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[20157,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 1351.85


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1644s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 8: Created slave operation "DensityMultiplicationWorker"
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.8548s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.02432s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37760
find clusters duration: 0.264634s
detected clusters: 103
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9526
size cluster i: 8 -> 10002
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10003
size cluster i: 13 -> 10001
size cluster i: 14 -> 10000
size cluster i: 15 -> 9989
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10001
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10004
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10003
size cluster i: 44 -> 3068
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9922
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10002
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10004
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10001
size cluster i: 70 -> 10002
size cluster i: 71 -> 10000
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10002
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 8182
size cluster i: 81 -> 10001
size cluster i: 82 -> 4968
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 8594
size cluster i: 94 -> 5008
size cluster i: 95 -> 6370
size cluster i: 96 -> 10000
size cluster i: 97 -> 398
size cluster i: 98 -> 4
size cluster i: 99 -> 2
size cluster i: 100 -> 2
size cluster i: 101 -> 2
datapoints in clusters: 966104
score: 91.8746


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:52:31 2019
elapsed time: 90.6605s


Finishing: 
---------- 
Beginning cleanup...
Node 2 received cleanup signal...
Node 6 received cleanup signal...
Node 7 received cleanup signal...
Node 3 received cleanup signal...
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 7: Exiting... 
Node 5: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 4: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node Warning: Node 4 is going to delete COUNT entry for 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:288003] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:288003] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 103
datapoints_clusters: 966104
score: 91.8746
elapsed_time: 90.6605

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 1351.85185185 percent_correct: 0.9470166666666666

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 1388.888888888889 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[19955,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 1388.89


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1834s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.8785s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.10433s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37690
find clusters duration: 0.259459s
detected clusters: 106
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9396
size cluster i: 8 -> 10001
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10002
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9983
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10002
size cluster i: 44 -> 2621
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9890
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10004
size cluster i: 67 -> 10001
size cluster i: 68 -> 10002
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10001
size cluster i: 73 -> 10000
size cluster i: 74 -> 10001
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 7895
size cluster i: 81 -> 10001
size cluster i: 82 -> 4623
size cluster i: 83 -> 10000
size cluster i: 84 -> 10001
size cluster i: 85 -> 10000
size cluster i: 86 -> 10001
size cluster i: 87 -> 10000
size cluster i: 88 -> 10000
size cluster i: 89 -> 10001
size cluster i: 90 -> 10000
size cluster i: 91 -> 10000
size cluster i: 92 -> 10001
size cluster i: 93 -> 8387
size cluster i: 94 -> 4602
size cluster i: 95 -> 6040
size cluster i: 96 -> 10000
size cluster i: 97 -> 284
size cluster i: 98 -> 4
size cluster i: 99 -> 3
size cluster i: 100 -> 2
size cluster i: 101 -> 2
datapoints in clusters: 963794
score: 88.8202


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:54:11 2019
elapsed time: 90.7742s


Finishing: 
---------- 
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 3 received cleanup signal...
Beginning cleanup...
Node 5 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 1 received cleanup signal...
Node 6 received cleanup signal...
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3: Exiting... 
Node 5: Exiting... 
Node 1: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Cleanup done!
Node 8: Exiting... 
Node 2: Exiting... 
Node 6: Exiting... 
Node 7: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:288333] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:288333] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 106
datapoints_clusters: 963794
score: 88.8202
elapsed_time: 90.7742

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 1388.88888889 percent_correct: 0.9447676470588235

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 1425.9259259259259 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[19504,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 1425.93


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 2: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 4: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1583s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.8864s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 8: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.06344s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37589
find clusters duration: 0.269253s
detected clusters: 106
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9245
size cluster i: 8 -> 10001
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9972
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10001
size cluster i: 44 -> 2224
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9853
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10004
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10001
size cluster i: 73 -> 9999
size cluster i: 74 -> 10001
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 7620
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 10000
size cluster i: 91 -> 10001
size cluster i: 92 -> 8122
size cluster i: 93 -> 4251
size cluster i: 94 -> 4221
size cluster i: 95 -> 5693
size cluster i: 96 -> 10000
size cluster i: 97 -> 195
size cluster i: 98 -> 2
size cluster i: 99 -> 2
size cluster i: 100 -> 2
size cluster i: 101 -> 2
size cluster i: 104 -> 2
datapoints in clusters: 961462
score: 88.6053


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:55:52 2019
elapsed time: 90.7076s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 8 received cleanup signal...
Beginning cleanup...
Node 3 received cleanup signal...
Node 4 received cleanup signal...
Node 2 received cleanup signal...
Node 5 received cleanup signal...
Node 7 received cleanup signal...
Node 6 received cleanup signal...
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 4 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 3: Exiting... 
Node 1: Exiting... 
Node 5: Exiting... 
Node 7: Exiting... 
Node 4: Exiting... 
Destroying the enviroment now!
Node 2: Exiting... 
Node 6: Exiting... 
Cleanup done!
Node 8: Exiting... 

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:288654] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:288654] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 106
datapoints_clusters: 961462
score: 88.6053
elapsed_time: 90.7076

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 1425.92592593 percent_correct: 0.9424911764705882

--------------- CMD -----------------
mpirun.openmpi -n 9 ./datadriven/examplesMPI/distributed_clustering_cmd --config OCL_configs/config_ocl_float_gtx1080ti.cfg --MPIconfig clustering_scripts/argon_job_scripts/GTXConf8.cfg --binary_header_filename ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise --level=7 --lambda=1.7782794100389227e-07 --k 6 --epsilon 0.001 --threshold 1462.962962962963 --print_cluster_sizes 1 --target_clusters 100 --write_cluster_map gaussian_c100_size1000000_dim10_noise_cluster_map.csv  >> tune_precision_10d_1000000s.log 2>tune_precision_10d_1000000s_error.log
---------------- OUTPUT -----------------
--------------------------------------------------------------------------
[[17225,1],4]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: argon-gtx

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------


Arguments (Scenario):
--------------------- 
binary_header_filename: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
OpenCL configuration file: OCL_configs/config_ocl_float_gtx1080ti.cfg
level: 7
lambda: 1.77828e-07
k: 6
threshold: 1462.96


Setup:
------ 
Using MPI network config setting: clustering_scripts/argon_job_scripts/GTXConf8.cfg
Network initialized and ready
dim: 10
Grid created! Number of grid points:     397825


Create right-hand side of density equation: 
-------------------------------------------- 
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 4: Created slave operation "DensityRHSWorker"
Node 1: Created slave operation "DensityRHSWorker"
Node 2: Created slave operation "DensityRHSWorker"
Node 3: Created slave operation "DensityRHSWorker"
Node 5: Created slave operation "DensityRHSWorker"
Node 7: Created slave operation "DensityRHSWorker"
Node 8: Created slave operation "DensityRHSWorker"
Node 6: Created slave operation "DensityRHSWorker"
rhs creation duration: 10.1488s
Solve for alpha: 
--------------- 
Starting Conjugated Gradients
All temp variables used in CG have been initialized
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
query_size: 4
max_compute_units: 28
Node 5: Created slave operation "DensityMultiplicationWorker"
Node 4: Created slave operation "DensityMultiplicationWorker"
Node 6: Created slave operation "DensityMultiplicationWorker"
Node 7: Created slave operation "DensityMultiplicationWorker"
Node 2: Created slave operation "DensityMultiplicationWorker"
Node 8: Created slave operation "DensityMultiplicationWorker"
Node 3: Created slave operation "DensityMultiplicationWorker"
Node 1: Created slave operation "DensityMultiplicationWorker"
Starting norm of residuum: 0.0440302
Target norm:               4.40302e-08
delta: 0.0228153
delta: 0.0542743
delta: 0.0441064
delta: 0.0313701
delta: 0.0366547
delta: 0.0577246
delta: 0.0345066
delta: 0.0250373
delta: 0.0360746
delta: 0.0147054
delta: 0.0686063
delta: 0.0551829
delta: 0.0180146
delta: 0.00777993
delta: 0.0108931
delta: 0.0394346
delta: 0.0101097
delta: 0.00994908
delta: 0.0195111
delta: 0.0150442
delta: 0.00403706
delta: 0.0577254
delta: 0.00629846
delta: 0.0128662
delta: 0.00295684
delta: 0.0064206
delta: 0.0278526
delta: 0.00341689
delta: 0.00367335
delta: 0.00163704
delta: 0.00120684
delta: 0.00526433
delta: 0.00119544
delta: 0.00154211
delta: 0.00705403
delta: 0.000817003
delta: 0.00630608
delta: 0.000912516
delta: 0.000688829
delta: 0.000410999
delta: 0.000721477
delta: 0.000572602
delta: 0.000608053
delta: 0.00228365
delta: 0.000159561
delta: 0.000416712
delta: 0.00188639
delta: 0.000170203
delta: 0.000490717
delta: 0.000142454
delta: 0.000437842
delta: 0.000358411
delta: 0.000188584
delta: 8.45118e-05
delta: 0.000326524
delta: 0.00151212
delta: 2.76399e-05
delta: 0.000141713
delta: 6.19822e-05
delta: 0.000280101
delta: 6.41816e-05
delta: 4.07428e-05
delta: 0.000209295
delta: 6.86574e-05
delta: 2.46897e-05
delta: 0.000136331
delta: 3.34524e-05
delta: 0.000170856
delta: 1.20865e-05
delta: 9.91311e-06
delta: 0.000184107
delta: 6.2606e-05
delta: 1.28652e-05
delta: 1.13997e-05
delta: 1.32501e-05
delta: 2.19924e-05
delta: 1.15252e-05
delta: 5.93622e-06
delta: 2.46551e-05
delta: 1.26051e-05
delta: 3.36076e-05
delta: 4.1083e-06
delta: 8.92724e-06
delta: 7.94026e-06
delta: 4.49991e-06
delta: 3.29353e-06
delta: 3.02546e-05
delta: 1.06477e-05
delta: 2.13764e-06
delta: 4.32391e-06
delta: 1.87058e-06
delta: 1.96467e-06
delta: 5.72366e-07
delta: 4.99663e-06
delta: 8.14135e-06
delta: 2.93283e-06
delta: 6.90314e-07
delta: 1.65562e-06
delta: 1.25007e-05
delta: 2.88016e-07
delta: 2.69174e-06
delta: 1.53046e-06
delta: 3.98744e-06
delta: 5.08736e-07
delta: 1.61589e-07
delta: 7.3312e-07
delta: 3.70994e-06
delta: 3.39097e-07
delta: 4.83726e-07
delta: 2.21575e-07
delta: 1.55182e-06
delta: 1.23801e-07
delta: 1.52344e-07
delta: 1.05668e-07
delta: 5.44886e-07
delta: 6.32114e-08
delta: 9.24658e-08
delta: 1.45014e-07
delta: 5.03731e-07
delta: 7.24178e-08
delta: 2.59937e-08
Number of iterations: 121 (max. 1000)
Final norm of residuum: 2.59937e-08
counted_mult_calls: 124
solver duration: 67.8506s
Number of grid points:     397825
alpha min: -1881.37 max: 5572.78
Create and prune graph: 
----------------------- 
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
Getting dataset from enviroment cache
Dataset should be cached
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
unpadded_datasize: 1020000 adding elements: 32 (* dims)
Node 6: Created slave operation "PrunedGraphCreationWorker"
Node 2: Created slave operation "PrunedGraphCreationWorker"
Node 3: Created slave operation "PrunedGraphCreationWorker"
Node 4: Created slave operation "PrunedGraphCreationWorker"
Node 7: Created slave operation "PrunedGraphCreationWorker"
Node 5: Created slave operation "PrunedGraphCreationWorker"
Node 1: Created slave operation "PrunedGraphCreationWorker"
Node 8: Created slave operation "PrunedGraphCreationWorker"
create knn operation duration: 9.00255s
Find clusters in pruned graph: 
------------------------------ 
connected_components merges: 37516
find clusters duration: 0.263465s
detected clusters: 109
size cluster i: 0 -> 10001
size cluster i: 1 -> 10000
size cluster i: 2 -> 10000
size cluster i: 3 -> 10000
size cluster i: 4 -> 10001
size cluster i: 5 -> 10000
size cluster i: 6 -> 10000
size cluster i: 7 -> 9068
size cluster i: 8 -> 10001
size cluster i: 9 -> 10000
size cluster i: 10 -> 10000
size cluster i: 11 -> 10000
size cluster i: 12 -> 10001
size cluster i: 13 -> 10000
size cluster i: 14 -> 10000
size cluster i: 15 -> 9959
size cluster i: 16 -> 10000
size cluster i: 17 -> 20001
size cluster i: 18 -> 10001
size cluster i: 19 -> 10000
size cluster i: 20 -> 10000
size cluster i: 21 -> 10001
size cluster i: 22 -> 10000
size cluster i: 23 -> 10002
size cluster i: 24 -> 10000
size cluster i: 25 -> 10002
size cluster i: 26 -> 10000
size cluster i: 27 -> 10001
size cluster i: 28 -> 10000
size cluster i: 29 -> 10001
size cluster i: 30 -> 10000
size cluster i: 31 -> 10000
size cluster i: 32 -> 10001
size cluster i: 33 -> 10000
size cluster i: 34 -> 10000
size cluster i: 35 -> 10001
size cluster i: 36 -> 10001
size cluster i: 37 -> 10001
size cluster i: 38 -> 10003
size cluster i: 39 -> 10001
size cluster i: 40 -> 10001
size cluster i: 41 -> 10001
size cluster i: 42 -> 10000
size cluster i: 43 -> 10001
size cluster i: 44 -> 1885
size cluster i: 45 -> 10001
size cluster i: 46 -> 20003
size cluster i: 47 -> 9799
size cluster i: 48 -> 10000
size cluster i: 49 -> 10001
size cluster i: 50 -> 10000
size cluster i: 51 -> 10000
size cluster i: 52 -> 10001
size cluster i: 53 -> 10001
size cluster i: 54 -> 10000
size cluster i: 55 -> 10000
size cluster i: 56 -> 10000
size cluster i: 57 -> 10000
size cluster i: 58 -> 10000
size cluster i: 59 -> 10001
size cluster i: 60 -> 10001
size cluster i: 61 -> 10002
size cluster i: 62 -> 10000
size cluster i: 63 -> 10002
size cluster i: 64 -> 10001
size cluster i: 65 -> 10001
size cluster i: 66 -> 10004
size cluster i: 67 -> 10001
size cluster i: 68 -> 10001
size cluster i: 69 -> 10000
size cluster i: 70 -> 10001
size cluster i: 71 -> 10000
size cluster i: 72 -> 10001
size cluster i: 73 -> 9999
size cluster i: 74 -> 10001
size cluster i: 75 -> 10001
size cluster i: 76 -> 10001
size cluster i: 77 -> 10000
size cluster i: 78 -> 10000
size cluster i: 79 -> 10000
size cluster i: 80 -> 7322
size cluster i: 81 -> 10000
size cluster i: 82 -> 10000
size cluster i: 83 -> 10001
size cluster i: 84 -> 10000
size cluster i: 85 -> 10001
size cluster i: 86 -> 10000
size cluster i: 87 -> 10000
size cluster i: 88 -> 10001
size cluster i: 89 -> 10000
size cluster i: 90 -> 10000
size cluster i: 91 -> 10001
size cluster i: 92 -> 7876
size cluster i: 93 -> 3919
size cluster i: 94 -> 3848
size cluster i: 95 -> 5311
size cluster i: 96 -> 10000
size cluster i: 97 -> 137
size cluster i: 98 -> 6
size cluster i: 99 -> 2
size cluster i: 100 -> 3
size cluster i: 101 -> 2
size cluster i: 102 -> 4
size cluster i: 104 -> 2
size cluster i: 105 -> 2
size cluster i: 106 -> 3
size cluster i: 107 -> 2
datapoints in clusters: 959205
score: 85.5761


Runtimes: 
--------- 
finished computation at Thu Jan  3 17:57:32 2019
elapsed time: 90.6083s


Finishing: 
---------- 
Node 1 received cleanup signal...
Node 2 received cleanup signal...
Beginning cleanup...
Node 4 received cleanup signal...
Node 7 received cleanup signal...
Node 8 received cleanup signal...
Node 5 received cleanup signal...
Node 3 received cleanup signal...
Node 6 received cleanup signal...
Node 4 All remaining workers are now deleted!
Node 2 All remaining workers are now deleted!
Node 8 All remaining workers are now deleted!
Node 6 All remaining workers are now deleted!
Node 1 All remaining workers are now deleted!
Node 5 All remaining workers are now deleted!
Node 3 All remaining workers are now deleted!
Node 7 All remaining workers are now deleted!
Node 4: Exiting... 
Node 8: Exiting... 
Node 3: Exiting... 
Node 5: Exiting... 
Node 2: Exiting... 
Node 7: Exiting... 
Node 6: Exiting... 
Node 1: Exiting... 
Destroying the enviroment now!
Cleanup done!

---------------- ERROR ------------------
Loading binary dataset: 
Now reading header file:../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise
Now reading binary data: ../../DissertationCodeTesla1/SGpp/datasets_WPDM18/gaussian_c100_size1000000_dim10_noise_binary_data.bin
dataset_size: 1020000
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
[argon-gtx:289015] 8 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[argon-gtx:289015] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning! Flag PREFERED_PACKAGESIZE is deprecated! Use PACKAGESIZE instead or rely on the defaults.
Warning! Flag REDISTRIBUTE is deprecated and will be ignored!
Warning: Node 1 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 2 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 4 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 8 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 7 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 3 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 6 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!
Warning: Node 5 is going to delete COUNT entry for GeForce GTX 1080 Ti and will insert its own SELECT entry with thevalue given in the MPI config!

--------------- SUMMARY -----------------
detected_clusters: 109
datapoints_clusters: 959205
score: 85.5761
elapsed_time: 90.6083

--------------- END RUN -----------------
attempt lambda: 1.7782794100389227e-07 threshold: 1462.96296296 percent_correct: 0.9402637254901961
final overall_best_percent_correct: 0.9563970588235294, overall_best_threshold: 1500.0, overall_best_lambda_value: 1e-07
